[
  {
    "path": "posts/2024-02-10-drawing-a-zodiac-chart/",
    "title": "Zodiac Charts",
    "description": "Zodiac signs are popular in many countries. In this post I am offering a routine for creation of fancy zodiac charts based on the birth dates. The example charts are drawn for the elected members of some national parliaments (US House of Representative, UK House of Commons, France National Assembly, German Bundestag, and Russian Duma). The text and the charts are free of any political implications.",
    "author": [
      {
        "name": "Aleksei Lutai",
        "url": {}
      }
    ],
    "date": "2024-02-10",
    "categories": [
      "r",
      "wikidata",
      "ggplot2"
    ],
    "contents": "\r\n\r\nContents\r\nNotes on Code\r\nFonts\r\nExtra Data\r\nData\r\nWestern Zodiac Chart\r\nChinese Zodiac Chart\r\nCombo Charts\r\nThings I would do…\r\nAcknowledgments\r\n\r\nWhy the parliaments? Or it could be “How dare you to look at our Parliament”? My thinking is that if there is any zodiacal influence (which I am not sure myself), I would expect it to be manifested to much higher extent amongst the politicians - as their work requires a very specific blend of moral, ethical, and personal traits. The politicians are also public persons, so it is not a problem to obtain their birth dates.\r\nThis is very much not a fundamental research with probabilities and null-hypotheses, there will be no solid proofs or conclusions. It’s just an exercise on using ggplot, wikidata, and the fonts.\r\nSince I know too little about astrology, I have to plead my ignorance for possible incorrect use of the terms and not going deeper. Wikipedia, I guess, could be a starting point for you to become acquainted with the basics:\r\nAstrological sign (Wikipedia/En)\r\nChinese Zodiac (Wikipedia/En)\r\nIn the text below I use a term “west zodiac” for the astrological signs like Libra or Virgo, and the terms “east zodiac” or “chinese zodiac” for the years of Dragon, Tiger, etc. Please do not take it as offence, if you think that the chosen names are kind of inappropriate.\r\nNotes on Code\r\nThere are few things that can help you to understand and adopt this script:\r\nduring the session I save the fonts and the downloaded data into a folder encoded in the variable dir_data, the charts – into a folder set as dir_charts. The code in the other chunks will be using those variables for saving and loading, so if you decide to use this routine, set the folder names according to your data management plan.\r\n\r\n\r\nShow code\r\n\r\ndir0 <- \"D://tmp_data/\"\r\nif(!dir.exists(dir0)){ dir.create(dir0) }\r\n\r\ndir_data <- paste0(dir0, \"data/\")\r\nif(!dir.exists(dir_data)){ dir.create(dir_data) }\r\n\r\ndir_charts <- paste0(getwd(), \"/images/\")\r\nif(!dir.exists(dir_charts)){ dir.create(dir_charts) }\r\n\r\n\r\nin the chunks where something is getting downloaded, I usually check if the destination file already exists to avoid double saving and speed up the script running.\r\nIn some cases I will have to parse the dates written as 01 Jan 2024 or 15 July 1918. If your regional setting are not english (like mine), most likely you will have to set the time locale as english (see below).\r\n\r\n\r\nShow code\r\n\r\nSys.setlocale(\"LC_TIME\", \"english\")\r\n\r\n[1] \"English_United States.1252\"\r\n\r\nFonts\r\nIn this project I am using 2 fonts with the zodiac glyphs and some other fonts for styling. The showtext package have a nice option to load the google fonts from internet (see vignette), but I am cautious about making my scripts too much dependent on the internet’s availability. That’s why in the script below the fonts are loaded from a local storage of my favourite fonts, referred as local_font_folder.\r\nIf you decide to use special fonts, you will have to set the correct paths to the font files (or you can try to use sysfonts::font_add_google()).\r\nEssential TTF Fonts:\r\nChinese Zodiac (I could not find its creator). The font can be downloaded from many web sites like onlinewebfonts.com.\r\nSL Zodiac Stencils Font (download from FontSpace, created by Su Lucas.\r\nI referred to these fonts as essential in a sense that you need to have these fonts at hand to draw zodiac charts. I downloaded them manually and saved into my local font storage as east_zodiac.ttf and west_zodiac.ttf.\r\nOther Fonts\r\nIn this project I use for styling few other fonts, distributed with SIL Open Font License (OFL):\r\nRoboto Condensed\r\nSofia Sans Condensed\r\nand\r\nFont Awesome 6 for the icons.\r\nThe script below shows how to load the fonts.\r\n\r\n\r\nShow code\r\n\r\nshowtext_auto()\r\n\r\nsysfonts::font_add(\"zodiac\", regular = paste0(local_font_folder, \"/astro.ttf\"))\r\nsysfonts::font_add(\"ch_zodiac\", regular = paste0(local_font_folder, \"/chn_zodiac.ttf\"))\r\n\r\nfont_add(\"RobotoC\", regular = paste0(local_font_folder, \"RobotoCondensed-Regular.ttf\"))\r\n\r\nfont_add(\"Sofia\", \r\n         regular = paste0(local_font_folder, \"SofiaSansCondensed-SemiBold.ttf\"),\r\n         bold = paste0(local_font_folder, \"SofiaSansCondensed-SemiBold.ttf\"))\r\n\r\nsysfonts::font_add(\"fa_solid\",\r\n                    regular = paste0(local_font_folder, \"/fontawesome/otfs/Font Awesome 6 Free-Solid-900.otf\"))\r\n\r\n\r\nStyling (ggplot2 theme)\r\n\r\n\r\nShow code\r\n\r\nblankbg <- theme(axis.line = element_blank(),  \r\n                 axis.text=element_blank(),\r\n                 axis.ticks=element_blank(),  \r\n                 axis.title = element_blank(),\r\n                 plot.background=element_rect(fill = \"#232323\", colour = \"#232323\"), \r\n                 panel.background=element_rect(fill = \"#232323\", colour = \"#232323\"),  \r\n                 panel.border = element_blank(),\r\n                 panel.grid.minor=element_blank(), \r\n                 panel.grid.major = element_blank(),\r\n                 plot.margin = unit(c(t=0.2,r=0.05,b=0.2,l=0.05), \"cm\"),\r\n                 plot.title.position = \"plot\", \r\n                 plot.caption.position = \"plot\",\r\n                 plot.title = element_text(size = 22, hjust = 0, \r\n                                           face = \"bold\", colour = \"#ffffff\", family = \"Sofia\"), \r\n                 plot.subtitle = element_text(hjust = 0, family = \"RobotoC\", vjust = 2,  \r\n                                              colour = \"#ffffff\", face = \"plain\", size = rel(0.7)),\r\n                 plot.caption = element_markdown(halign = 0, hjust = 0, \r\n                                                 size = rel(0.8),colour = \"#827C82\"))\r\n\r\nshowtext::showtext_opts(dpi = 192)\r\n\r\n\r\nExtra Data\r\nThe fonts need to be correctly associated with the zodiac names and the preferred order. For this I create the small tables where the letters correspond to zodiac symbols (a letter against each zodiac sign corresponds to its graphical representations provided by the font).\r\nThe tables below are created with a tribble function, you may just copy/paste it into your script, and tinkle further as you wish (for example, change the order values to ensure that some particular sign opens a list).\r\n\r\n\r\nShow code\r\n\r\nwest_symbols <- tibble::tribble(\r\n  ~unicode, ~sign, ~order, ~letter, \r\n  \"\\u2652\", \"Aquarius\",1, \"k\",\r\n  \"\\u2653\", \"Pisces\", 2, \"l\",\r\n  \"\\u2648\", \"Aries\", 3, \"a\",\r\n  \"\\u2649\", \"Taurus\", 4, \"b\", \r\n  \"\\u264A\", \"Gemini\", 5, \"c\",\r\n  \"\\u264B\", \"Cancer\", 6, \"d\",\r\n  \"\\u264C\", \"Leo\", 7, \"e\",\r\n  \"\\u264D\", \"Virgo\", 8, \"f\",\r\n  \"\\u264E\", \"Libra\", 9, \"g\",\r\n  \"\\u264F\", \"Scorpio\",10, \"h\",\r\n  \"\\u2650\", \"Sagittarius\", 11, \"i\",\r\n  \"\\u2651\", \"Capricorn\", 12, \"j\"\r\n) \r\npaged_table(west_symbols, options = list(rows.print = 6))\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\neast_symbols <- tribble(\r\n        ~sign, ~order, ~letter,\r\n        \"Rat\", 1, \"a\",\r\n        \"Ox\", 2, \"b\",\r\n        \"Tiger\", 3, \"c\",\r\n        \"Rabbit\", 4, \"d\",\r\n        \"Dragon\", 5, \"e\",\r\n        \"Snake\", 6, \"f\",\r\n        \"Horse\", 7, \"g\",\r\n        \"Goat\", 8, \"h\",\r\n        \"Monkey\", 9, \"i\",\r\n        \"Rooster\", 10, \"j\",\r\n        \"Dog\", 11, \"k\",\r\n        \"Pig\", 12,\"l\")\r\npaged_table(east_symbols, options = list(rows.print = 6))\r\n\r\n\r\n\r\n\r\nWestern zodiac signs (Libra, Capricorn, etc) are easy to calculate, as the date ranges are the same for each year (i.e. Taurus period is always from 21.04 till 21.05). So it’s just 12 date ranges to be matched against to get a astrological sign for any birth date. I found a package DescTools providing a function named Zodiac. Its using is as simple as DescTools::Zodiac(birth_date, lang = “eng”), where birth_date is a date in POSIXlt format.\r\nWith Chinese zodiacs (Rat, Rooster, Dragon, etc) things are a bit more complicated, as the Chinese New Year date is not fixed. Yes, the years begin on different dates, you my check Wikipedia article Chinese calendar for additional details.\r\nI extracted the Chinese zodiac years from Wikipedia/En article named Chinese zodiac.\r\nFirst, I use API call to get a list of sections for the article: https://en.wikipedia.org/w/api.php?action=parse&page=Chinese_zodiac&prop=sections. The section Years contains a Table with the Chinese zodiac years from 1924 till 2044, its index is 4.\r\nNext, I use API call to obtain an html for the section 4, and extract the table data (see code in the chunk below).\r\n\r\n\r\nShow code\r\n\r\nch_dates_file <- paste0(dir_data, \"ch_dates.RDS\")\r\nif(!file.exists(ch_dates_file)){\r\n  ch_dates <- \"https://en.wikipedia.org/w/api.php?action=parse&page=Chinese_zodiac&section=4&prop=text&format=json\" |>\r\n    rvest::read_html(encoding = \"utf8\") |>\r\n    rvest::html_table(trim = T) |> pluck(1) |>\r\n    select(year1 = 2, year2 = 3, element = 4, heavenly_stem = 5, earthly_branch = 6, sign = 7) |>\r\n    # first row does not contain the data, it originates from 2-level table header.\r\n    filter(row_number()>1) |>\r\n    pivot_longer(contains(\"year\"), names_to = NULL, values_to = \"year\") |>\r\n    separate(year, sep = \"\\\\\\\\u2013\", into = c(\"date_start\", \"date_end\")) |>\r\n    separate(element, sep = \" \", into = c(\"element_ch\", \"element_en\")) |>\r\n    mutate(sign = str_replace(sign, \"\\\\\\\\n\", \"\"),\r\n           across(contains(\"date\"), ~as.Date(.x, \"%B %d %Y\"))) |>\r\n    arrange(date_start) |> \r\n    left_join(ch_symbols)\r\n  \r\n   write_rds(ch_dates, ch_dates_file)\r\n} else {\r\n  ch_dates <- read_rds(ch_dates_file)\r\n}\r\n\r\npaged_table(ch_dates, options = list(rows.print = 6))\r\n\r\n\r\n\r\n\r\nNow we are ready to collect the data with the birth dates.\r\nData\r\nWikipedia and Wikidata have versatile APIs to ease our collecting the birth dates of the selected parliaments. Below I use SPARQL queries to Wikidata in order to get the birth dates for a particular group of parliament members.\r\n\r\nIn some cases a number of parliament members were a bit different than it should be according to the corresponding Wikipedia articles. Some members may not be present or may not have a birth date in Wikidata. Or some persons may be inaccurately assigned to a chamber of parliament, being not an MP, but member of committee or something. I am not an expert in the electoral and mid-electoral lifecyles in the national parliaments, so I did not try to seek the causes. The data is provided as it is collected from Wikidata.\r\n\r\nMembers of the 9th European Parliament\r\n\r\n\r\nShow code\r\n\r\neu_file <- paste0(dir_data, \"eu_data.RDS\")\r\nif(!file.exists(eu_file)){\r\n  eu <- query_wikidata('SELECT DISTINCT ?item ?dob WHERE {\r\n                     ?item p:P39 ?pSt. ?pSt ps:P39 wd:Q27169; pq:P2937 wd:Q64038205. \r\n                     OPTIONAL {?item wdt:P569 ?dob.}}') |> \r\n    na.omit()\r\n  write_rds(eu, eu_file, compress = \"gz\")\r\n } else {\r\n   eu <- read_rds(eu_file)\r\n  }\r\n\r\npaged_table(eu, options = list(rows.print = 5))\r\n\r\n\r\n\r\n\r\nMembers of the 58th Parliament of the United Kingdom\r\n\r\n\r\nShow code\r\n\r\nuk_file <- paste0(dir_data, \"uk_data.RDS\")\r\nif(!file.exists(uk_file)){\r\n  uk <- query_wikidata('SELECT DISTINCT ?item ?dob WHERE {\r\n                     ?item p:P39 ?pSt. ?pSt ps:P39 wd:Q77685926. \r\n                     OPTIONAL {?item wdt:P569 ?dob.}}') |> \r\n    na.omit()\r\n  write_rds(uk, uk_file, compress = \"gz\")\r\n } else {\r\n   uk <- read_rds(uk_file)\r\n  }\r\n\r\npaged_table(uk, options = list(rows.print = 5))\r\n\r\n\r\n\r\n\r\nElected members of the U.S. House of Representatives (the 117th United States Congress)\r\n\r\n\r\nShow code\r\n\r\nus_file <- paste0(dir_data, \"us_data.RDS\")\r\nif(!file.exists(us_file)){\r\n us <- query_wikidata('SELECT DISTINCT ?item ?dob WHERE {\r\n                     ?item p:P39 ?pSt. ?pSt ps:P39 wd:Q13218630; pq:P2937 wd:Q65089999. \r\n                     OPTIONAL {?item wdt:P569 ?dob.}}') |> \r\n    na.omit()\r\n  write_rds(us, us_file, compress = \"gz\")\r\n } else {\r\n   us <- read_rds(us_file)\r\n  }\r\n\r\npaged_table(us, options = list(rows.print = 5))\r\n\r\n\r\n\r\n\r\nElected members of the 16th legislature of the Fifth French Republic (French National Assembly)\r\n\r\n\r\nShow code\r\n\r\nfr_file <- paste0(dir_data, \"fr_data.RDS\")\r\nif(!file.exists(fr_file)){\r\n fr <- query_wikidata('SELECT DISTINCT ?item ?dob WHERE {\r\n                     ?item p:P39 ?pSt. ?pSt ps:P39 wd:Q3044918; pq:P2937 wd:Q112567597. \r\n                     OPTIONAL {?item wdt:P569 ?dob.}}') |> \r\n    na.omit()\r\n  write_rds(fr, fr_file, compress = \"gz\")\r\n } else {\r\n   fr <- read_rds(fr_file)\r\n  }\r\n\r\npaged_table(fr, options = list(rows.print = 5))\r\n\r\n\r\n\r\n\r\nElected members of the 16th legislature of the Fifth French Republic (French National Assembly)\r\n\r\n\r\nShow code\r\n\r\nde_file <- paste0(dir_data, \"de_data.RDS\")\r\nif(!file.exists(de_file)){\r\n de <- query_wikidata('SELECT DISTINCT ?item ?dob WHERE {\r\n                     ?item p:P39 ?pSt. ?pSt ps:P39 wd:Q1939555; pq:P2937 wd:Q33091469. \r\n                     OPTIONAL {?item wdt:P569 ?dob.}}') |> \r\n    na.omit()\r\n  write_rds(de, de_file, compress = \"gz\")\r\n } else {\r\n   de <- read_rds(de_file)\r\n  }\r\n\r\npaged_table(de, options = list(rows.print = 5))\r\n\r\n\r\n\r\n\r\nRussian Duma\r\n\r\n\r\nShow code\r\n\r\nru_file <- paste0(dir_data, \"ru_data.RDS\")\r\nif(!file.exists(ru_file)){\r\n ru <- query_wikidata('SELECT DISTINCT ?item ?dob WHERE {\r\n                     ?item p:P39 ?pSt. ?pSt ps:P39 wd:Q17276321; pq:P2937 wd:Q106638694.\r\n                     OPTIONAL {?item wdt:P569 ?dob.}}') |> \r\n    na.omit()\r\n  write_rds(ru, ru_file, compress = \"gz\")\r\n } else {\r\n   ru <- read_rds(ru_file)\r\n  }\r\n\r\npaged_table(ru, options = list(rows.print = 5))\r\n\r\n\r\n\r\n\r\nJust in few seconds we collected the birth dates for 3,775 members of 6 large parliaments, elected via independent voting process.\r\nNow it’s time to draw the charts.\r\nWestern Zodiac Chart\r\nBelow is a function that draws a chart, with the following arguments:\r\ndf (a dataframe from Wikidata with dob column)\r\nsymbols_table (a table that orders the zodiac signs and link them to font-coding letters. In this script it is named “west_symbols”)\r\ntitle\r\nsubtitle\r\n\r\n\r\nShow code\r\n\r\nwest_zodiac_chart <- function(df, symbols_table, ch_title, ch_subtitle){\r\n  df |> \r\n    mutate(sign = DescTools::Zodiac(dob, lang = \"eng\"), \r\n           total = n()) |>\r\n    count(total, sign) |>\r\n    left_join(symbols_table, by = join_by(sign)) |> \r\n    arrange(order) |> na.omit() |> \r\n    mutate(n = 2000*n/total) |>\r\n    ggplot() +\r\n    geom_hline(yintercept = base::pretty(c(0,200), n = 5),\r\n               linetype = 2, linewidth = 0.2, colour = \"grey80\") +\r\n    geom_col(aes(x = order, y = n*1.02, fill = n), \r\n             alpha = 0.9, width = 0.9, colour = \"grey90\", linewidth = 0.1) +\r\n    geom_richtext(aes(x = order, y = 315, label = letter),\r\n                      vjust = 1.7, colour = \"white\", family = \"zodiac\", \r\n                      size = 15, label.colour = NA, fill = NA)+\r\n    scale_x_reverse(expand = expansion(add = 0.05))+\r\n    coord_polar(start = -pi / 6) +\r\n    scale_fill_viridis_c(option = \"H\", direction = 1, begin = 0.1, end = 0.8)+\r\n    guides(fill = \"none\") + \r\n    blankbg +\r\n    labs(title = ch_title, \r\n         subtitle = ch_subtitle, \r\n         caption = paste0(\"<span style='font-family:\\\"fa_solid\\\"'>&#xf781;<\/span>\",\r\n                         \"<span style='font-family:\\\"RobotoC\\\";'>   dwayzer.netlify.app<\/span>\",\r\n                         \"<br/><span style='font-family:\\\"RobotoC\\\"; font-size:\\\"5.3pt\\\";'>\",\r\n                         \"Fonts: SL Zodiac Stencils Font &#169; Su Lucas, 2002 | \",\r\n                         \"Roboto &#169; The Roboto Project Authors, 2011 | \",\r\n                         \"Sofia Sans &#169; The Sofia Sans Project Authors, 2019<\/span>\"))\r\n}\r\n\r\n\r\nEU\r\n\r\n\r\nShow code\r\n\r\neu_chart_file <- paste0(dir_charts, \"eu_west_zodiac.png\")\r\n\r\nif(!file.exists(eu_chart_file)){\r\n  title <- \"EU: European Parliament (2019)\"\r\n  subtitle <- paste0(\"Data source: Wikidata (Q27169, Q64038205), total number: \", \r\n                     scales::number(nrow(eu), big.mark = \",\"))\r\n  \r\n  ch <- west_zodiac_chart(df = eu, \r\n                          symbols_table = west_symbols, \r\n                          ch_title = title,\r\n                          ch_subtitle = subtitle)\r\n  \r\n  ggplot2::ggsave(filename = eu_chart_file, plot = ch, \r\n                  height = 9.2, width = 8, units = \"cm\", dpi = 300)\r\n}\r\nknitr::include_graphics(eu_chart_file)\r\n\r\n\r\n\r\nFigure 1: Zodiac portrait of the MEPs elected into 9th EU Parliament\r\n\r\n\r\n\r\nUK\r\n\r\n\r\nShow code\r\n\r\nuk_chart_file <- paste0(dir_charts, \"uk_west_zodiac.png\")\r\n\r\nif(!file.exists(uk_chart_file)){\r\n  title <- \"UK: the House of Commons (2019)\"\r\n  subtitle <- paste0(\"Data source: Wikidata (Q77685926), total number: \", \r\n                     scales::number(nrow(us), big.mark = \",\"))\r\n  \r\n  ch <- west_zodiac_chart(df = uk, \r\n                          symbols_table = west_symbols, \r\n                          ch_title = title,\r\n                          ch_subtitle = subtitle)\r\n  \r\n  ggplot2::ggsave(filename = uk_chart_file, plot = ch, \r\n                  height = 9.2, width = 8, units = \"cm\", dpi = 300)\r\n}\r\nknitr::include_graphics(uk_chart_file)\r\n\r\n\r\n\r\nFigure 2: Zodiac portrait of the MPs elected into UK Parliament in 2019\r\n\r\n\r\n\r\nUS\r\n\r\n\r\nShow code\r\n\r\nus_chart_file <- paste0(dir_charts, \"us_west_zodiac.png\")\r\n\r\nif(!file.exists(us_chart_file)){\r\n  title <- \"US: the House of Representatives (2019)\"\r\n  subtitle <- paste0(\"Data source: Wikidata (Q13218630, Q65089999), total number: \", \r\n                     scales::number(nrow(us), big.mark = \",\"))\r\n  \r\n  ch <- west_zodiac_chart(df = us, \r\n                          symbols_table = west_symbols, \r\n                          ch_title = title, \r\n                          ch_subtitle = subtitle)\r\n  \r\n  ggplot2::ggsave(filename = us_chart_file, plot = ch, \r\n                  height = 9.2, width = 8, units = \"cm\", dpi = 300)\r\n  \r\n  remove(ch, title, subtitle)\r\n}\r\nknitr::include_graphics(us_chart_file)\r\n\r\n\r\n\r\nFigure 3: Zodiac portrait of the politicians elected into the US House of Representatives (the 117th US Congress)\r\n\r\n\r\n\r\nFR\r\n\r\n\r\nShow code\r\n\r\nfr_chart_file <- paste0(dir_charts, \"fr_west_zodiac.png\")\r\n\r\nif(!file.exists(fr_chart_file)){\r\n  title <- \"FR: the 16th legislature (2022)\"\r\n  subtitle <- paste0(\"Data source: Wikidata (Q3044918, Q112567597), total number: \", \r\n                     scales::number(nrow(fr), big.mark = \",\"))\r\n  \r\n  ch <- west_zodiac_chart(df = fr, \r\n                          symbols_table = west_symbols, \r\n                          ch_title = title, \r\n                          ch_subtitle = subtitle)\r\n  \r\n  ggplot2::ggsave(filename = fr_chart_file, plot = ch, \r\n                  height = 9.2, width = 8, units = \"cm\", dpi = 300)\r\n  \r\n  remove(ch, title, subtitle)\r\n}\r\nknitr::include_graphics(fr_chart_file)\r\n\r\n\r\n\r\nFigure 4: Zodiac portrait of the politicians elected into the 16th legislature of the 5th French Republic (French National Assembly) (2021)\r\n\r\n\r\n\r\nDE\r\n\r\n\r\nShow code\r\n\r\nde_chart_file <- paste0(dir_charts, \"de_west_zodiac.png\")\r\n\r\nif(!file.exists(de_chart_file)){\r\n  title <- \"DE: the 20th German Bundestag (2021)\"\r\n  subtitle <- paste0(\"Data source: Wikidata (Q1939555, Q33091469), total number: \", \r\n                     scales::number(nrow(de), big.mark = \",\"))\r\n  \r\n  ch <- west_zodiac_chart(df = de, \r\n                          symbols_table = west_symbols, \r\n                          ch_title = title, \r\n                          ch_subtitle = subtitle)\r\n  \r\n  ggplot2::ggsave(filename = de_chart_file, plot = ch, \r\n                  height = 9.2, width = 8, units = \"cm\", dpi = 300)\r\n  \r\n  remove(ch, title, subtitle)\r\n}\r\nknitr::include_graphics(de_chart_file)\r\n\r\n\r\n\r\nFigure 5: Zodiac portrait of the politicians elected into the 20th German Bundestag (2021)\r\n\r\n\r\n\r\nRU\r\n\r\n\r\nShow code\r\n\r\nru_chart_file <- paste0(dir_charts, \"ru_west_zodiac.png\")\r\n\r\nif(!file.exists(ru_chart_file)){\r\n  title <- \"RU: the 8th Russian Duma (2021)\"\r\n  subtitle <- paste0(\"Data source: Wikidata (Q17276321, Q106638694), total number: \", \r\n                     scales::number(nrow(ru), big.mark = \",\"))\r\n  \r\n  ch <- west_zodiac_chart(df = ru, \r\n                          symbols_table = west_symbols, \r\n                          ch_title = title, \r\n                          ch_subtitle = subtitle)\r\n  \r\n  ggplot2::ggsave(filename = ru_chart_file, plot = ch, \r\n                  height = 9.2, width = 8, units = \"cm\", dpi = 300)\r\n  \r\n  remove(ch, title, subtitle)\r\n}\r\nknitr::include_graphics(ru_chart_file)\r\n\r\n\r\n\r\nFigure 6: Zodiac portrait of the members of the VIII’s Russian State Duma (2021)\r\n\r\n\r\n\r\nChinese Zodiac Chart\r\nHere is another function that draws a chart for Chinese zodiac signs, with the following arguments:\r\ndf (a dataframe from Wikidata with dob column)\r\nch_dates (the date range for the Chinese calendar years. In this script it is named “ch_dates”). I also use a function get_ch_sign() to find a chinese zodac year for a specified date.\r\nsymbols_table (a table that orders the zodiac signs and link them to font-coding letters. In this script it is named “east_symbols”)\r\ntitle\r\nsubtitle\r\n\r\n\r\nShow code\r\n\r\n# converting the chinese dates into integer\r\nch_dates <- ch_dates |>\r\n  mutate(across(contains(\"date\"), ~as.integer(.x), .names = \"{.col}_int\"))\r\n\r\n# function to assign the chinese zodiac signs \r\nget_ch_sign <- function(date, ch_dates){\r\n  date <- as.integer(as.Date(date))\r\n  k <- ch_dates |> filter(date_start_int <= date & date_end_int >= date)\r\n  if(nrow(k)>1 | nrow(k) == 0) { return(NA) } else { return( pull(k, sign) ) }\r\n}\r\n\r\neast_zodiac_chart <- function(df, ch_dates, symbols_table, ch_title, ch_subtitle){\r\n  df |>\r\n    mutate(sign = purrr::map_chr(dob, ~get_ch_sign(date = .x, ch_dates)), \r\n           total = n()) |>\r\n    count(total, sign) |>\r\n    left_join(symbols_table, by = join_by(sign)) |> \r\n    arrange(order) |> na.omit() |> \r\n    mutate(n = 2000*n/total) |>\r\n    ggplot() +\r\n    geom_hline(yintercept = base::pretty(c(0,200), n = 5),\r\n               linetype = 2, linewidth = 0.2, colour = \"grey80\") +\r\n    geom_col(aes(x = order, y = n*1.02, fill = n), \r\n             alpha = 0.9, width = 0.9, colour = \"grey90\", linewidth = 0.1) +\r\n    geom_richtext(aes(x = order, y = 315, label = letter),\r\n                      vjust = 0.48, hjust = 0.48, colour = \"white\", family = \"ch_zodiac\", \r\n                      size = 18, label.colour = NA, fill = NA)+\r\n    scale_x_reverse(expand = expansion(add = 0.05))+\r\n    coord_polar(start = -pi / 6) +\r\n    scale_fill_viridis_c(option = \"H\", direction = 1, begin = 0.1, end = 0.8)+\r\n    guides(fill = \"none\") + \r\n    blankbg +\r\n    labs(title = ch_title, \r\n         subtitle = ch_subtitle, \r\n         caption = paste0(\"<span style='font-family:\\\"fa_solid\\\"'>&#xf781;<\/span>\",\r\n                         \"<span style='font-family:\\\"RobotoC\\\";'>   dwayzer.netlify.app<\/span>\",\r\n                         \"<br/><span style='font-family:\\\"RobotoC\\\"; font-size:\\\"5.3pt\\\";'>\",\r\n                         \"Fonts: Chinese Zodiac | \",\r\n                         \"Roboto &#169; The Roboto Project Authors, 2011 | \",\r\n                         \"Sofia Sans &#169; The Sofia Sans Project Authors, 2019<\/span>\"))\r\n}\r\n\r\n\r\nEU\r\n\r\n\r\nShow code\r\n\r\neu_chart_file <- paste0(dir_charts, \"eu_east_zodiac.png\")\r\n\r\nif(!file.exists(eu_chart_file)){\r\n  title <- \"EU: European Parliament (2019)\"\r\n  subtitle <- paste0(\"Data source: Wikidata (Q27169, Q64038205), total number: \", \r\n                     scales::number(nrow(eu), big.mark = \",\"))\r\n  \r\n  ch <- east_zodiac_chart(df = eu, \r\n                          ch_dates = ch_dates,\r\n                          symbols_table = east_symbols, \r\n                          ch_title = title,\r\n                          ch_subtitle = subtitle)\r\n  \r\n  ggplot2::ggsave(filename = eu_chart_file, plot = ch, \r\n                  height = 9.2, width = 8, units = \"cm\", dpi = 300)\r\n}\r\nknitr::include_graphics(eu_chart_file)\r\n\r\n\r\n\r\nFigure 7: Chinese zodiac portrait of the MEPs elected into 9th EU Parliament\r\n\r\n\r\n\r\nUK\r\n\r\n\r\nShow code\r\n\r\nuk_chart_file <- paste0(dir_charts, \"uk_east_zodiac.png\")\r\n\r\nif(!file.exists(uk_chart_file)){\r\n  title <- \"UK: the House of Commons (2019)\"\r\n  subtitle <- paste0(\"Data source: Wikidata (Q77685926), total number: \", \r\n                     scales::number(nrow(us), big.mark = \",\"))\r\n  \r\n  ch <- east_zodiac_chart(df = uk, \r\n                          ch_dates = ch_dates,\r\n                          symbols_table = east_symbols, \r\n                          ch_title = title,\r\n                          ch_subtitle = subtitle)\r\n  \r\n  ggplot2::ggsave(filename = uk_chart_file, plot = ch, \r\n                  height = 9.2, width = 8, units = \"cm\", dpi = 300)\r\n}\r\nknitr::include_graphics(uk_chart_file)\r\n\r\n\r\n\r\nFigure 8: Chinese zodiac portrait of the MPs elected into UK Parliament in 2019\r\n\r\n\r\n\r\nUS\r\n\r\n\r\nShow code\r\n\r\nus_chart_file <- paste0(dir_charts, \"us_east_zodiac.png\")\r\n\r\nif(!file.exists(us_chart_file)){\r\n  title <- \"US: the House of Representatives (2019)\"\r\n  subtitle <- paste0(\"Data source: Wikidata (Q13218630, Q65089999), total number: \", \r\n                     scales::number(nrow(us), big.mark = \",\"))\r\n  \r\n  ch <- east_zodiac_chart(df = us, \r\n                          ch_dates = ch_dates,\r\n                          symbols_table = east_symbols, \r\n                          ch_title = title, \r\n                          ch_subtitle = subtitle)\r\n  \r\n  ggplot2::ggsave(filename = us_chart_file, plot = ch, \r\n                  height = 9.2, width = 8, units = \"cm\", dpi = 300)\r\n  \r\n  remove(ch, title, subtitle)\r\n}\r\nknitr::include_graphics(us_chart_file)\r\n\r\n\r\n\r\nFigure 9: Chinese zodiac portrait of the politicians elected into the US House of Representatives (the 117th US Congress)\r\n\r\n\r\n\r\nFR\r\n\r\n\r\nShow code\r\n\r\nfr_chart_file <- paste0(dir_charts, \"fr_east_zodiac.png\")\r\n\r\nif(!file.exists(fr_chart_file)){\r\n  title <- \"FR: the 16th legislature (2022)\"\r\n  subtitle <- paste0(\"Data source: Wikidata (Q3044918, Q112567597), total number: \", \r\n                     scales::number(nrow(fr), big.mark = \",\"))\r\n  \r\n  ch <- east_zodiac_chart(df = fr, \r\n                          ch_dates = ch_dates,\r\n                          symbols_table = east_symbols, \r\n                          ch_title = title, \r\n                          ch_subtitle = subtitle)\r\n  \r\n  ggplot2::ggsave(filename = fr_chart_file, plot = ch, \r\n                  height = 9.2, width = 8, units = \"cm\", dpi = 300)\r\n  \r\n  remove(ch, title, subtitle)\r\n}\r\nknitr::include_graphics(fr_chart_file)\r\n\r\n\r\n\r\nFigure 10: Chinese zodiac portrait of the politicians elected into the 16th legislature of the 5th French Republic (French National Assembly) (2021)\r\n\r\n\r\n\r\nDE\r\n\r\n\r\nShow code\r\n\r\nde_chart_file <- paste0(dir_charts, \"de_east_zodiac.png\")\r\n\r\nif(!file.exists(de_chart_file)){\r\n  title <- \"DE: the 20th German Bundestag (2021)\"\r\n  subtitle <- paste0(\"Data source: Wikidata (Q1939555, Q33091469), total number: \", \r\n                     scales::number(nrow(de), big.mark = \",\"))\r\n  \r\n  ch <- east_zodiac_chart(df = de, \r\n                          ch_dates = ch_dates,\r\n                          symbols_table = east_symbols, \r\n                          ch_title = title, \r\n                          ch_subtitle = subtitle)\r\n  \r\n  ggplot2::ggsave(filename = de_chart_file, plot = ch, \r\n                  height = 9.2, width = 8, units = \"cm\", dpi = 300)\r\n  \r\n  remove(ch, title, subtitle)\r\n}\r\nknitr::include_graphics(de_chart_file)\r\n\r\n\r\n\r\nFigure 11: Chinese zodiac portrait of the politicians elected into the 20th German Bundestag (2021)\r\n\r\n\r\n\r\nRU\r\n\r\n\r\nShow code\r\n\r\nru_chart_file <- paste0(dir_charts, \"ru_east_zodiac.png\")\r\n\r\nif(!file.exists(ru_chart_file)){\r\n  title <- \"RU: the 8th Russian Duma (2021)\"\r\n  subtitle <- paste0(\"Data source: Wikidata (Q17276321, Q106638694), total number: \", \r\n                     scales::number(nrow(ru), big.mark = \",\"))\r\n  \r\n  ch <- east_zodiac_chart(df = ru, \r\n                          ch_dates = ch_dates,\r\n                          symbols_table = east_symbols, \r\n                          ch_title = title, \r\n                          ch_subtitle = subtitle)\r\n  \r\n  ggplot2::ggsave(filename = ru_chart_file, plot = ch, \r\n                  height = 9.2, width = 8, units = \"cm\", dpi = 300)\r\n  \r\n  remove(ch, title, subtitle)\r\n}\r\nknitr::include_graphics(ru_chart_file)\r\n\r\n\r\n\r\nFigure 12: Chinese zodiac portrait of the members of the VIII’s Russian State Duma (2021)\r\n\r\n\r\n\r\nCombo Charts\r\nIn a chunk below I build a preview chart for this post. For that purpose I am using a patchwork package.\r\n\r\n\r\nShow code\r\n\r\neu_chart_file <- paste0(dir_charts, \"eu_combo_zodiac.png\")\r\n\r\nif(!file.exists(eu_chart_file)){\r\n  library(patchwork)\r\n  ch1 <- west_zodiac_chart(df = eu, symbols_table = west_symbols, \r\n                          ch_title = NULL, ch_subtitle = NULL) + \r\n    labs(caption = NULL)\r\n  ch2 <- east_zodiac_chart(df = eu, ch_dates, symbols_table = east_symbols, \r\n                          ch_title = NULL, ch_subtitle = NULL) + \r\n    labs(caption = NULL)\r\n  \r\n ch <- (free(ch1) + free(ch2)) + \r\n   plot_annotation(\r\n     title = 'Zodiac Charts for the 9th European Parliament MEPs',\r\n     subtitle = paste0(\"Data source: Wikidata. Query: SELECT DISTINCT ?item ?dob WHERE {?item p:P39 ?pSt. ?pSt ps:P39 wd:Q27169; pq:P2937 wd:Q64038205. ?item wdt:P569 ?dob.}\"),\r\n    caption = paste0(\"<span style='font-family:\\\"fa_solid\\\"'>&#xf781;<\/span>\",\r\n                        \"<span style='font-family:\\\"RobotoC\\\";'>   dwayzer.netlify.app<\/span>\",\r\n                        \"<br/><span style='font-family:\\\"RobotoC\\\"; font-size:\\\"7.3pt\\\";'>\",\r\n                        \"Fonts: SL Zodiac Stencils Font &#169; Su Lucas, 2002 | Chinese Zodiac | \",\r\n                        \"Roboto &#169; The Roboto Project Authors, 2011 | \",\r\n                        \"Sofia Sans &#169; The Sofia Sans Project Authors, 2019<\/span>\"),\r\n    theme = blankbg + theme(plot.caption = element_markdown(hjust = 1, halign = 1))) &\r\n   theme(plot.margin = unit(c(t=0.2,r=0.1,b=0.2,l=0.1), \"cm\"))\r\n   \r\n   \r\n  ggplot2::ggsave(filename = eu_chart_file, plot = ch, \r\n                  height = 9, width = 14, units = \"cm\", dpi = 300)\r\n}\r\nknitr::include_graphics(eu_chart_file)\r\n\r\n\r\n\r\nThings I would do…\r\n…if I had a spare time, of course. Even though we see that the national parliaments have some peculiar differences, it would be much more interesting to see how different are the left- and right- political parties. Of course, one may argue that for many politicians a party is not a choice dictated by temper or morality, but rather a career-wise opportunity the person decided to take and pursue at some moment of life. Alas, this could be true.\r\nAnother interesting exercise would be to make a metric to assess an astrological stability of the groups and committees, with a special attention to those formed not via independent voting, but through the appointments (i.e. the members are picked up or recommended by a team leader, or senior members).\r\nAcknowledgments\r\nAllaire J, Xie Y, Dervieux C, McPherson J, Luraschi J, Ushey K, Atkins A, Wickham H, Cheng J,\r\nChang W, Iannone R (2023). rmarkdown: Dynamic Documents for R. R package version 2.22,\r\nhttps://github.com/rstudio/rmarkdown.\r\nPedersen T (2024). patchwork: The Composer of Plots. R package version 1.2.0,\r\nhttps://CRAN.R-project.org/package=patchwork.\r\nPopov M (2020). WikidataQueryServiceR: API Client Library for ‘Wikidata Query Service’. R\r\npackage version 1.0.0, https://CRAN.R-project.org/package=WikidataQueryServiceR.\r\nQiu Y, details. aotifSfAf (2022). sysfonts: Loading Fonts into R. R package version 0.8.8,\r\nhttps://CRAN.R-project.org/package=sysfonts.\r\nQiu Y, details. aotisSfAf (2023). showtext: Using Fonts More Easily in R Graphs. R package\r\nversion 0.9-6, https://CRAN.R-project.org/package=showtext.\r\nWickham H (2022). stringr: Simple, Consistent Wrappers for Common String Operations. R package\r\nversion 1.5.0, https://CRAN.R-project.org/package=stringr.\r\nWickham H (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN\r\n978-3-319-24277-4, https://ggplot2.tidyverse.org.\r\nWickham H, François R, Henry L, Müller K, Vaughan D (2023). dplyr: A Grammar of Data\r\nManipulation. R package version 1.1.2, https://CRAN.R-project.org/package=dplyr.\r\nWickham H, Henry L (2023). purrr: Functional Programming Tools. R package version 1.0.1,\r\nhttps://CRAN.R-project.org/package=purrr.\r\nWickham H, Hester J, Bryan J (2023). readr: Read Rectangular Text Data. R package version\r\n2.1.4, https://CRAN.R-project.org/package=readr.\r\nWickham H, Seidel D (2022). scales: Scale Functions for Visualization. R package version\r\n1.2.1, https://CRAN.R-project.org/package=scales.\r\nWickham H, Vaughan D, Girlich M (2023). tidyr: Tidy Messy Data. R package version 1.3.0,\r\nhttps://CRAN.R-project.org/package=tidyr.\r\nWilke C, Wiernik B (2022). ggtext: Improved Text Rendering Support for ‘ggplot2’. R package\r\nversion 0.1.2, https://CRAN.R-project.org/package=ggtext.\r\nXie Y (2023). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package\r\nversion 1.43, https://yihui.org/knitr/.\r\nXie Y (2015). Dynamic Documents with R and knitr, 2nd edition. Chapman and Hall/CRC, Boca\r\nRaton, Florida. ISBN 978-1498716963, https://yihui.org/knitr/.\r\nXie Y (2014). “knitr: A Comprehensive Tool for Reproducible Research in R.” In Stodden V, Leisch\r\nF, Peng RD (eds.), Implementing Reproducible Computational Research. Chapman and Hall/CRC.\r\nISBN 978-1466561595.\r\nXie Y, Allaire J, Grolemund G (2018). R Markdown: The Definitive Guide. Chapman and Hall/CRC,\r\nBoca Raton, Florida. ISBN 9781138359338, https://bookdown.org/yihui/rmarkdown.\r\nXie Y, Dervieux C, Riederer E (2020). R Markdown Cookbook. Chapman and Hall/CRC, Boca Raton,\r\nFlorida. ISBN 9780367563837, https://bookdown.org/yihui/rmarkdown-cookbook.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-02-10-drawing-a-zodiac-chart/images/eu_combo_zodiac.png",
    "last_modified": "2024-02-11T15:23:38+03:00",
    "input_file": {},
    "preview_width": 1653,
    "preview_height": 1062
  },
  {
    "path": "posts/2021-09-20-update-on-riro-project/",
    "title": "Update on RIRO project",
    "description": "RIRO v.1.2 is out! Here are just some personal reflections on RIRO project's development.",
    "author": [
      {
        "name": "Aleksei Lutai",
        "url": "https://www.linkedin.com/in/lutaya/"
      }
    ],
    "date": "2021-09-20",
    "categories": [
      "ror",
      "wikidata",
      "russian data",
      "organization identifier",
      "riro",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\n(1) Russian organizations\r\n(2) Scopus profiles\r\n(3) ROR\r\n(4) Wikidata\r\n\r\nRIRO stands for the Russian Index of Research Organizations - this is a web project matching the Russian research organizations with the official info and their profiles in ROR, Scopus, Wikidata, and other databases.\r\nThere is an official press release in English at the web site, so here I can focus only on the things that I find most crucial about the project.\r\n(1) Russian organizations\r\nStep by step the project grows and now includes 3214 unique (head only, no branches) Russian organizations. With the branches and predecessors there are more than 8000 records. The project is focused on public sector of science - i.e. on the organizations having web sites (do not expect to find any secret underground orc plants).\r\n(2) Scopus profiles\r\nWe matched more than 4000 Scopus profiles. More than 800 organizations have more than 1 Scopus profile. More interesting that 300+ profiles were present in July, but disappeared in August. Any researcher relying on the Scopus affiliation profiles should not forget about its constant evolution. Though the profiles are not stable, a list of matched RIRO profiles can be helpful if one has a data exported from SciVal. We used SciVal data on 2020 Russian publications, assigned it to the regions using RIRO, and analyzed a cooperation of the organizations in the bordering regions i.e. interregional collaboration.\r\nThe idea was pretty simple - the organizations produce the publications in different collaborative models. There are (1) intraregional articles, produced by a sole organization or by the organizations within one region, (2) double interregional collaborations where the organizations from 2 regions have found a reason to cooperate regardless of the distance and produced something, and (3) there are collaborations with 3 or more regions involved.\r\nThe figure below shows the tile map of Russian regions and for each region the number shows a contribution of the neighbour regions into the publication output produced by the double interregional (2 regions only) collaboration (i.e. no multicollaborations, no domestic, no intra-regional publications).\r\n\r\nShow code\r\nknitr::include_graphics(paste0(img_dir, \"/map_region.png\"))\r\n\r\n\r\n\r\n\r\nFigure 1: Figure from RIRO project https://openriro.github.io/posts/regional-collaboration/\r\n\r\n\r\n\r\nThis could be not an issue for many countries, but for Russia it is very interesting - some regions have very low direct collaboration with their neighbours.\r\n(3) ROR\r\nEverything I wrote about ROR 2 months ago, unfortunately, is still valid. Global brouhaha about the values is understood, the solution was long time expected, but there is still a list of questions to be answered - who will be updating the ROR? who will be updating the CrossRef to fix the incorrectly assigned RORs (like “Russian Academy of Sciences”)? What type of hierarchy & historical background will be reflected in ROR profiles?\r\n(4) Wikidata\r\nIn the last 2 months this is my favourite topic. I am impressed with a flexibility of Wikidata properties and thinking that every research organization or university should pay attention to its Wikidata profile. This is the only service that allows any organization to set the things right (history, hierarchy, relations, identifiers).\r\nIn the RIRO v.1.2. the table 5 lists over 1600 Wikidata items matched to Russian organizations. There’s lot of info yet to be added adn set up, but I already managed to parse the web sites and added into the profiles the social networks info - so there are 564 Facebook pages, 676 VKontakte pages, 149 Telegram, 205 Twitter, and 373 Youtube channels. I am also trying to set up the hierarchies, linking the organizations to the ministries, branches and predecessors.\r\nExample 1: Wikidata items for the organizations whose parent organization is the MInistry of Healthcare and their Facebook IDs.\r\n\r\n\r\nExample 2: Wikidata items for the organizations whose parent organization is the Ministry of Science and Higher Education that have coordinates (set in Wikidata). A piece of this map you can also see as a post preview picture (at the blog’s parent page).\r\n\r\n\r\nI do hope that the Wikidata info about the Russian organizations will be the most accurate source of information, and other service providers (like ROR, Scopus, the Lens) will be using the Wikidata at least for checking.\r\nIn order to attract more attention to Wikidata profiles, the RIRO team releases an article “Wikidata profile for the research organizations” (so far in Russian only).\r\nIf anyone is interested in this topic - how the research organizations are present in Wikidata - I would be glad to chat or share more from my experience.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-20-update-on-riro-project/images/leninsky.png",
    "last_modified": "2021-09-19T21:17:00+03:00",
    "input_file": {},
    "preview_width": 540,
    "preview_height": 309
  },
  {
    "path": "posts/2021-07-27-new-release-of-riro-is-here/",
    "title": "New Release of RIRO is here",
    "description": "RIRO is a Russian Index of Research Organizations and here I am writing about (briefly) what this project is by its 1.1 version.",
    "author": [
      {
        "name": "Aleksei Lutai",
        "url": "https://www.linkedin.com/in/lutaya/"
      }
    ],
    "date": "2021-07-26",
    "categories": [
      "ror",
      "russian data",
      "organization identifier",
      "riro",
      "r"
    ],
    "contents": "\r\n\r\nContents\r\n1. Web site\r\n2. Coverage\r\n3. New Org ID type\r\n4. Quick links to Zenodo\r\n5. Use Case - ROR and RIRO\r\nNext steps\r\nAcknowledgments\r\n\r\nAs promised, briefly, in “5 things you need to know about…” format.\r\n1. Web site\r\nWith its 1.1 release RIRO project has got a web site https://openriro.github.io/.\r\n\r\nShow code\r\nknitr::include_graphics(paste0(img_dir, \"site.png\"))\r\n\r\n\r\n\r\n\r\nThis is another Distill-driven online site for RIRO official releases, use cases & code examples. The RIRO releases are both in English and in Russian.\r\n2. Coverage\r\nThe current version of RIRO covers 2818 parent organizations (universities, research centres, regional hospitals, etc) - together with branches and predecessors it is over 8000 entities.\r\n\r\nShow code\r\nknitr::include_graphics(paste0(img_dir, \"chart_upset_v1.1.eng.png\"))\r\n\r\n\r\n\r\n\r\n3. New Org ID type\r\nWe have a long expected newcomer - eLIBRARY organization identifier. eLIBRARY is the largest Russian aggregator of scholarly publications, so their index of organizations is almost 15000 names. We matched 1827 largest accounts and publish it in Table 12.\r\nSo far eLIBRARY offers no free/freemium API to use this ID, so if you are not a subscriber to their special services, the value of these IDs is not high.\r\nWell, one can open organization profile on eLIRBARY.ru web site using the ID. The picture below shows the profile of Kaluga State University named after K. E. Tsiolkovski (ID = 1052).\r\n\r\nShow code\r\nknitr::include_graphics(paste0(img_dir, \"elibrary.png\"))\r\n\r\n\r\n\r\n\r\n4. Quick links to Zenodo\r\nAll versions of RIRO dataset (CSV tables) are available in Zenodo community, one can easily get it via OAI-PMH Harvesting API or by using REST API. On assumption that somebody is not willing to use Zenodo APIs we decided to publish the direct URLs to RIRO CSV files (always the latest version) here: https://openriro.github.io/latest_riro_links.csv\r\n\r\nShow code\r\nread_csv(\"https://openriro.github.io/latest_riro_links.csv\") %>% \r\n  mutate(download = paste0('<a href=\\\"',download,'\\\" target=\\\"_blank\\\">',download,'<\/a>')) %>% \r\n  datatable(rownames = FALSE, escape = FALSE,\r\n            options = list(pageLength = 4, deferRender = FALSE,\r\n                           dom = \"Brtip\",  autoWidth = FALSE))\r\n\r\n\r\n\r\n\r\nTo cite RIRO dataset (without version):\r\nSterligov, Ivan, & Lutay, Aleksei. (2021). Russian Index of the Research Organizations (RIRO) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.4775290\r\n5. Use Case - ROR and RIRO\r\nROR and GRID records for the Russian organizations are confusing, because they are based on outdated sources. Like this Russian Academy of Sciences account that lists both parent orgs, branchs, territorial divisions, subject departments, and many others who are no longer with RAS.\r\nMany relations currently present in ROR are of specific, non-judicial nature. One institution can belong to the Mathematical Division of RAS, to Ural Branch of RAS, to the Federal Research Center. These relations are of different nature:\r\nbeing a part of RAS Math Division is mostly about who votes for whom on RAS elections, and how the results are packed into the RAS Annual Reports. Little funding is behind these relations and no legal responsibility.\r\nbeing a part of the Ural or Siberian Branch is a bit about money that RAS bracnhes are provided (tiny part of total R&D budget) and no responsibility compared to the relations between the preant organization and its subsidiary.\r\nbeing part of the regional research center can be different. In some cases the institutions are the subsidiaries of the federal research center (the branches). In other cases the institutions bear the historic name referring to the regional RAS center, but are totally independent in legal way.\r\nIs it important to keep the hierarchy of RAS subject divisions and territorial branches?\r\nWell, only if the relations are clearly labelled and defined. Without labelling the relations are mixed with the real subsidiary-based relations and can be misleading.\r\nRIRO data can help to this.\r\nROR is one of the identifiers present in RIRO. We have matched 1210 ROR IDs related to the Russian Federation (ROR Dataset v.9 https://doi.org/10.6084/m9.figshare.c.4596503.v9) to the RIRO identifiers - not all, but a majority of those that are state-owned and public.\r\n\r\nShow code\r\nror <- list.dirs(paste0(dir, \"/final_tables/\"), recursive =  FALSE) %>% \r\n  sort(., decreasing = TRUE) %>% .[grepl(\"1.1.1\",.)] %>% \r\n  list.files(full.names = TRUE) %>% \r\n  .[grepl(\"table4_\",.)] %>% \r\n  read_csv(col_types = cols(.default = col_character()))\r\n\r\nror %>%  datatable(rownames = FALSE, filter = \"none\", \r\n            escape = FALSE, class = \"row-border\", \r\n            options = list(columnDefs = list(\r\n              list(width = '250px', targets = c(3:4)),\r\n              list(width = '400px', targets = c(5,8)))))\r\n\r\n\r\n\r\n\r\nIn RIRO the relations between ROR accounts are packed in compact strings. In the code below I will unpack the strings and build a network to see an hierarchy of ROR Russian accounts and to match them against the relationships present in RIRO.\r\n\r\nShow code\r\nror_net <- ror %>% \r\n  select(ror_id, ror_name, ror_relationships) %>%\r\n  filter(nchar(ror_relationships)>2) %>% \r\n  separate(ror_relationships, into = c(\"label\", \"type\", \"id\"),  sep = \"\\\\|\") %>% \r\n  mutate_at(c(\"label\", \"type\", \"id\"), ~str_replace(.x, \"^[^:]+:\",\"\")) %>% \r\n  mutate_at(c(\"label\", \"type\", \"id\"), ~gsub(\"^c|\\\\(|\\\\)\",\"\", .x)) %>%  \r\n  mutate_at(c(\"label\", \"type\", \"id\"), ~str_extract_all(.x, '\".+?\"')) %>% \r\n  unnest(c(\"label\", \"type\", \"id\")) %>% \r\n  mutate_all(~gsub('\"', '', .x)) %>% \r\n  mutate(id = str_extract(id, \"(?<=/)0.+$\")) %>% \r\n  select(ror_id, ror_name, type, id, label) %>% \r\n  mutate_all(~str_squish(.x))\r\n\r\ng_ror <- ror_net %>% \r\n  select(from = ror_id, to = id) %>% \r\n  graph_from_data_frame(directed = FALSE)\r\n\r\nsummary(g_ror)\r\n\r\n\r\nIGRAPH 3581c45 UN-- 135 202 -- \r\n+ attr: name (v/c)\r\n\r\nSo the hierarchy of the Russian ROR IDs comprises of 135 organizations connected by 202 relations.\r\n\r\nShow code\r\ng_ror %>% as_tbl_graph() %>% \r\n  mutate(node_degree = degree(.)) %>% \r\n  ggraph(layout = \"stress\", bbox = 10) + \r\n  geom_edge_link0(alpha = 0.5) + \r\n  geom_node_point(aes(x = x, y = y, size = node_degree), \r\n                  fill = \"lightblue\", shape = 21, alpha = 0.9)+\r\n  scale_size_continuous(range = c(2,8), name = \"Degree\")+\r\n  labs(title = \"Hierarchy of Russian ROR accounts\", \r\n       subtitle = \"based on 1210 ROR Russian accounts match against RIRO\", \r\n       caption = \"dwayzer.netlify.app\")+\r\n  theme_graph()\r\n\r\n\r\n\r\n\r\nAs stated earlier, the subject and territorial subdivisions of the Russian Academy of Sciences play central roles in this network.\r\nIn RIRO the Table 3 lists the hierarchical relations between the parent orgs and their subsidiaries, and also with the predecessors. Use of predecessors helps to deal with the accounts that ceased to exist after merger, but still present in the foreign registries of Org IDs (like ROR, Scopus, etc).\r\n\r\nShow code\r\nriro <- list.dirs(paste0(dir, \"/final_tables/\"), recursive =  FALSE) %>% \r\n  sort(., decreasing = TRUE) %>% .[grepl(\"1.1.1\",.)] %>% \r\n  list.files(full.names = TRUE) %>% \r\n  .[grepl(\"table3_\",.)] %>% \r\n  read_csv(col_types = cols(.default = col_character())) \r\n\r\n\r\n\r\nLet’s build a RIRO network where the nodes are the organizations matched to ROR. In other words we will use RIRO for linking the ROR accounts and build a network where every node corresponds to ROR account, and a proved relation (subsidiary or predecessor) is behind every edge.\r\n\r\nShow code\r\nriro_net <- riro %>% \r\n  left_join(ror %>% select(code, ror_id) %>% distinct()) %>% \r\n  left_join(ror %>% select(child_code = code, ror_id2 = ror_id) %>% distinct()) %>% \r\n  select(ror_id, relation, ror_id2) %>% distinct() %>% na.omit() \r\n\r\ng_riro <- riro_net %>% \r\n  select(from = ror_id, to = ror_id2) %>% \r\n  graph_from_data_frame(directed = FALSE) \r\n\r\nsummary(g_riro)\r\n\r\n\r\nIGRAPH 3a08b37 UN-- 195 123 -- \r\n+ attr: name (v/c)\r\n\r\nSuch network has 195 nodes and 123 edges. Over 15%+ of ROR accounts that we matched to RIRO has some relations (in juridicial way, like parent and subsidiary entities). Attributions to the Ministries or RAS are not included here.\r\n\r\nShow code\r\ng_riro %>% as_tbl_graph() %>% \r\n  mutate(node_degree = degree(.)) %>% \r\n  ggraph(layout = \"nicely\") + \r\n  geom_edge_link0(alpha = 0.5) + \r\n  geom_node_point(aes(x = x, y = y, size = node_degree), fill = \"coral\", shape = 21, alpha = 0.9)+\r\n  scale_size_continuous(range = c(2,4), name = \"Degree\")+\r\n  labs(title = \"Hierarchy of RIRO accounts matched to ROR\", \r\n       subtitle = \"based on 1210 ROR Russian accounts matched against RIRO\", \r\n       caption = \"dwayzer.netlify.app\")+\r\n  theme_graph()\r\n\r\n\r\n\r\n\r\nThe critical part here is that these 2 graphs have a tiny intersection - there are just 3 shared relations.\r\n\r\nShow code\r\ng1 <- g_riro %s% g_ror\r\nprint_all(g1)\r\n\r\n\r\nIGRAPH 3bfd225 UN-- 305 3 -- \r\n+ attr: name (v/c)\r\n+ edges from 3bfd225 (vertex names):\r\n[1] 00g4bcb66--006knem90 01jkd3546--00bg9m975 01fz81h65--02sp4ja91\r\n\r\nIn other words:\r\n198 out of 202 relations (that we built based on ROR data) do not exist in RIRO (because of specific nature of those relations)\r\njust 3 out of 123 proven relations present in RIRO are also present in ROR.\r\nThe chart below shows 2 networks joint on share plot. The shared nodes (present in both networks are shown in violet). The 3 shared edges are exactly those that connect the violet nodes (not marked with color).\r\n\r\nShow code\r\ngraph_join(g_riro %>% as_tbl_graph(), \r\n           g_ror %>% as_tbl_graph()) %>% \r\n  mutate(color = ifelse(name %in% V(g_ror)$name, \"lightblue\", \"coral\")) %>% \r\n  mutate(color = ifelse(name %in% V(g_ror)$name & name %in% V(g_riro)$name, \"violet\", color)) %>% \r\n  mutate(node_degree = degree(.)) %>% \r\n  ggraph(layout = \"nicely\") + \r\n  geom_edge_link0(alpha = 0.5) + \r\n  geom_node_point(aes(x = x, y = y, size = node_degree, fill = color), shape = 21, alpha = 0.9)+\r\n  scale_size_continuous(range = c(1.5,5), name = \"Degree\")+\r\n    labs(title = \"Hierarchy of ROR and RIRO accounts\", \r\n       subtitle = \"based on 1210 ROR Russian accounts matched against RIRO\", \r\n       caption = \"dwayzer.netlify.app\")+\r\n  scale_fill_manual(labels = c(\"lightblue\" = \"in ROR only\", \r\n                                 \"coral\" = \"in RIRO only\", \r\n                               \"violet\" = \"in both RIRO and ROR\"),\r\n                    values = c(\"lightblue\" = \"lightblue\", \r\n                                 \"coral\" = \"coral\", \"violet\" = \"violet\"), \r\n                    name = \"Accounts with\\nhierarchical relations\")+\r\n  guides(\"fill\" = guide_legend(override.aes = list(size = 3), order = 1))+\r\n  theme_graph()\r\n\r\n\r\n\r\n\r\nNext steps\r\nThe RIRO roadmap is still in the air, but we will certainly try to\r\ncooperate with ROR\r\npay more attention to Wikidata\r\npublish more use cases.\r\nAcknowledgments\r\nAllaire J, Xie Y, McPherson J, Luraschi J, Ushey K, Atkins A, Wickham H, Cheng J, Chang W, Iannone R (2021). rmarkdown: Dynamic Documents for R. R package version 2.7, <URL: https://github.com/rstudio/rmarkdown>.\r\nCsardi G, Nepusz T (2006). “The igraph software package for complex network research.” InterJournal, Complex Systems, 1695. <URL: https://igraph.org>.\r\nGagolewski M (2020). R package stringi: Character string processing facilities. <URL: http://www.gagolewski.com/software/stringi/>.\r\nHenry L, Wickham H (2020). purrr: Functional Programming Tools. R package version 0.3.4, <URL: https://CRAN.R-project.org/package=purrr>.\r\nPedersen T (2021). ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. R package version 2.0.5, <URL: https://CRAN.R-project.org/package=ggraph>.\r\nPedersen T (2020). tidygraph: A Tidy API for Graph Manipulation. R package version 1.2.0, <URL: https://CRAN.R-project.org/package=tidygraph>.\r\nWickham H (2020). tidyr: Tidy Messy Data. R package version 1.1.2, <URL: https://CRAN.R-project.org/package=tidyr>.\r\nWickham H (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=stringr>.\r\nWickham H, Francois R, Henry L, Muller K (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.3, <URL: https://CRAN.R-project.org/package=dplyr>.\r\nWickham H, Hester J (2020). readr: Read Rectangular Text Data. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=readr>.\r\nXie Y (2020). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.30, <URL: https://yihui.org/knitr/>.\r\nXie Y (2015). Dynamic Documents with R and knitr, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, <URL: https://yihui.org/knitr/>.\r\nXie Y (2014). “knitr: A Comprehensive Tool for Reproducible Research in R.” In Stodden V, Leisch F, Peng RD (eds.), Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595, <URL: http://www.crcpress.com/product/isbn/9781466561595>.\r\nXie Y, Allaire J, Grolemund G (2018). R Markdown: The Definitive Guide. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9781138359338, <URL: https://bookdown.org/yihui/rmarkdown>.\r\nXie Y, Cheng J, Tan X (2021). DT: A Wrapper of the JavaScript Library ‘DataTables’. R package version 0.17, <URL: https://CRAN.R-project.org/package=DT>.\r\nXie Y, Dervieux C, Riederer E (2020). R Markdown Cookbook. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9780367563837, <URL: https://bookdown.org/yihui/rmarkdown-cookbook>.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-27-new-release-of-riro-is-here/images/ror_network.png",
    "last_modified": "2021-09-19T17:24:13+03:00",
    "input_file": {},
    "preview_width": 784,
    "preview_height": 474
  },
  {
    "path": "posts/2021-07-02-importing-author-information-from-crossref-to-wikidata-via-quickstatements/",
    "title": "Importing Author Information from CrossRef to Wikidata via QuickStatements",
    "description": "The post continues my quest to improve the presence of academic journal(s) in Wikidata. I reviewed the different searching approaches to find the authors, especially those with with non-English names, in Wikidata and upload the author metadata via QuickStatements.",
    "author": [
      {
        "name": "Aleksei Lutai",
        "url": "https://www.linkedin.com/in/lutaya/"
      }
    ],
    "date": "2021-07-05",
    "categories": [
      "crossref",
      "wikidata",
      "r",
      "quickstatements",
      "sparql",
      "author metadata"
    ],
    "contents": "\r\n\r\nContents\r\nWikidata for Academic Journal\r\nEditing Wikidata\r\nData\r\nFinding the articles in Wikidata\r\nAuthor Information in Wikidata\r\nFinding Authors in Wikidata by PIDs\r\nText Search in Wikidata\r\nWikibase API: wbsearchentities\r\nWikibase API: query&list=search\r\nSearch Strategies\r\nStep 1. Finding Given Names in Wikidata\r\nStep 2. Building the Name Combinations\r\nStep 3. Finding authors in Wikidata (by name)\r\nUploading the Author Info into Wikidata\r\nFinal Remarks (to the journal editors)\r\nAcknowledgments\r\n\r\nIn a previous post I was trying to do what is called an author disambiguation using the open metadata from CrossRef, Microsoft Academic and ORCID. I took 10 DOIs as an example and identified the ORCIDs for a number of authors. This initial idea was also to upload the author metadata info to Wikidata in the same post.\r\nWhile I was working on it I realized that I completely lost a sense of smell. I decided to publish the previous part and postpone exercising with Wikidata till better days. I was lucky, it was not as bad as it is said it could be - a week later my only discomfort is that I still do not know the smells.\r\nSo, I am back, full of forces, and in this post I am going to prepare and uploade the author metadata to Wikidata.\r\nWikidata for Academic Journal\r\nYou may be wondering why to bother about the journal’s standing in Wikidata, when there are many citation indices and A&I databases that present the metadata. Apologies for not having a detailed and well-argumented intro for such a question. Instead, I would give you few examples that I find illustrative:\r\nthis is how the online service Scholia shows the journal Atmospheric Chemistry and Physics and the article The Alzheimer’s disease-associated amyloid beta-protein is an antimicrobial peptide, based on the Wikidata. If the journals and articles exists as Wikidata items and contain the metadata, the items can be cited in Wikipedia articles or be linked to other non-journal Wikidata items like author biographical facts, events, awards, organizations, topics, molecules, news, products, you name it. Altogether this forms a huge knowledge graph, absolutely incomparable with the citation/search engines we used to. This is the only way to highlight the journal impact in multiple facets, to move beyond the citation obsession.\r\nhere is a collection of Wikidata queries prepared by Martin Poulter, demonstrating how the linked items can be investigated. There are more high-brow collections like this one with biomedical queries. You can run yourself any of those examples by clicking Try it! or Execute url links next to the SPARQL snippets.\r\nBut not every journal looks this great in Wikidata to benefit from semantic blessing. In one of my previous posts I described how the Russian academic journals look in Wikidata and Wikipedia.\r\nMore argumentation on Wikidata’s value for the academic journals can be found in the original works Google Scholar: Wikidata + Journal.\r\nEditing Wikidata\r\nHow the Wikidata contents can be added or improved? There are few ways:\r\nmanually. This is plain. You need to register to Wikidata, find the journal record and start copy/pasting the metadata bits.\r\nwith a help of tools, but still manually. There are some great tools like Mix'n'match, Author Disambiguator, Wikidata Link Reconciler and many others (listed here). Some of it requries of one to be an Autoconfirmed user, which is a special status you get after having edited a certain number of items (a meritocracy in action if you will).\r\nautomatically, with API-based tools.\r\nIn this post I am going to use the most popular tool named QuickStatements. This tool is fantastic, as it allows you to create/edit the Wikidata items by submitting the materials in very simple formats like CSV/TSV or a URL-encoded string. Like many other Wiki tools it was developed by Magnus Manske.\r\nYou will not be able to submit the materials in batches via QuickStatements until you get the status of Autoconfirmed User. But take it as a hint - running QuickStatements through URL does not require the status, so you can start with it to reach the status and then switch to the batches. Or you can just deal with Wikimedia API directly. In this post I am going to describe only the batch uploading available for autoconfirmed users.\r\nThe good practice is to:\r\ntest your editing approaches on Wikidata Sandbox,\r\nalways check what you have done,\r\nalways fix the errors you made.\r\nThis discipline can seem very annoying to some creative minds, but this is a public tool, it will be spoiled without the rules.\r\nData\r\nIn previous post I took 10 DOIs and made few manipulations to obtain the ORCIDs for some of the authors (see below). The CRF in the column names stands for CrossRef (as a source), MAG - for Microsoft Academic.\r\n\r\nShow code\r\nfinal_data_checked <- paste0(dir, \"/final.data.xlsx\") %>% \r\n  readxl::read_xlsx(col_types = \"text\") %>% \r\n  filter(score!=\"D\") %>% \r\n  arrange(doi, score) \r\n\r\nfinal_data_checked %>% \r\n  mutate(orcid = ifelse(score==\"A\", orcid, NA_character_)) %>% \r\n  select(doi, order, orcid, `CRF family`, `CRF given`, \r\n         Scopus_AuthID, ResearcherID, `MAG Author ID`) %>%  \r\n  DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: see in the text.'),\r\n    options = list(searchHighlight = TRUE, dom = 'Brtip',\r\n                   autoWidth = FALSE,\r\n                columnDefs = list(\r\n                  list(width = '150px', targets = c(0,2)),\r\n                  list(className = 'dt-center', targets = c(1)))\r\n                  ))\r\n\r\n\r\n\r\n\r\nBut we can not submit DOI-ORCID links to Wikidata. The process needs to be organizaed a bit different:\r\nwe have check if the article are present in Wikidata (searching by DOI) and what metadata are available\r\nnext we should check for every author if it is present in Wikidata as an item (searching by ORCID, Scopus Author ID, Researcher ID, Microsoft Academic ID, and also by the author’s name)\r\nonce we have collected the details about article and author, we can submit one of 2 statements connecting those details:\r\nwikidata item corresponding to the article (Q….) has an author (P50), which is Wikidata item (Q….), corresponding to the author.\r\nwikidata item corresponding to the article (Q….) has an author string (P2093), which is not a Wikidata item, but a plain text.\r\nThe statements above are simplified a bit, the Wikidata rules require the statements to be supported with the references and some specific qualifiers (statements for additional properties). The author information for the scholarly article needs to be backed with a source (i.e. CrossRef) and should have an ordinal. It may also have other properties like affiliation or affiliation string.\r\nFinding the articles in Wikidata\r\nThe package WikidataR, still in development, provides very practical options to work with Wikidata. We will use its function qid_from_identifier to find Wikidata by DOI.\r\n\r\nShow code\r\ndois_file <- paste0(dir, \"dois_wiki.csv\")\r\n\r\nif(!file.exists(dois_file)){\r\n  dois <- final_data_checked %>% select(doi) %>% distinct()\r\nfor (i in 1:nrow(dois)){\r\n  print(i)\r\n  res <- qid_from_identifier(property = \"DOI\", value = dois$doi[i]) %>% \r\n    unnest(qid)\r\n  dois[i, \"wikidata\"] <- ifelse(nrow(res)==1,  as.character(res$qid[1]), \"\") \r\n  }\r\n  write_excel_csv(dois, dois_file)\r\n} else {\r\n  dois <- read_csv(dois_file)\r\n}\r\ndois %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org.'),\r\n    options = list(searchHighlight = TRUE, dom = 'Brtip',\r\n                   autoWidth = FALSE))\r\n\r\n\r\n\r\n\r\nThe publications are already present in Wikidata.\r\nLet’s check of they already have anything about the authors. We can collect the author information using another function of WikidataR named get_item.\r\nAuthor Information in Wikidata\r\nThe code below obtains all the claims of the Wikidata item and extracts those that we are interested in:\r\nP50 (is author of) - statements linking the articles to the persons exisisting in Wikidata\r\nP2093 (author name string) - statements assigning the autor name string to the article\r\nP1545 (series ordinal) - a qualifier showing the author’s place in the author list\r\nP248 (stated in) - statements used as the references (for the statements) pointing at a source of information. I suspect that there could be other properties used as references (maybe P3452 - “inferred from”), but this does not matter in our case. If the author data is supported by strange reference, we will re-write it and refer to CrossRef as a source of metadata.\r\n\r\nShow code\r\nwd_auth_info_file <- paste0(dir, \"wikidata_author_existing_details.csv\") \r\n\r\nif(!file.exists(wd_auth_info_file)){\r\n  wd_pub_existing_data <- c()\r\n  \r\n  for (i in 1:nrow(dois)){\r\n    b <- get_item(dois$wikidata[i]) %>% map(pluck, \"claims\")\r\n    b <- b[[1]][c(\"P2093\", \"P50\")]\r\n    \r\n    wd_pub_auth_data <- cbind(\r\n      #1 part. author strings or Wiki items\r\n      b %>% map(~{.x[[\"mainsnak\"]][\"datavalue\"] %>% map_df(\"value\") %>% \r\n          select(any_of(c(\"author_wiki\" = \"id\", \r\n                          \"author_wiki\" = \"datavalue\")))}) %>% \r\n        map_df(~.x) %>% \r\n        mutate(author_wiki_type = ifelse(grepl(\"^Q\\\\d+\",author_wiki), \r\n                                         \"P50\", \"P2093\")),\r\n      #2 part. author statement IDs\r\n      b %>% map(~{.x[[\"id\"]]}) %>% unlist(use.names = FALSE) %>% \r\n        enframe(name = NULL, value = \"Statement_ID\"),\r\n      #3 part. author order qualifiers\r\n      b %>% map_df(~{.x[[\"qualifiers\"]][[\"P1545\"]] %>% \r\n            map_df(\"datavalue\")}) %>% \r\n            select(any_of(c(\"order\" = \"value\"))) %>% \r\n            mutate(qualif_prop = \"P1545\"),\r\n      #4 part. references/sources\r\n      b %>% map_df(~{.x[[\"references\"]] %>% \r\n            map_df(pluck, \"snaks\", \"P248\") %>% \r\n            map_df(list(\"value\",\"id\"))}) %>%\r\n        select(source = datavalue) %>%\r\n        mutate(source_prop = \"P248\")\r\n    ) %>% \r\n      mutate(qid = dois$wikidata[i]) %>% \r\n      relocate(qid)\r\n    \r\n    wd_pub_existing_data <- wd_pub_existing_data %>% bind_rows(wd_pub_auth_data)\r\n    print(i)\r\n  } \r\n  write_excel_csv(wd_pub_existing_data, wd_auth_info_file)\r\n } else {\r\n  wd_pub_existing_data <- read_csv(wd_auth_info_file, \r\n                                   col_types = cols(.default = col_character()))\r\n } \r\n\r\nwd_pub_existing_data %>% \r\n  relocate(Statement_ID, .after = source_prop) %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org.'),\r\n     options = list(searchHighlight = TRUE, dom = 'Brtip',\r\n                   autoWidth = TRUE,\r\n                columnDefs = list(\r\n                  list(width = '430px', targets = c(7)),\r\n                   list(width = '150px', targets = c(1)),\r\n                  list(className = 'dt-center', targets = c(2,3,4,5,6)))\r\n                  ))\r\n\r\n\r\n\r\n\r\nThis is how the author relations look like:\r\na Wikidata item corresponding to the article (Q107266290) has a text (Kalendzhyan S.O.) as an author name string (P2093), its position amoung authors (P1545) is 1, and we should trust this info as it is stated in (P248) Scopus (Q371467). This relation is stored as a statement with number Q107266290$77458B66-…-8A7DB0DB875D.\r\nOr, in cases when the author exists as Wikidata item and linked to the Wikidata item for the article, the relation look like:\r\na Wikidata item corresponding to the article (Q107266359) has an author (P50), which is a Wikidata item (Q93427896), corresponding to a person, which place in the authors list (P1545) is 5, as stated in (P248) in CrossRef (Q5188229). This relation is stored as a statement with number Q107266359$1E953F5D-…-750C1A9627B6.\r\nAre we satisfied with the author names present in Wikidata as text (authorname string)? Of course, we should not be. With text strings for the authors Wikidata is just like any other A&I database. To see a magic of Wiki, we have to substitute the author name strings (defined by P2093) with the Wikidata items corresponding to the persons (defined by P50). With those authors our analytical reach will expand beyond the names, as we will be able to analyze the relations of the authors (known to Wikidata).\r\nLet’s search the Wikidata items corresponding to the authors - for this we can use the author’s name and the personal identificators (PID).\r\nFinding Authors in Wikidata by PIDs\r\nFirst, we will try to find the Wikidata items by PIDs. In order to do that I will take the initital data and filter out the authors for which I earlier found the ORCIDs and other PIDs.\r\n\r\nShow code\r\nzz1 <- final_data_checked %>% \r\n    filter(score==\"A\") %>% \r\n    select(doi, order, orcid, Scopus_AuthID, ResearcherID, `MAG Author ID`) %>% \r\n    filter(!is.na(orcid)) %>% \r\n    mutate_at(c(\"Scopus_AuthID\", \"ResearcherID\"), ~str_split(.x, \"\\\\|\")) %>%\r\n    unnest(c(\"Scopus_AuthID\", \"ResearcherID\")) %>% \r\n    pivot_longer(-c(\"doi\", \"order\"), names_to = \"src\", values_to = \"id\") %>%\r\n    mutate_all(~str_squish(.x)) %>% \r\n    filter(!is.na(id))\r\n\r\nzz1 %>%  DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org.'),\r\n    options = list(searchHighlight = TRUE, dom = 'Brtip',\r\n                   autoWidth = FALSE))\r\n\r\n\r\n\r\n\r\nLet’s search them one by one in Wikidata using qid_from_identifier function from WikidataR package with the appropriate property for each PID type.\r\n\r\nShow code\r\nwd_authors_by_ids_file <- paste0(dir, \"wd_authors_by_ids.csv\")\r\n\r\nif(!file.exists(wd_authors_by_ids_file)){\r\n  for (i in 1:nrow(zz1)){\r\n    if(zz1$src[i]==\"orcid\"){\r\n      res <- qid_from_identifier(property = \"ORCID iD\", \r\n                                 value = zz1$id[i]) %>% unnest(qid)\r\n      }\r\n    if(zz1$src[i]==\"MAG Author ID\"){\r\n      res <- qid_from_identifier(property = \"Microsoft Academic ID\", \r\n                                 value = zz1$id[i]) %>% unnest(qid)\r\n    }\r\n    if(zz1$src[i]==\"Scopus_AuthID\"){\r\n      res <- qid_from_identifier(property = \"Scopus author ID\", \r\n                                 value = zz1$id[i]) %>% unnest(qid)\r\n    }\r\n    if(zz1$src[i]==\"ResearcherID\"){\r\n      res <- qid_from_identifier(property = \"ResearcherID\", \r\n                                 value = zz1$id[i]) %>% unnest(qid)\r\n    }\r\n    zz1[i, \"wikidata\"] <- ifelse(nrow(res)==1, \r\n                                 as.character(res$qid[1]), \"unusual_response\") \r\n    print(i)\r\n  }\r\n  \r\n  zz1 <- zz1 %>% select(doi, order, wikidata) %>% distinct() %>% na.omit()\r\n  \r\n  write_excel_csv(zz1, wd_authors_by_ids_file)\r\n} else { \r\n  zz1 <- read_csv(wd_authors_by_ids_file, col_types = cols(.default = col_character()))\r\n}\r\n\r\nzz1 %>% DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org.'),\r\n    options = list(searchHighlight = TRUE, dom = 'Brtip',\r\n                   autoWidth = FALSE))\r\n\r\n\r\n\r\n\r\nWe found nothing.\r\nI know that some of our authors are 100% present in Wikidata, but their items do not have the statements linking the items to PIDs like ORCID, Scopus Author ID, etc, therefore, we can not find them using PIDs. This is just our case, many researchers have Wikidata items with PIDs.\r\nOur last chance in this situation is to search by name.\r\nLet’s prepare the names.\r\n\r\nShow code\r\nzz2 <- final_data_checked %>% \r\n  # anti_join is for cases when something is found by PIDs \r\n  anti_join(zz1, by = c(\"doi\", \"order\")) %>% \r\n  select(doi, order, `CRF family`, `CRF given`, `MAG family`, `MAG given`) %>% \r\n  mutate_at(3:6, ~str_to_title(str_extract(.x, \"^[^\\\\s]+\"))) %>% \r\n  pivot_longer(c(\"CRF family\", \"CRF given\", \r\n                  \"MAG family\", \"MAG given\"), \r\n                names_to = c(\"source\", \"name_type\"), \r\n                names_pattern = \"(.+) (.+)\", \r\n                values_to = \"name\") %>% \r\n  distinct() %>% \r\n  pivot_wider(names_from = name_type, values_from = name) %>%\r\n  filter(!is.na(family)) %>% \r\n  select(-source) %>% distinct()\r\n\r\nzz2 %>% DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org.'),\r\n    options = list(searchHighlight = TRUE, dom = 'Brtip',\r\n                   autoWidth = FALSE))\r\n\r\n\r\n\r\n\r\nBefore we start looking up these names in Wikidata, let me share with you what I learned about a text search in Wikidata.\r\nText Search in Wikidata\r\nSearching Wikidata items by text is tricky. If you are interested to know more about it, I can recommend you an article Running a reconciliation service for Wikidata by Antonin Delpeuch, where the author explains the reasons for using separately 2 search APIs:\r\n?action=wbsearchentities\r\n?action=query&list-search\r\nBelow is my experience with both.\r\nWikibase API: wbsearchentities\r\nThis API is cool and simple, but you may be surprized with a variety of results that are totally irrelevant to what you had in mind.\r\nFor example, searching for a prominent Soviet physicist Sergey Vavilov via wbsearchentities (try API query, JSON result produces 2 items for the human beings and 1 for the research institute named after S.I.Vavilov.\r\n\r\nShow code\r\nfromJSON(\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search=sergey%20vavilov&language=en&format=json\", flatten = TRUE) -> a  \r\n\r\na %>% pluck(\"search\") %>% \r\n  select(title, label, description) %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org.'),\r\n    options = list(searchHighlight = TRUE, dom = 'Brtip',\r\n                   autoWidth = FALSE))\r\n\r\n\r\n\r\n\r\nOne strategy of making this search more specific is to query the API via SPARQL and apply some filters. For instance, we can limit the results to be only the humans by filtering the items having a property Q5 (human).\r\nA SPARQL request below will return only 2 results, omitting that corresponding to the organization.\r\nSELECT ?person ?personlabel_en ?persondesc_en\r\n   WHERE {hint:Query hint:optimizer \"None\".\r\n      SERVICE wikibase:mwapi {\r\n          bd:serviceParam wikibase:endpoint \"www.wikidata.org\";\r\n             wikibase:api \"EntitySearch\";\r\n              mwapi:search \"Sergey Vavilov\"; \r\n              mwapi:language \"en\".\r\n          ?person wikibase:apiOutputItem mwapi:item.\r\n      }\r\n      FILTER BOUND (?person)       \r\n        ?person wdt:P31/wdt:P279* wd:Q5.\r\n      optional{?person rdfs:label ?personlabel_en . FILTER(lang(?personlabel_en)='en')}\r\n      optional{?person schema:description ?persondesc_en . FILTER(lang(?persondesc_en)='en')}\r\n      SERVICE wikibase:label {bd:serviceParam wikibase:language \"en\".}\r\n}\r\nCheck it!\r\nI used this approach in one of my recent posts about matching the scientific terms to Wikidata items, filtering only those results that are most likely to be the scientific terms.\r\nQuerying wbsearchentities API via SPARQL with additional filters provides a lot of flexibility, but… the approach has one serious limitation - the API searches the entity labels or aliases for a perfect match. This can be a problem with the non-English names that often have ambiguous Latin spelling. The name Sergey can also be written as Sergei (or also as Sergej, Serhii, etc). People whose Wikidata item contains Sergei will not be found by a query with Sergey, unless the alternative labels are present in the Wikidata item. Searching for Sergei Vavilov will return only one person. I did not find a way to make a fuzzy search for wbsearchentities API. But the other API allows it.\r\nWikibase API: query&list=search\r\nThe Help page for this API is minimal, the more detailed information about the available parameters, use of regular expressions, fuzzy search is available at Cirrus Search page. Be warned, it is a brain-boiling stuff.\r\nThis API allows you to do various searches:\r\nsrsearch=Sergey Vavilov (AND) - 14 results\r\nsrsearch=“Sergey Vavilov” (quoted for strict match) - 5 results\r\nsrsearch=Serge? Vavilov (for 1-letter variations in the end of the given name) - 31 results\r\nsrsearch=Serg* Vavilov (for zero or any length variations in the end of the given name) - 35 results\r\nsrsearch=Sergey Vavilov~ (for fuzzy search) - 34 results\r\nsrsearch=“Sergey Vavilov”~1 (for fuzzy search with no more than 1 word between Sergey and Vavilov) - 8 results\r\nsrsearch=“Sergei Vavilov” OR “Sergey Vavilov” (read the page about Logical Operators).\r\nLet’s see the results of the strict match search:\r\n\r\nShow code\r\nfromJSON(\"https://www.wikidata.org/w/api.php?action=query&list=search&srsearch=%22sergey%20vavilov%22&format=json&srlimit=20\", flatten = TRUE) -> a  \r\n\r\na %>% pluck(\"query\") %>% pluck(\"search\") %>% \r\n  select(title, snippet) %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org.'),\r\n    options = list(searchHighlight = TRUE, dom = 'Brtip',\r\n                   autoWidth = FALSE))\r\n\r\n\r\n\r\n\r\nIn addition to 2 persons and 1 organization, these results include also a ship (Q4059130) and a scholarly article (Q4059130).\r\nWe can also query this API via SPARQL and filter the results - let’s to do this for more relaxed (Serge\\?) query. Mind quoting that a question mark for SPARQL need to be escaped, so in R code this requires more backslashes.\r\n\r\nShow code\r\nWikidataQueryServiceR::query_wikidata(sparql_query = 'select \r\n  ?person ?personlabel_en ?persondesc_en\r\nWhere {\r\n  SERVICE wikibase:mwapi {\r\n    bd:serviceParam wikibase:api \"Search\";\r\n                    wikibase:endpoint \"www.wikidata.org\";\r\n                    mwapi:srsearch \"Serge\\\\\\\\? Vavilov\" .          \r\n    ?person wikibase:apiOutputItem mwapi:title.    \r\n  } \r\n  FILTER BOUND (?person)       \r\n        ?person wdt:P31/wdt:P279* wd:Q5.\r\n      optional{?person rdfs:label ?personlabel_en . FILTER(lang(?personlabel_en)=\"en\")}\r\n      optional{?person schema:description ?persondesc_en . FILTER(lang(?persondesc_en)=\"en\")}\r\n      SERVICE wikibase:label {bd:serviceParam wikibase:language \"en\".}\r\n  }'    \r\n  ) %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org.'),\r\n    options = list(searchHighlight = TRUE, dom = 'Brtip',\r\n                   autoWidth = FALSE))\r\n\r\n\r\n\r\n\r\nThis search returns 5 persons, 3 of which we haven’t found 3 with wbsearchentities API. Why? Because two results (Q12562032, Q19907955) do not have labels, aliases, or description in English - but the Wikidata items have the first name (Sergey) and the last name (Vavilov) properties, which is not queried by wbsearchentities API. Another one found only by this API is spelled in Wikidata as Sergei(!) V Vavilov (Q91652500).\r\nThe main advantages of using Search API (query&list=search) instead of wbsearchentities API for searching the people are:\r\nthe former also covers the first name and last name fields of the Wikidata item\r\n\r\nthe former API allows some lexical flexibility (fuzzy search and wildcards).\r\n\r\nUnfortunately, it is not only the endings that change when the Cyrillic names are transliterated to Latin.\r\nAleksei can be not not only Aleksey, but also Alexei and Alexey\r\nOksana can be Oxana\r\nFedor can be Fyodor\r\nJulia can be Yulia or Ioulia (see an example)\r\nSo you can hardly guess where to put the wildcard.\r\nEven being armed with wikibase:api “Search” + wildcards/fuzzy + SPARQL filters, we still have some limitations, having to decide which strategy is the best for searching the researchers in Wikidata by their names!\r\nSearch Strategies\r\nI considered few options:\r\nto substitute the ending of all given names ? (without further thinking) and ignore the names like Oxana, Aleksei or Fyodor with ambiguous spelling in the middle of the given name. Dismissed this approach.\r\nto use a built-in option of fuzzy search operator (~). But there are some aspects…described at CirrusSearch.\r\nSearching “Oxana~1” covers both Oksana and Oxana (as for a word search suceeded with “~2” means 2 extra added or changed characters). Together with the family name it is processed in different way - tilda tells how many extra words to fit in. Hence, “Oksana~1 Ivanova” and “Oxana~1 Ivanova” produce different results.\r\nTrying to UNION 2 requests (Oxana~1 and Ivanova) in SPARQL is likely to be too “expensive” for most popular names. Same as searching by the family name only and further filtering with regular expressions in SPARQL (see some examples here).\r\nto find the Wikidata items corresponding to the given name (with fuzzy search), collect all the relevant name variants (P460: said to be the same as + P2440:transliterations), and use them to generate all possible combinations (family name + given name variant) for wbsearchentities.\r\nThis strategy may look like a great trick, but dealing with the names in Wikidata is not an easy walk.\r\nFirst of all, a fuzzy search does not change first 2 letters (here), so Yulia~ will never lead to Julia (and vice versa). Same with Fyodor and Fedor. Tilda will not help everyone.\r\nIf we try to search a female given name Yulia by wbsearchentities, we will find few Wikidata items (Yuliya of Ukrainian/Belarussian origin) and (Yulia of Russian/Bulgarian origin). Their (P460:said to be the same as) forms vary from Giulietta to Uliana. So P460 could be more an extra burden than a solution.\r\nIt may seem that the English transliterations (P2440), filtered by Wikimedia language code (P424), could help, but this property is not mandatory and can be missed for some names. Moreover, the property constraints can also be totally different. For Yulia the transliterations are defined via the following constraints (Wikimedia language code / writing system / determination method), but for Oksana there is only a determination method. For Yulia the determination method for English transliteration is “romanization of Russian”, but for Oksana this method has many more academic values - “German romanization of Ukrainian (Duden)”, “BGN/PCGN romanization of Russian”, “BGN/PCGN romanization of Ukrainian (1965 system)”, “Ukrainian National System”, “scientific transliteration of Cyrillic”, “ALA-LC romanization”, “modified Library of Congress system”. So using the transliterations can be an option only after most popular “determination method” values are collected (to be used as a filter for English transliteration).\r\nWhat strategy is optimal? I decided to go with this:\r\nStep 1. search for a given name in English using wbsearchentities API (e.g. wikibase:api “EntitySearch”; mwapi:search “Oxana”) and filter the items that are instances or subclasses of the given name (Q202444), retrieve all the English aliases and labels.\r\nStep 2. Generate the unique combinations of found Given names with the Family name.\r\nStep 3. Search all the combinations via SPARQL (one by one or like … OR …) and filter the human beings (Q5).\r\nStep 4. Manual check the results.\r\nLet’s do it with the code.\r\nStep 1. Finding Given Names in Wikidata\r\nThe code below sets up a function to collect the name variants.\r\n\r\nShow code\r\nget_wiki_name_variants <- function(given_name){\r\n  paste0('SELECT DISTINCT ?personlabel_en WHERE {\r\n  SERVICE wikibase:mwapi {\r\n      bd:serviceParam wikibase:endpoint \"www.wikidata.org\";\r\n        wikibase:api \"EntitySearch\";\r\n        mwapi:search \"', given_name, '\"; \r\n        mwapi:language \"en\".\r\n      ?item wikibase:apiOutputItem mwapi:item.\r\n  }\r\n    ?item wdt:P31/wdt:P279* wd:Q202444. \r\n  SERVICE wikibase:label {bd:serviceParam wikibase:language \"en\".}\r\n  optional {?item rdfs:label|skos:altLabel ?personlabel_en . FILTER(lang(?personlabel_en)=\"en\")}\r\n}') %>% \r\n    WikidataQueryServiceR::query_wikidata() %>% \r\n    mutate_at(c(\"personlabel_en\"), \r\n              ~str_squish(str_replace_all(.x, \"\\\\([^\\\\(^\\\\)]+\\\\)\", \"\"))) %>% \r\n  distinct()\r\n# example: get_wiki_name_variants(\"Aleksei\") \r\n}\r\n\r\n\r\n\r\nNow we are going to collect all the variants of the given names for our 28 authors (present in 10 articles selected as an example).\r\n\r\nShow code\r\nwiki_given_names_file <- paste0(dir,\"wiki_given_names.RDS\")\r\n\r\nif(!file.exists(wiki_given_names_file)){\r\n  wiki_given_names <- unique(zz2$given) %>%\r\n    map_df(~get_wiki_name_variants(.x) %>% \r\n            filter(!str_detect(personlabel_en, \r\n                              \"[\\\\p{Letter}--\\\\p{script=latin}]+\")) %>% \r\n            mutate(given = .x))\r\n\r\n  write_rds(wiki_given_names, wiki_given_names_file)\r\n} else { \r\n  wiki_given_names <- read_rds(wiki_given_names_file)\r\n}\r\n\r\nwiki_given_names %>% relocate(given) %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org.'),\r\n    options = list(searchHighlight = TRUE, dom = 'Brtip',\r\n                   autoWidth = FALSE))\r\n\r\n\r\n\r\n\r\nWell, a conversion of Marina to Marinette or Alexander to Sasha may be considered as an exaggeration, but for many given names (column “given”) our adding the name variants from Wikidata (column “personlabel_en”) seem to increase the chances to be found.\r\nWikidata returned many name variants with the special symbols (diacritical marks). I think that it will be no harm if we convert those names into ASCII-form to use only the English letters and unique spellings. For such transformations I use a function stri_trans_general(“Latin-ASCII”) from stringi package. Even though stringr provides a lot of str_* substitutions for original stringi functions (optimized for coding in tidyverse style), I am not aware of stringr-based way of doing such a transformation.\r\n\r\nShow code\r\nwiki_given_names <- wiki_given_names %>% \r\n  mutate(personlabel_en = stringi::stri_trans_general(personlabel_en, \"Latin-ASCII\")) %>% \r\n  distinct()\r\n\r\n\r\n\r\nThis transformation decreases a total number of name variants from 104 to 93.\r\nStep 2. Building the Name Combinations\r\nI merged the found name variants with the family names to form a new column “name variant”, which I am going to use further for finding the persons in Wikidata.\r\n\r\nShow code\r\nzz2 <- zz2 %>% left_join(wiki_given_names) %>% \r\n  unite(name_variant, c(\"personlabel_en\", \"family\"), \r\n        sep = \" \", na.rm=TRUE, remove = FALSE) %>% \r\n  relocate(doi, order, family, given, personlabel_en)\r\n  \r\nzz2 %>%\r\n  DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org.'),\r\n    options = list(searchHighlight = TRUE, dom = 'Bfrtip',\r\n                   autoWidth = FALSE))\r\n\r\n\r\n\r\n\r\nStep 3. Finding authors in Wikidata (by name)\r\nTo find the persons in Wikidata and get the most interesting details we will be using the following function based on a SPARQL request. As R seems to truncate the arguments of 1000+ chars in length, I also made a separate URL to WDS where you can see a SPARQL query.\r\n\r\nShow code\r\nsparql_author_query <- function(person_name){\r\n  paste0('SELECT ?person ?personlabel_ru ?personlabel_en\r\n          ?persondesc_en ?occupations ?employers  \r\n        ?viaf ?magid ?isni ?lc ?elibrary ?scopus_id ?reseacher_id ?orcid ?publons\r\nWITH {\r\n  SELECT ?person  \r\n        (GROUP_CONCAT(DISTINCT ?occupation; SEPARATOR=\" | \") AS ?occupations)\r\n        (GROUP_CONCAT(DISTINCT ?employer; SEPARATOR=\" | \") AS ?employers)\r\n   WHERE {\r\n    hint:Query hint:optimizer \"None\".\r\n      SERVICE wikibase:mwapi {\r\n        bd:serviceParam wikibase:api \"Search\";\r\n                        wikibase:endpoint \"www.wikidata.org\";\r\n                        mwapi:srsearch \"', person_name, [1163 chars quoted with ''']) %>% \r\n    WikidataQueryServiceR::query_wikidata() \r\n}\r\n\r\n\r\n\r\nThis function accepts the person name, quoted or unquoted, and can also process the name variants separated by OR (like in example below designed to find the Wikidata profiles of 2 prominent Russian scientists from Novosibirsk).\r\n\r\nShow code\r\nsparql_author_query(\"\\\\\\\"Valentin Parmon\\\\\\\" OR \\\\\\\"Valentin Vlasov\\\\\\\"\") %>%\r\n  DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org.'),\r\n    options = list(searchHighlight = TRUE, dom = 'Brtip',\r\n                   autoWidth = TRUE, \r\n                columnDefs = list(\r\n                  list(width = '350px', targets = c(5)),\r\n                   list(width = '170px', targets = c(1,3,13)))\r\n                  ))\r\n\r\n\r\n\r\n\r\nBy adding the name variants we increase 3.5 times a number of requests required to find the Wikidata items for 28 authors (97 name variants). Therefore, I am going to use … OR … syntax to reduce a number of queries back to 28.\r\n\r\nShow code\r\n## grouping the name variants into ... OR ... strings\r\nzz2grouped <- zz2 %>% \r\n  mutate(name_variant = paste0('\\\\\\\"',name_variant,'\\\\\\\"')) %>% \r\n  group_by(doi, order) %>% \r\n  summarize(name_variants = paste0(name_variant, collapse = \" OR \")) %>% \r\n  ungroup()\r\n\r\nwiki_persons_file <- paste0(dir,\"wiki_persons.RDS\")\r\n\r\nif(!file.exists(wiki_persons_file)){\r\n  wiki_persons <- unique(zz2grouped$name_variants) %>%\r\n    map_df(~sparql_author_query(.x) %>%\r\n             ## as some identifiers can be returned as characters & double\r\n             mutate_all(~as.character(.x)) %>% \r\n             mutate(name_variants = .x))\r\n\r\n  write_rds(wiki_persons, wiki_persons_file)\r\n} else { \r\n  wiki_persons <- read_rds(wiki_persons_file)\r\n}\r\n\r\nwiki_persons %>%  \r\n  DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org.'),\r\n    options = list(searchHighlight = TRUE, dom = 'Brtip', autoWidth = TRUE, \r\n                columnDefs = list(\r\n                  list(width = '350px', targets = c(3, 5)),\r\n                   list(width = '450px', targets = c(15)),\r\n                   list(width = '170px', targets = c(1,8, 13)))\r\n                  ))\r\n\r\n\r\n\r\n\r\nWe have found 11 persons, of which 4 are the right persons (Wladimir Andreff, Peter Kaznacheev, Viktor Ryazanov, and twice Sergey Kalendzhyan). This I decided by manually checking the suggested variants (Step 4). It was helpful, of course, that 3 of 4 are also marked as economists in the “description” or “occupations” columns. By the way, Sergey Kalendzhyan in the original articles was spelled both as Sergei and Sergey, but due to step 1 (when we added all known name variants), we have found Sergey Kalendzhyan’s Wikidata profile in both cases.\r\n\r\nShow code\r\nwd_authors_by_names_file <- paste0(dir, \"wd_authors_by_names.csv\")\r\nright_ones <- c(\"Wladimir Andreff\", \"Peter Kaznacheev\", \r\n                \"Viktor Ryazanov\", \"Sergey Kalendzhyan\")\r\nwiki_persons %>% \r\n  filter(personlabel_en %in% right_ones) %>% \r\n  left_join(zz2grouped, .) %>%\r\n  write_excel_csv(wd_authors_by_names_file)\r\n\r\n\r\n\r\nNow we are ready for a final step!\r\nUploading the Author Info into Wikidata\r\nThis is what we have had by the end of our journey.\r\n\r\nShow code\r\ndata_wd  <- bind_rows(\r\n  read_csv(wd_authors_by_ids_file, \r\n           col_types = cols(.default = col_character())) %>% \r\n    select(any_of(c(\"doi\", \"order\", \"wikidata\"))),\r\n  read_csv(wd_authors_by_names_file, \r\n           col_types = cols(.default = col_character()))%>% \r\n    mutate(wikidata = str_extract(person, \"Q\\\\d+$\")) %>% \r\n    select(any_of(c(\"doi\", \"order\", \"wikidata\")))\r\n  ) %>% \r\n    filter(!is.na(wikidata)) %>% \r\n    rename(wd_author = wikidata)\r\n  \r\ndata4wiki <- final_data_checked %>% \r\n  left_join(dois) %>% \r\n  left_join(data_wd) %>%\r\n  left_join(wd_pub_existing_data %>% \r\n              select(wikidata = qid, order, current_author = author_wiki, \r\n                     current_statement = author_wiki_type)) %>% \r\n  unite(\"author_string\", c(\"CRF given\", \"CRF family\"), sep = \" \") %>% \r\n  mutate(author_string = str_to_title(author_string)) %>% \r\n  mutate(author_statement = ifelse(is.na(wd_author), \"P2093\", \"P50\")) %>%  \r\n  select(article_wiki = wikidata, order,\r\n         current_author, current_statement, \r\n         author_statement, author_wiki = wd_author, author_string) \r\n\r\ndata4wiki %>%  \r\n  DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org.'),\r\n    options = list(searchHighlight = TRUE, dom = 'Brtip', autoWidth = TRUE))\r\n\r\n\r\n\r\n\r\nWe know the wikidata items, corresponding to the articles (column “article_wiki”), and how the authors are currently present in Wikidata (see the columns “current_author” and “current_statement”). The “current_author” strings have only the initials, so we can improve it by substituting with the authorname strings from CrossRef (P2093 property for the statement). For the authors with Wikidata items (4 authors with non-empty values in the author_wiki column) we will introduce the P50 statements (connecting the Wikidata items for article and person).\r\nIn both cases we should also not to forget to delete the existing statements (see below the example of statements).\r\n\r\nShow code\r\nwd_statements <-  data4wiki %>% rowwise() %>% \r\n  mutate(statement = ifelse(!is.na(author_wiki),\r\n           paste0(\"-\",article_wiki,\"|\", current_statement,\"|\\\"\", \r\n                  current_author,\"\\\"||\",\r\n                  article_wiki,\"|P50|\",author_wiki,\"|P1545|\\\"\", \r\n                  order,\"\\\"|S248|Q5188229\"), \r\n           paste0(\"-\",article_wiki,\"|\", current_statement,\"|\\\"\", \r\n                  current_author,\"\\\"||\",\r\n                  article_wiki,\"|P2093|\\\"\",author_string, \"\\\"|P1545|\\\"\", \r\n                  order,\"\\\"|S248|Q5188229\"))) %>% \r\n  select(statement) %>% \r\n  mutate(statement = str_split(statement, \"\\\\|\\\\|\")) %>% \r\n  unnest(statement) %>% \r\n  unlist(use.names = FALSE) \r\n\r\nwd_statements %>% \r\n  paste0(., collapse = \"\\n\") %>% \r\n  write_file(paste0(dir, \"for qs_auths.tsv\"))  \r\n  \r\nwd_statements %>% enframe(name = NULL, value = \"wikidata_statements\") %>% \r\n   DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org.'),\r\n    options = list(searchHighlight = TRUE, dom = 'Brtip', autoWidth = FALSE))\r\n\r\n\r\n\r\n\r\nWith the line (-Q107266290|P2093|“Kalendzhyan S.O.”) we remove the existing statement that claims {wikidata item Q107266290 has an author string “Kalendzhyan S.O.”}.\r\nWith the next line (Q107266290|P50|Q4209279|P1545|“1”|S248|Q5188229) we create a new statement that claims {wikidata item Q107266290 has an author (P50) relation to Q4209279 that is positioned (P1545) first (“1”) as stated (P248) in CrossRef (Q5188229)}.\r\nMind S instead of P for the statement used as a reference. More details on QuickStatements syntax can be found here)\r\nThe process looks like this:\r\nOpen QuickStatemets interface. To have an access, you have to be an autoconfirmed user. Paste the prepared statements (the pictures below show the result just for 2 lines, as an example!). Click Import V1 commands.\r\n\r\nShow code\r\nknitr::include_graphics(paste0(getwd(),\"/images/wd_statement_sok1.PNG\"))\r\n\r\n\r\n\r\n\r\nCheck that the suggested revisions make sense (the properties are recognized and shown as active URLs). Click Run.\r\n\r\nShow code\r\nknitr::include_graphics(paste0(getwd(),\"/images/wd_statement_sok2.PNG\"))\r\n\r\n\r\n\r\n\r\nWatch. Enjoy.\r\n\r\nShow code\r\nknitr::include_graphics(paste0(getwd(),\"/images/wd_statement_sok3.PNG\"))\r\n\r\n\r\n\r\n\r\nThis is how the updated article page looks like in Wikidata. The “authors” and “author name strings” are separated.\r\n\r\nShow code\r\nknitr::include_graphics(paste0(getwd(),\"/images/wd_statement_sok4.PNG\"))\r\n\r\n\r\n\r\n\r\nAnd this is how the article page looks like in Scholia.\r\n\r\nShow code\r\nknitr::include_graphics(paste0(getwd(),\"/images/wd_statement_sok5.PNG\"))\r\n\r\n\r\n\r\n\r\nIf you click on the authors marked as UNRESOLVED:, you will be moved to the Author Disambiguator, another extra-useful tool for editing the author information for academic publications in Wikidata.\r\n\r\nShow code\r\nknitr::include_graphics(paste0(getwd(),\"/images/wd_statement_sok6.PNG\"))\r\n\r\n\r\n\r\n\r\nThis application tries to find the author profile in Wikidata (I am not sure how complex its recommendation algorithm), and you can search the person by ORCID and by name. If nothing is found, the application suggests to create the new Wikidata item (this is something I decided not to do in my exercise).\r\n\r\nShow code\r\nknitr::include_graphics(paste0(getwd(),\"/images/wd_statement_sok7.PNG\"))\r\n\r\n\r\n\r\n\r\nThe only “but”, again, it is also for autoconfirmed users only.\r\nFinal Remarks (to the journal editors)\r\nWikidata has a lot of valuable tools and options for the academic journals to highlight their impact. To deal with it is just a question of having one dedicated person enrolled to take care of uploading the articles’ metadata into Wikidata and doing the author disambiguation (after getting a status of autoconfirmed user). Even if I failed to convince you to deal with Wikidata, please, do pay attention to a quality of articles’ metadata.\r\nI will continue this exercise to add the affiliations and other details to Wikidata in close future.\r\nAcknowledgments\r\nAllaire J, Xie Y, McPherson J, Luraschi J, Ushey K, Atkins A, Wickham H, Cheng J, Chang W, Iannone R (2021). rmarkdown: Dynamic Documents for R. R package version 2.7, <URL: https://github.com/rstudio/rmarkdown>.\r\nGagolewski M (2020). R package stringi: Character string processing facilities. <URL: http://www.gagolewski.com/software/stringi/>.\r\nHenry L, Wickham H (2020). purrr: Functional Programming Tools. R package version 0.3.4, <URL: https://CRAN.R-project.org/package=purrr>.\r\nOoms J (2014). “The jsonlite Package: A Practical and Consistent Mapping Between JSON Data and R Objects.” arXiv:1403.2805 [stat.CO]. <URL: https://arxiv.org/abs/1403.2805>.\r\nPopov M (2020). WikidataQueryServiceR: API Client Library for ‘Wikidata Query Service’. R package version 1.0.0, <URL: https://CRAN.R-project.org/package=WikidataQueryServiceR>.\r\nShafee T, Keyes O, Signorelli S, Lum A, Graul C, Popov M (2021). WikidataR: Read-Write API Client Library for ‘Wikidata’. R package version 2.2.0, <URL: https://github.com/TS404/WikidataR/issues>.\r\nWickham H (2020). tidyr: Tidy Messy Data. R package version 1.1.2, <URL: https://CRAN.R-project.org/package=tidyr>.\r\nWickham H (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=stringr>.\r\nWickham H, Francois R, Henry L, Muller K (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.3, <URL: https://CRAN.R-project.org/package=dplyr>.\r\nWickham H, Hester J (2020). readr: Read Rectangular Text Data. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=readr>.\r\nXie Y (2020). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.30, <URL: https://yihui.org/knitr/>.\r\nXie Y (2015). Dynamic Documents with R and knitr, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, <URL: https://yihui.org/knitr/>.\r\nXie Y (2014). “knitr: A Comprehensive Tool for Reproducible Research in R.” In Stodden V, Leisch F, Peng RD (eds.), Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595, <URL: http://www.crcpress.com/product/isbn/9781466561595>.\r\nXie Y, Allaire J, Grolemund G (2018). R Markdown: The Definitive Guide. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9781138359338, <URL: https://bookdown.org/yihui/rmarkdown>.\r\nXie Y, Cheng J, Tan X (2021). DT: A Wrapper of the JavaScript Library ‘DataTables’. R package version 0.17, <URL: https://CRAN.R-project.org/package=DT>.\r\nXie Y, Dervieux C, Riederer E (2020). R Markdown Cookbook. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9780367563837, <URL: https://bookdown.org/yihui/rmarkdown-cookbook>.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-02-importing-author-information-from-crossref-to-wikidata-via-quickstatements/images/wd_statement_sok2.PNG",
    "last_modified": "2021-07-05T12:31:01+03:00",
    "input_file": {},
    "preview_width": 1345,
    "preview_height": 524
  },
  {
    "path": "posts/2021-06-28-post-publication-collecting-orcids-for-the-authors/",
    "title": "Post-Publication Collecting ORCIDs for the Authors",
    "description": "What if the journal ignored ORCID for years? Is there a simple way to collect their ORCIDs except emailing'em with gentle reminders? In this post I exploit the ways to collect ORCIDs for the authors of already published articles using the open sources (CrossRef, Microsoft Academic, ORCID). Writing it was an unusually long journey, as I had to figth inconsistencies and revise the approaches. An initial share of ORCIDs in CrossRef for the authors of selected articles was 10%. It was raised to 32% with just algorithm-based approach and further was improved to 64% after additional manual check-up of the automatically pre-selected candidates. This is another long post about the scholarly articles, metadata quality, blessed open APIs, and utility of R packages.",
    "author": [
      {
        "name": "Aleksei Lutai",
        "url": "https://www.linkedin.com/in/lutaya/"
      }
    ],
    "date": "2021-06-28",
    "categories": [
      "russian data",
      "orcid",
      "r",
      "crossref",
      "microsoft academic",
      "igraph/tidygraph",
      "fuzzy matching",
      "open citations",
      "coauthorship analysis"
    ],
    "contents": "\r\n\r\nContents\r\nInitial DOIs\r\nCrossRef\r\nMicrosoft Academic\r\nOpenAIRE\r\nORCID (by DOI)\r\nORCID (by Name)\r\nConfirmation by Co-Authorship\r\nConfirmation by Citation\r\nConfirmation by Context\r\nFinal Result\r\nAcknowledgments\r\n\r\nAssume that you have just agreed to manage a scientific journal and among many ideas there is one where the journal data is available in Wikidata (and all the world is using Scholia to see its impact (like here)). And then you find out that the internal database contains incomplete metadata, patchy for some years, so literally the only reliable data you have are… just DOIs.\r\nA journey to Wikidata will be long (and dangerous, yes!). This post is the first part of it - we will try to utilize the open academic services to update the author metadata and prepare it for Wikidata (which may happen in the next parts).\r\nIn this post I will use as an example the randomly selected articles from the journal Economic Policy (ISSN: 1994-5124) published in 2016-2020. I claim that the context above about the manager and patchy data is made up for the context and has nothing to do with the selected journal. I am not a manager of it, nor am I planning to become anyhow related to it. So no allegations towards the journal selected as an example and quality of their editorial work - the context is made up, the other things, down the road, will be real.\r\nInitial DOIs\r\nI randonly selected 10 DOIs from the journal Economic Policy (ISSN: 1994-5124) published in 2016-2020.\r\n\r\nShow code\r\ndata <- tibble(\r\n  doi = c(\"10.18288/1994-5124-2020-3-134-151\", \"10.18288/1994-5124-2018-4-02\", \r\n           \"10.18288/1994-5124-2016-5-06\", \"10.18288/1994-5124-2020-3-74-105\",\r\n           \"10.18288/1994-5124-2015-5-07\", \"10.18288/1994-5124-2016-5-09\", \r\n          \"10.18288/1994-5124-2017-6-08\", \"10.18288/1994-5124-2016-3-06\",\r\n          \"10.18288/1994-5124-2015-3-02\", \"10.18288/1994-5124-2016-5-10\")\r\n          )\r\n\r\nDT::datatable(data, rownames = F, \r\n      options = list(dom = \"tp\", deferRender = TRUE, \r\n                    ordering = TRUE, autoWidth = FALSE, \r\n                    scrollX = F))\r\n\r\n\r\n\r\n\r\nCrossRef\r\nOur next step is, of course, to download the article’s metadata from CrossRef.\r\nI expect that some of the readers may question - how come is that the metadata submitted to CrossRef is not available in the editorial office.\r\nThis requires a separate remark - in Russia there are many scientific journals that are still published by the state organizations, not by the commercial publishers. One may think that this is cool because (1) the commercial services provided by the third parties decrease the profit, which is an endangered species in academic publishing; (2) open software as PKP OJS and new formats (like overlayed journals, where SIGMA is a bright example) should allow to cut the costs. The problem is that the state organizations are under constant budget optimization processes and subject to “some” regulations. As a result, the optimized editorial teams often have to request layout/pre-press services from external suppliers (selected each year through a tender process). In addition to those services, the external suppliers who work on layout, are also often asked to pay an annual fee to CrossRef, and to upload the metadata (the university’s procurement department and legal services will eat the guts of anyone who ask them to pay few hundred bucks without a proper agreement in Russian with Russian arbitrary law and penalties delays…). And as the organs weaken, if not regularly used, some editorial teams nowadays have no a person knowing how to upload/edit CrossRef metadata. I hope this explain that a described situation is not fully made up, it is quite often met in Russia. (NB! This explanation does not relate to the selected journal - I have not checked who they paid for what and what they do themselves).\r\nFor simple queries to CrossRef I am using rcrossref package. Please mind the API etiquette and set your email into R.environment with the following steps:\r\nusethis::edit_r_environ()\r\n\r\npaste the edited line crossref_email = here_is_my_email@postserver.name\r\n\r\nsave the file\r\n\r\nrestart R\r\n\r\n\r\nShow code\r\n# as always I download the data into file once and \r\n# do not repeat it again when re-knitting\r\ncrossref_data_file <- paste0(dir,\"/crossref.RDS\")\r\n\r\nif(!file.exists(crossref_data_file)){\r\n  crossref_data <- map(data$doi, ~cr_works(.x))\r\n  \r\n  write_rds(crossref_data, crossref_data_file)\r\n  } else {\r\n  crossref_data <- read_rds(crossref_data_file)\r\n}\r\n\r\n\r\n\r\nAs this post is only about the author information, I take only the author info and convert it a bit.\r\n\r\nShow code\r\ncrf_author_data <- crossref_data %>% \r\n  map(\"data\") %>% map_df(~.x) %>% \r\n  select(any_of(c(\"doi\", \"author\", \"ORCID\"))) %>% \r\n  unnest(author, keep_empty = TRUE) %>% \r\n  mutate(ORCID = gsub(\"http://orcid.org/\",\"\",ORCID))\r\n  \r\ncrf1 <- crf_author_data %>% \r\n  filter(!is.na(family)) %>% \r\n  group_by(doi) %>% \r\n  # adding author order \r\n  mutate(ord = as.character(row_number())) %>% \r\n  ungroup() %>% \r\n  select(doi, family, given, ord, orcid = ORCID) \r\n\r\ncrf2 <- crf_author_data %>% \r\n  unite(col = \"org_name\",  \r\n        c(\"affiliation.name\", \"name\"), na.rm = TRUE) %>% \r\n  filter(!org_name==\"\") %>%\r\n  group_by(doi) %>% \r\n  mutate(ord = as.character(row_number())) %>% \r\n  ungroup() %>% \r\n  select(doi, org_name, ord) \r\n\r\ncrf_data <- data %>% left_join(crf1) %>% left_join(crf2)\r\n\r\ncrf_data %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n               caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; \r\n               text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: crossref.org (see in the text).'),\r\n    options = list(searchHighlight = TRUE, dom = 'Bfrtip',\r\n                columnDefs = list(\r\n                  list(width = '150px', targets = c(0,4)),\r\n                  list(width = '350px', targets = c(5)),\r\n                  list(className = 'dt-center', targets = c(3)))\r\n                  ))\r\n\r\n\r\n\r\n\r\nFirst results here:\r\nauthor and affiliation details in CrossRef (for these articles) are not bad, the author names are present in consistent format\r\n9 out of 10 articles have the affiliation strings\r\n3 out of 28 authors have ORCIDs.\r\nNot too many ORCIDs are registered in CrossRef, so naturally we would like to find more. We can try to seek the persons in ORCID, but before doing that it would be helpful to get also the author names in Cyrillic, as many Russian researchers register in ORCID under its name in Cyrillic. As this journal’s orginal version is published in Russian, I would start searching the Cyrillic names in Microsoft Academic (will be missed).\r\nMicrosoft Academic\r\nThe service indexes many Russian journals with the original Cyrillic names. I will use its API and microdemic package to query the author and affiliation info for each DOI.\r\nThe code below is longer than expected because in Cyrillic format a family name is placed first, followed by a given name and further by patronymic, while in Latin format a given name is succeeded by a family name. I needed to match the Cyrillic and Latin name variants to use them both for querying ORCID, therefore I put additional transformations into the code below.\r\nI detected the language with a regular expression [\\\\p{Letter}--\\\\p{script=latin}]+\r\nI extracted the family and given names using the regular expressions (depending on whether the author info is in Cyrillic or Latin format).\r\nI hoped that this would be enough and I could match the authors by their position in the list of the authors. And I found out that an order can be different in Russian and English versions of the same article (a real case in our selected articles). And as we need to match the author names in Latin and Cyrillic spelling, I transliterated the Cyrillic names from Microsoft Academic and re-ordered the them according to the order in CrossRef. For transliteration from Cyrillic to Latin I used my own function (gist).\r\n\r\nShow code\r\nmag_data_file <- paste0(dir,\"/mag.RDS\")\r\n\r\nif(!file.exists(mag_data_file)){\r\n  mag_data <- map_df(data$doi, \r\n                   ~ ma_evaluate(query = paste0(\"DOI=\\'\", .x, \"\\'\"), \r\n                                  atts = c('DOI', 'AA.AuN', 'AA.AuId', 'AA.AfN', 'AA.AfId')))\r\n  \r\n  mag_data_details <- mag_data %>% unnest_wider(AA) %>% \r\n    unnest(cols = c(AuN, AuId, AfN, AfId)) %>%\r\n     group_by(DOI) %>% mutate(ord = as.character(row_number())) %>% ungroup() %>% \r\n    select(-logprob, -prob) %>% rename(doi = DOI) %>% \r\n    mutate(AuN = str_to_title(AuN)) %>% \r\n    mutate(rus_lang = ifelse(str_detect(AuN, \"[\\\\p{Letter}--\\\\p{script=latin}]+\"), \r\n                               TRUE, FALSE)) %>% \r\n    mutate(mag_family = ifelse(rus_lang, \r\n                               str_extract(AuN, \"^[^\\\\s]+\"), \r\n                               str_extract(AuN, \"(?<= )[^\\\\s]+$\")),\r\n           mag_given = ifelse(rus_lang, \r\n                              str_extract(AuN, \"(?<= )[^\\\\s]+(?= )\"), \r\n                              str_extract(AuN, \"^[^\\\\s]+\")))\r\n  \r\n# transliterator function is available as a gist    \r\n# https://gist.github.com/alexeilutay/3c784476f47a61c74752609e2f9b564f/raw/6d780af44452bb5d0be714738e46ca3bb09f69c2/transliterator.R\r\nsource(paste0(onedrive, \"/rscripts/Rscripts/TranslitRusEng.R\"), encoding = \"UTF-8\")  \r\n\r\n  mag_data_details <- mag_data_details %>% \r\n    mutate(mag_family = enc2native(mag_family)) %>% \r\n    mutate(tr_mag_family = map_chr(mag_family, translit2))\r\n  \r\n  ## re-ordering\r\n  for (s in 1:nrow(mag_data_details)){\r\n    acr <- crf_data %>% filter(doi == mag_data_details$doi[s]) \r\n    pos <- which.max(stringsim(mag_data_details$tr_mag_family[s], acr$family))\r\n    mag_data_details[s, \"ord\"] <- acr$ord[pos]\r\n    }\r\n  \r\n  write_rds(mag_data_details, mag_data_file)\r\n} else {\r\n  mag_data_details <- read_rds(mag_data_file)\r\n}\r\n\r\ncrf_mag_data <- crf_data %>% left_join(mag_data_details)\r\n\r\ncrf_mag_data %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n             caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: crossref.org; academic.microsoft.com (see in the text).'),\r\n            options = list(searchHighlight = TRUE, dom = 'Bfrtip',\r\n                           columnDefs = list(\r\n                  list(width = '150px', targets = c(0,4)),\r\n                  list(width = '350px', targets = c(5)),\r\n                  list(className = 'dt-center', targets = c(3,4)))\r\n                  ))\r\n\r\n\r\n\r\n\r\nIf you page the table to the row 6 and scroll a bit to the right, you will see the names in Cyrillic.\r\nThere are some other sources that can be used for name extraction:\r\nfull texts: CrossRef often provides the URLs for text-data mining, if not you can use fulltext and roadoi packages to find the full text. Don’t forget to check at CORE. And just recently a new package doilinker was released with an R implementation of the linking approach described in Cabanac G, Oikonomidi T, Boutron I. Day-to-day discovery of preprint-publication links. Scientometrics. 2021; 1–20. DOI: 10.1007/s11192-021-03900-7.\r\nindexing services: well, Scopus/Web of Science can be very helpful, but the providers limit a scope of possible using. I haven’t checked with them, but I do not think that using those services for amelioration of the open data (e.g. Wikidata) would a use case favored by commercial data providers. I am wrong in thinking so, the better.\r\nopen A&I databases: Lens is cool in many ways. For example, they already matched the articles to ORCID dataset, you can see the ORCID of the authors even if it is not present in CrossRef (here is an example: no ORCID in CrossRef, no ORCID in Microsoft Academic, ORCID is present in The Lens, apparently matched via DOI present in the author’s ORCID profile). But… ORCID is not present in the Lens export, although the creators told me that this is a possible development, so they will look how soon this can be integrated.\r\nRussian indexing services are few and not quite friendly to parsing - eLibrary blocks the user by IP after a dozen of GET requests (my own experience), Cyberleninka can choke to death with the ads (until paid ~1.5 USD monthly rent), and explicitly prohibits any use of parsers or harvesters. In some countries “open” still means being “available via web UI”. And yet Cyberleninka deserves a good word - they pass(-ed?/-es?) their contents to OpenAIRE.\r\nOpenAIRE provides few APIs. You can either obtain a personal token or make unauthorized queries at lower rate, and get the results in XML/JSON/CSV formats (query should look like api.openaire.eu/search/publications?doi={DOI}&format=json). There is an R package ropenaire, but as it was updated 3 years ago, I did not use it and queried OpenAIRE with unauthorized GET requests to use JSON output.\r\nOpenAIRE\r\nLet’s look at what author details we can get from OpenAIRE.\r\n\r\nShow code\r\nopenaire_file <- paste0(dir,\"openaire.RDS\")\r\nif(!file.exists(openaire_file)){\r\n  oa_data <- data$doi %>% \r\n    map(~paste0(\"http://api.openaire.eu/search/publications?doi=\", .x,\r\n                   \"&format=json\") %>% \r\n          jsonlite::fromJSON()\r\n        ) %>% \r\n    map(pluck,  \"response\", \"results\", \"result\", \"metadata\", \r\n         \"oaf:entity\", \"oaf:result\")\r\n  \r\n write_rds(oa_data, openaire_file)\r\n} else {\r\n  oa_data <- read_rds(openaire_file)\r\n}  \r\n\r\n# the author info\r\noa_authors <- oa_data %>% map(pluck,\"creator\") %>% \r\n    map_if(is_list, ~flatten_df(.x)) %>% \r\n    map_df(~.x)\r\n\r\noa_authors  %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, \r\n                class = 'compact striped',\r\n             caption = htmltools::tags$caption(\r\n               style = 'caption-side: bottom; text-align: left; font-size: 80%; \r\n               color: #969696; font-family: Roboto Condensed;',\r\n               'Data: openaire.eu (see in the text).'),\r\n            options = list(searchHighlight = TRUE, dom = 'Bfrtip',\r\n                           columnDefs = list(\r\n                  list(width = '200px', targets = c(1,2, 5,6)),\r\n                  list(className = 'dt-center', targets = c(0)))\r\n                  ))\r\n\r\n\r\n\r\n\r\nSearching 10 DOIs in OpenAIRE retrieved 8 ORCIDs - 3 of which are the same as found in CrossRef (see the tables above) and placed in a column @orcid_pending. The other 5 ORCIDs is a result of OpenAIRE matching procedures.\r\nBut also.. the researchers’ names are present in 3 different formats and mixed with the organizations in one column. Not cool, of course.\r\nIf Lens and OpenAIRE could match ORCIDs, it is very likely that we also can do this.\r\nORCID (by DOI)\r\nORCID API allows to do search by many parameters by DOI.\r\nThe code below shows 3 functions that will be my helpers in communication with rorcid.\r\n\r\nShow code\r\n## function 1 - search ORCIDs by query\r\nget_orcid_by_doi <- function(query_doi){\r\n  orcid_doi(query_doi, rows = 10) %>% map_df(~.x) %>% mutate(doi = query_doi)\r\n}\r\n\r\n## function 2 - extract author details by ORCID \r\norcid_author_details <- function(orcid_ID){\r\n  xx <- orcid_id(orcid_ID)\r\n  \r\n  a1 <- map(xx,\"name\") %>%  \r\n    unlist() %>% enframe() %>% \r\n    filter(!grepl(\"-date\", name)) %>% \r\n    mutate(name = gsub(\"\\\\.value$\", \"\", name)) %>% \r\n    mutate(name = str_extract(name, \"(?<=\\\\.)[^\\\\.]+$\")) %>% \r\n    mutate_all(~as.character(.))\r\n  \r\n  a2 <- map(xx, \"external-identifiers\") %>% \r\n    map_df(\"external-identifier\")\r\n  if(exists(\"external-id-type\", a2)){\r\n    a2 <- a2 %>% \r\n      select(name = `external-id-type`, \r\n             value = `external-id-value`) %>% \r\n      mutate_all(~as.character(.))\r\n  } else {a2 <- tibble()}\r\n\r\n  xy <- orcid_employments(orcid_ID)\r\n  \r\n  a3 <- map(xy, \"affiliation-group\") %>% \r\n    map_df(\"summaries\") %>% \r\n    unlist() %>% enframe() %>% \r\n    mutate_all(~as.character(.))\r\n  if(nrow(a3)>0){\r\n    a3 <- a3 %>% filter(grepl(\"organization\", name))\r\n  } \r\n  if(nrow(a3)>0){\r\n    a3 <- a3 %>% \r\n      mutate(name = str_extract(name, \"(?<=\\\\.)[^\\\\.]+$\")) %>% \r\n      mutate(orcid = orcid_ID)\r\n  }\r\n  output <- bind_rows(a1, a2, a3) %>% mutate(orcid = orcid_ID)\r\n  return(output)\r\n}\r\n\r\n## function 3 - group author details by ORCID\r\norcid_author_details_group <- function(orcid_author_details_data){\r\n  orcid_author_details_data %>% \r\n  distinct() %>%\r\n  mutate(name = gsub(\"\\\\d+$\",\"\", name)) %>%   \r\n  group_by(doi, orcid, name) %>%\r\n  summarize(values = paste(unique(na.omit(value)), collapse = \" | \")) %>%\r\n  ungroup() %>%\r\n  pivot_wider(names_from = name, values_from = values) %>%\r\n  select(any_of(c(\"doi\", \"orcid\", \"orcid_family_name\" = \"family-name\", \r\n                  \"orcid_given_name\" = \"given-names\", \r\n                  \"ResearcherID\", \"Scopus_AuthID\" = \"Scopus Author ID\", \r\n                  \"orgname\" = \"name\", \"city\", \"country\", \r\n                  \"orgID\" = \"disambiguated-organization-identifier\", \r\n                  \"orgID_type\" = \"disambiguation-source\")))\r\n}\r\n\r\n\r\n\r\nAnd now we query the ORCID API using DOIs. It could be as simple as it sounds, but it is not. For 1 author the family and given names in ORCID are swapped. Therefore, I use stringdist package to fuzzy match the authors’ family names (as present in CrossRef) against both family and given names retrieved from ORCID. In a table below you may filter with 10.18288/1994-5124-2015-5-07 to see the record in question.\r\n\r\nShow code\r\norcid_doi_file <- paste0(dir, \"orcid_doi_data.RDS\")\r\n\r\nif(!file.exists(orcid_doi_file)){\r\n  orcid_by_doi_data <- data$doi %>% map_df(~get_orcid_by_doi(.x)) %>% \r\n    select(doi, orcid = contains(\"path\"))   \r\n\r\n  if(exists(\"orcid\", crf_data)){\r\n  orcid_by_doi_data <- orcid_by_doi_data %>% \r\n    bind_rows(crf_data[,c(\"doi\", \"orcid\")] %>% na.omit()) %>% \r\n    distinct()\r\n  }    \r\n  orcid_by_doi_details <- orcid_by_doi_data$orcid %>% \r\n    map_df(~orcid_author_details(.x)) %>% \r\n    left_join(orcid_by_doi_data, .) %>% \r\n    orcid_author_details_group()\r\n  \r\n  orcid_by_doi_details %>% write_rds(orcid_doi_file)\r\n} else {\r\n  orcid_by_doi_details <- read_rds(orcid_doi_file)\r\n}\r\n\r\ndata2 <- crf_mag_data %>% select(-orcid) %>% \r\n  left_join(orcid_by_doi_details) %>% \r\n   mutate(sim1 = sapply(stringsim(tolower(family), \r\n                                  tolower(orcid_family_name)), \r\n                        function(x) x)) %>% \r\n  mutate(sim2 = sapply(stringsim(tolower(family), \r\n                                  tolower(orcid_given_name)), \r\n                        function(x) x)) %>% \r\n  mutate(dec1 = case_when(\r\n      sim1>0.75 | sim2 > 0.75 ~ \"keep\",\r\n    TRUE ~ \"remove\")\r\n  )  %>% \r\n  filter(dec1 == \"keep\")\r\n\r\ncrf_mag_orc_data <- crf_mag_data %>% select(-orcid) %>% \r\n  left_join(data2) %>% \r\n  select(-sim1, -sim2, -dec1)\r\n\r\ndata2 %>% mutate_at(c(\"sim1\", \"sim2\"), ~round(.x,2)) %>% \r\n   datatable(rownames = FALSE, escape = FALSE, \r\n            editable = TRUE, class = 'compact striped',\r\n             caption = htmltools::tags$caption(style = 'caption-side: bottom; \r\n                text-align: left; font-size: 80%; color: #969696; font-family: Roboto Condensed;',\r\n               'Data: see in the text.'),\r\n            extensions = 'Buttons',\r\n            options = list(searchHighlight = TRUE,\r\n                            dom = 'Bfrtip', buttons = c('csv', \"excel\"), \r\n                           columnDefs = list(\r\n                  list(width = '150px', targets = c(0,13)),\r\n                   list(width = '350px', targets = c(4,18)),\r\n                  # list(width = '65px', targets = c(1,2)),\r\n                  list(className = 'dt-center', targets = c(2)))\r\n                  )\r\n            ) \r\n\r\n\r\n\r\n\r\nOur result: in addition to 3 ORCIDs found in CrossRef, our searching in ORCID by DOI helped to retrieve another 6 ORCIDs (including all those found in OpenAIRE. Searching in OpenAIRE could be skipped).\r\nBut 9 ORCIDs for 28 authors are also few. I guess that for the articles published in some high-brow international venues - those requiring to provide ORCIDs for each author - a proportion could be substantially higher. But this is no more, no less, but an economic journal from Russia.\r\nORCID (by Name)\r\nNow, at last, we can utilize the names in Cyrillic that we earlier found in Microsoft Academic.\r\nI made few additional steps:\r\ncreated the proper family and given names (i.e. removed the initials).\r\nlinked the authors’ order (in the list of authors) to the DOI - this will help me to match the new results to the original records further.\r\nFor 19 authors not having ORCIDs we combined the family names in Latin and Cyrillic - overall, 27 name variants and make a queries to ORCID.\r\n\r\nShow code\r\nxdata <- crf_mag_orc_data %>% filter(is.na(orcid)) %>% \r\n  select(doi, ord, cr_family = family, \r\n         cr_given = given, cr_affil = org_name, \r\n         mag_family, mag_given, mag_affil = AfN) %>%\r\n  pivot_longer(-c(1:2)) %>% \r\n  separate(col = \"name\", into = c(\"source\", \"variable\")) %>% \r\n  pivot_wider(names_from = variable, values_from = value) %>% \r\n  filter(!is.na(family)) %>% \r\n  mutate(given_token = str_extract(given, \"^[^\\\\s]+\")) %>% \r\n  select(doi, ord, family, given_token, affil) %>% \r\n  mutate_at(c(\"family\", \"given_token\", \"affil\"), \r\n            ~str_squish(tolower(.x))) %>% \r\n  distinct() %>% \r\n  group_by(doi, ord, family, given_token) %>% \r\n  summarize(affils = paste(unique(na.omit(affil)), \r\n                           collapse = \" | \")) %>% \r\n  ungroup() %>% \r\n  unite(col = \"doix\", c(\"doi\", \"ord\"), sep = \"|\")\r\n\r\nDT::datatable(xdata, rownames = F, \r\n        options = list(dom = \"Btip\", deferRender = TRUE, \r\n            ordering = TRUE, autoWidth = FALSE, scrollX = F))\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n## another function-helper\r\nget_orcid_by_name <- function(name_family, name_given, query_doi){\r\n  orcid_search(family_name = name_family, given_name = name_given, rows = 20) %>% \r\n    mutate(doi = query_doi)\r\n}\r\n\r\norcid_name_file <- paste0(dir, \"orcid_name_data.RDS\")\r\nif(!file.exists(orcid_name_file)){\r\n  orcid_by_name_data <- list(name_family = xdata$family, \r\n                             name_given = xdata$given_token, \r\n                             query_doi =  xdata$doix\r\n                             ) %>% \r\n    pmap(get_orcid_by_name) %>% \r\n    map_df(~.x)\r\n\r\n  orcid_by_name_details <- orcid_by_name_data$orcid %>% \r\n    map_df(~orcid_author_details(.x)) %>% \r\n    left_join(orcid_by_name_data, .) %>%   \r\n    orcid_author_details_group() %>% \r\n    arrange(doi, orcid_family_name) %>% \r\n    separate(col = doi, into = c(\"doi\", \"ord\"), sep = \"\\\\|\")\r\n\r\n    orcid_by_name_details %>% write_rds(orcid_name_file)\r\n} else {\r\n  orcid_by_name_details <- read_rds(orcid_name_file) \r\n} \r\n\r\norcid_by_name_details %>%   \r\n  DT::datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n             caption = htmltools::tags$caption(style = 'caption-side: bottom; \r\n                text-align: left; font-size: 80%; color: #969696; font-family: Roboto Condensed;',\r\n               'Data: orcid.org (see in the text).'),\r\n             extensions = 'Buttons',\r\n            options = list(searchHighlight = TRUE,\r\n                            dom = 'Bfrtip', buttons = c('csv', \"excel\"), \r\n                           columnDefs = list(\r\n                  list(width = '150px', targets = c(0,2,8)),\r\n                   list(width = '350px', targets = c(7)),\r\n                  list(className = 'dt-center', targets = c(2)))\r\n                  )\r\n            ) \r\n\r\n\r\n\r\n\r\nOur efforts brought another 21 ORCIDs. We would certainly get much more ORCIDs, if anyone from our author had a popular family name like Ivanov or Petrov.\r\nOh, let’s take a pause on Petrov. For some of my readers “Petrov from Russia” may raise an association with a fictional Russian president in House of Cards, or even worse - with a pseudo-tourist whose real name is certainly not Petrov and who is probably sent with a secret task to stain the door handles with some crazy chemical stuff. Even if you live not in a Western part of the world, but in, say, Australia, there was also Petrov who turned out to be fake Petrov (see Wikipedia: Petrov Affair). The Russian Petrovs seem to be sort of demonized, which is not good, so I am going to use this chance and give you few references to other, the real Petrovs. Here you are: (1) Wikipedia: Stanislav Petrov - a guy who saved the Western world from a nuclear war in 1983. Or if you like more sotries about the science and discovery, (2) Wikipedia: Vasily Petrov - a self-taught electrical technician who discovered and described an electric arc. None of Petrovs has asked me about this remark, it was just a pause.\r\nLet’s get back to the ORCIDs that we found by name and need to verify.\r\nIn its merged form a table look like below, all found ORCIDs are marked either as “by_doi” or as “by_name”.\r\n\r\nShow code\r\nfinal_data <- bind_rows(\r\n  ## found by doi\r\n  crf_mag_orc_data  %>% filter(!is.na(orcid)) %>% mutate(cred = \"by_doi\"),\r\n  ## found by name\r\n  crf_mag_data %>% select(-orcid) %>% \r\n    anti_join(data2, by = c(\"doi\", \"ord\")) %>% \r\n    left_join(orcid_by_name_details, by = c(\"doi\", \"ord\")) %>% \r\n    mutate(cred = ifelse(is.na(orcid), NA_character_, \"by_name\"))\r\n    ) %>%\r\n  distinct() %>% \r\n  arrange(doi, ord) %>% \r\n  relocate(doi, ord, cred)\r\n  \r\nfinal_data %>% \r\n  datatable(rownames = FALSE, escape = FALSE, \r\n            editable = TRUE, class = 'compact striped',\r\n             caption = htmltools::tags$caption(style = 'caption-side: bottom; text-align: left; font-size: 80%; color: #969696; font-family: Roboto Condensed;',\r\n               'Data: see in the text.'),\r\n            extensions = 'Buttons',\r\n            options = list(searchHighlight = TRUE,\r\n                            dom = 'Bfrtip', buttons = c('csv', \"excel\"), \r\n                           columnDefs = list(\r\n                  list(width = '150px', targets = c(0,14,20)),\r\n                  list(width = '350px', targets = c(5,19)),\r\n                  list(width = '65px', targets = c(1,2)),\r\n                  list(className = 'dt-center', targets = c(2)))\r\n                  )\r\n            ) \r\n\r\n\r\n\r\n\r\nWe have not so many options for checking here - the public details in many profiles are minimal, so automatic checking can hardly be fulfiled. Or can it?\r\nThe ways of checking:\r\nwe can make our decision by looking at the names and affiliation/employment info - some combinations of the family and given names are rare enough. Provided that there is another matching point (e.g. affiliation), the match can be confirmed. For example, I’ve never heard about anyone with the family name and given name like mine in any organizations I was affiliated. Moreover, in a whole Russia with 100+ mln population, there is no a researcher with such combination of family and given names (according to the national electronic library). This means that to identify me by name among the Russian authors is easy. The found ORCID for Nicolas Scelles shows the same affiliation (University of Stirling) as present in the original article (School of Sport University of Stirling). But can we be sure? Is this him? Is his name is rare enough? I can speak for the Russian names, but my perception of Nicolas Scelles can be misleading.\r\nwe can find in CrossRef all the publications associated with the found ORCIDs and select the ORCID having the highest score. The scores can be calculated based on the coauthorship, citation, contextual analysis.\r\nLet’s try (or at least consider these approaches):\r\nConfirmation by Co-Authorship\r\nIf there are 2 co-authors (А and B) for a particular article and we have found 2 ORCIDs for each author name (A1, A2, B1, B2), then we just need to check if there are any other publications where {A1/A2} and {B1/B2} are co-authors.\r\nIn our example there are 2 articles (out of selected 10) for which we have 2 or more authors with unconfirmed ORCIDs - overall, 2 articles, 9 authors, 14 found ORCID variants. Of course, we can do the queries checking each ORCID(A)-ORCID(B) pair (https://api.crossref.org/works?filter=orcid:{orcid1},orcid:{orcid2}&mailto={your_email}&rows=10&select=author), which even in our case would require 30+ queries.\r\nAlternatively, we can extract from CrossRef the metadata of all articles where those 14 ORCIDs are present, then build a co-authorship network, and check if any ORCIDs (candidates) are preent together in any other articles. This method has its limits, of course. For the articles in High-Energy Physics with their long author lists it will require a lot of adjustements. But for Russian journal in economics, where most of the authors are Russian (i.e. less ambiguity), the scenario may work.\r\n\r\nShow code\r\ncrossref_data_file2 <- paste0(dir,\"crossref_orcids.RDS\")\r\n\r\nif(!file.exists(crossref_data_file)){\r\n  cr_data_4_orcids <- unique_orcids_2_check$orcid %>% \r\n    map(~cr_works_(filter=c(orcid=.x), \r\n                   select = c('DOI', 'author', 'abstract', 'reference'), \r\n                   limit= 100) %>% \r\n          jsonlite::fromJSON(flatten = TRUE)\r\n      )\r\n  \r\n  write_rds(cr_data_4_orcids, crossref_data_file2)\r\n  } else {\r\n  cr_data_4_orcids <- read_rds(crossref_data_file2)\r\n}\r\n\r\ncr_data_4_orcids <- cr_data_4_orcids %>% \r\n  map_df(~.x %>% map(pluck,  \"items\") %>% map_df(~.x)) %>%  \r\n  unnest(author) %>%\r\n    unite(col = \"person\", c(\"family\", \"given\"), sep = \", \") %>% \r\n  select(DOI, ORCID, person) %>% distinct() %>% filter(!is.na(ORCID)) %>% \r\n  mutate(ORCID = gsub(\"http://orcid.org/\",\"\",ORCID))\r\n\r\nDT::datatable(cr_data_4_orcids, rownames = F, \r\n              options = list(dom = \"Btip\", deferRender = TRUE, \r\n                             ordering = TRUE, autoWidth = FALSE, scrollX = F))\r\n\r\n\r\n\r\n\r\nIn the table above we can see the ORCID-DOI combinations obtained from CrossRef for 14 ORCIDs. I added to this 9 ORCID-DOI pairs found earlier, and converted this into a network with a help of very convenient igraph and tidygraph packages.\r\n\r\nShow code\r\n## data for a network\r\norcid_network_data <- data2 %>% \r\n  unite(col = \"person\", c(\"family\", \"given\"), sep = \", \") %>% \r\n  select(DOI=doi, ORCID=orcid, person) %>% \r\n  bind_rows(cr_data_4_orcids) %>% \r\n  group_by(ORCID) %>% \r\n  mutate(len = nchar(person)) %>% \r\n  arrange(-len) %>% \r\n  mutate(person = person[1]) %>%\r\n  select(-len) %>% \r\n  ungroup()\r\n\r\n## creating a graph\r\norcid_network <- orcid_network_data %>% \r\n  select(from = ORCID, to = DOI) %>% \r\n  graph_from_data_frame(directed = FALSE) %>% \r\n  as_tbl_graph() %>%\r\n  ## setting attributes for edges\r\n  activate(edges) %>% \r\n  mutate(edge_vers = ifelse(edge_is_incident(which(.N()$name %in% data$doi)), \r\n                            \"initial\", \"new\")) %>% \r\n  ## setting attributes for nodes\r\n  activate(nodes) %>% \r\n  mutate(item_type = ifelse(name %in% orcid_network_data$DOI, \"doi\", \"orcid\")) %>% \r\n  mutate(orcid_type = case_when(\r\n    name %in% data2$orcid ~ \"by_DOI\",\r\n    name %in% orcid_by_name_details$orcid ~ \"by_Name\",   \r\n    TRUE ~ \"as co-authors\")\r\n  ) %>% \r\n  mutate(orcid_type = ifelse(orcid_type==\"as co-authors\" & item_type==\"doi\", \r\n                             NA_character_, orcid_type)\r\n         ) %>% \r\n  left_join(orcid_network_data %>% \r\n              filter(ORCID %in% data2$orcid|ORCID %in% orcid_by_name_details$orcid) %>%  \r\n              select(name = ORCID, person) %>% unique()) \r\n\r\nsummary(orcid_network)\r\n\r\n\r\nIGRAPH 3ef9d06 UN-- 73 92 -- \r\n+ attr: name (v/c), item_type (v/c), orcid_type (v/c), person\r\n| (v/c), edge_vers (e/c)\r\n\r\nThis summary output for a graph tells that we have got a graph with 73 nodes and 92 edges, the nodes have 3 attributes (item_type, orcid_type and person) and there is also one edge attribute (edge_vers).\r\n\r\nShow code\r\ngraph_pic <- paste0(getwd(), \"/images/graph.png\") \r\n\r\nif(!file.exists(graph_pic)){\r\n  gg <- orcid_network %>% ggraph(layout = \"fr\") +\r\n    geom_edge_link0(aes(color = edge_vers))+\r\n    geom_node_point(aes(shape = item_type , fill = orcid_type, size = item_type))+\r\n    geom_node_text(aes(label = gsub(\",\",\"\\n\", person), color = orcid_type), \r\n                   repel = TRUE, size = 2.8, hjust = 1, family = \"Roboto Condensed\", \r\n                   point.padding = 0.1, force = 0.5)+\r\n    scale_fill_manual(na.value = \"grey20\",\r\n      name = \"How ORCID\\nis found?\", \r\n      values = c(\"by_Name\"= \"lightgreen\", \"by_DOI\"=\"coral\",  \r\n                 \"as co-authors\" = \"grey90\"),\r\n      limits = c(\"by_Name\", \"by_DOI\", \"as co-authors\"))+\r\n    scale_color_manual(na.value = \"grey20\",\r\n      values = c(\"by_Name\"= \"#005824\", \"by_DOI\"=\"#e31a1c\", \r\n                 \"as co-authors\" = \"grey90\"),\r\n      limits = c(\"by_Name\", \"by_DOI\", \"as co-authors\"))+\r\n    scale_shape_manual(name = \"Type of Node\",\r\n                       values = c(\"orcid\"=22, \"doi\"=21))+\r\n    scale_size_manual(values = c(\"orcid\"=2, \"doi\"=1.5))+\r\n    scale_edge_color_manual(name = \"ORCID-DOI\\nlink\",\r\n      values = c(\"initial\" = \"red\", \"new\" = \"grey60\"))+\r\n    guides(fill = guide_legend(override.aes = list(shape = 22, size = 3)), \r\n           color = \"none\", size = \"none\",\r\n           shape = guide_legend(override.aes = list(size = 3))\r\n           )+\r\n    labs(title = \"Network of ORCIDs and DOIs\", \r\n         subtitle = \"(ORCIDs are found either by DOI in CrossRef or ORCID, or by Name)\", \r\n        caption = paste0(\"Date: \", format(Sys.time(), '%d/%m/%Y'),\r\n                           \"\\nData: crossref.org\"))+\r\n    theme_graph()+\r\n     theme(text=element_text(family=\"PT Sans\", color = \"#53565A\", size = 11),\r\n          legend.title = element_text(size=rel(1), color=\"grey10\"),\r\n          legend.text = element_text(size=rel(0.9), color=\"grey10\"),\r\n          legend.position = \"right\",\r\n          legend.box.background = element_rect(size = 0.2, \r\n                  color = \"grey90\", fill = \"white\"),\r\n          legend.box.margin = margin(10,0,10,0),\r\n          legend.box.spacing = unit(0,\"cm\"),\r\n          plot.caption.position = \"plot\",\r\n          plot.caption = element_text(size = rel(0.9), hjust=1, \r\n                            family = \"PT Sans\", color = \"#969696\", face = \"plain\"),\r\n          plot.subtitle = element_text(hjust=0, size = rel(1), \r\n                            family=\"PT Sans Narrow\"),\r\n          plot.title.position = \"plot\",\r\n          plot.title = element_text(size=rel(1.2), family=\"PT Sans\", \r\n                                    face=\"bold\", color = \"#253494\"),\r\n          plot.margin = margin(5, 5, 5, 5))  \r\n    \r\n  \r\n  ggsave(gg, file =  graph_pic, dpi = 400, width = 18, unit = \"cm\", height = 13)\r\n}\r\nknitr::include_graphics(graph_pic)\r\n\r\n\r\n\r\n\r\nEverything in red are the persons identified in CrossRef in the begining of our experiment (or when searched ORCID service by DOI). In green are the ORCIDs that were found by name.\r\nSee the nodes for Alexander Frenkel - there are 3 ones on the graph, and only one is connected with the other green nodes (which correspond to the co-authors of Alexander Frenkel in one of the initial 10 articles). And they have several co-authored articles (black dots), so we can make a decision that the ORCIDs behind those 3 green nodes correspond to the same authos (Frenkel, Tikhomirov, and Sergienko) that appear in the original article. We can qualify them as a good match!\r\nIn contrast, Durand Christophe has a lot of grey nodes (which are the ORCIDs of other authors) and no green ones. Actually, in some publications he co-authored people “we are looking for”, but the metadata for those articles in CrossRef do not contain ORCIDs (just one for Durand Christophe). And Andreff Wladimir and Nicolas Scelles are also here with their own articles (which means that there are some publications in CrossRef listing their ORCIDs), but they are not connected to a green node associated with Durand Christophe. Thus, based on co-authorship we can not confirm that the ORCIDs for Andreff Wladimir or Nicolas Scelles are a correct match.\r\nOf course, this selection can be made by script - just select the graph components (linked group of nodes) having 2 or more green nodes, and filter them down.\r\n\r\nShow code\r\norcid_network %>% components() %>% \r\n  .[[\"membership\"]] %>% enframe() %>% \r\n  inner_join(orcid_by_name_details %>% \r\n               select(name = orcid, doi, ord) %>% \r\n               distinct()) %>% \r\n  add_count(value) %>% filter(n>1) %>%\r\n  select(doi, ord, ORCID = name) %>% \r\n  left_join(crf_data %>% \r\n              select(doi, ord, family, given)) %>% \r\n  arrange(doi, ord) %>% \r\n  DT::datatable(rownames = F, \r\n                options = list(dom = \"t\", deferRender = TRUE, \r\n                               ordering = TRUE, autoWidth = FALSE, \r\n                               scrollX = F))\r\n\r\n\r\n\r\n\r\nConfirmation by Citation\r\nLet’s look at Nicolas Scelles. We have one his publication in the initial set of DOIs (10.18288/1994-5124-2016-3-06) and 3 other are found in CrossRef (10.3390/ijfs5040025, 10.3390/economies9010031, 10.1111/ssqu.12782) using the ORCID that corresponds to someone who is also named Nicolas Scelles.\r\nCan we make a fair judgememt that these 2 set of articles relate to one person?\r\nWe can use a package citecorp, which is a client for Open Citations database. It will not return you all the citations, as part of publishers still restrain distribution of the reference lists they deposited to CrossRef. Here we mix both citing and cited works for each DOI.\r\n\r\nShow code\r\ngraph_pic2 <- paste0(getwd(), \"/images/graph2.png\") \r\n\r\nif(!file.exists(graph_pic2)){\r\ncit_graph1 <- bind_rows(\r\n  crf_data$doi[grepl(\"Scelles\", crf_data$family)] %>% \r\n    map_df(~oc_coci_refs(.x)),\r\n  crf_data$doi[grepl(\"Scelles\", crf_data$family)] %>% \r\n    map_df(~oc_coci_cites(.x))\r\n) %>% select(from = citing, to = cited) %>% \r\n  graph_from_data_frame(directed = TRUE) %>% as_tbl_graph()\r\n  \r\ncit_graph2 <-  bind_rows(\r\n  cr_data_4_orcids$DOI[grepl(\"Scelles\", cr_data_4_orcids$person)] %>% \r\n    map_df(~oc_coci_refs(.x)),\r\n  cr_data_4_orcids$DOI[grepl(\"Scelles\", cr_data_4_orcids$person)] %>% \r\n    map_df(~oc_coci_cites(.x))\r\n) %>% select(from = citing, to = cited) %>% \r\n  graph_from_data_frame(directed = TRUE) %>% as_tbl_graph() \r\n\r\n\r\ngg2 <- graph_join(cit_graph1, cit_graph2) %>% \r\n    mutate(color = case_when(\r\n      name %in% cr_data_4_orcids$DOI[grepl(\"Scelles\", cr_data_4_orcids$person)] ~ \"green\", \r\n      name %in% crf_data$doi[grepl(\"Scelles\", crf_data$family)] ~ \"coral\",\r\n      TRUE ~ \"grey30\")\r\n      )%>% \r\n    ggraph(layout = \"stress\") + \r\n    geom_node_point(aes(x = x, y = y, color = color), size = 3) +\r\n    geom_edge_link(arrow = arrow(angle = 15, length = unit(0.1, \"inches\")), \r\n                   label_alpha = 1,\r\n                   start_cap = circle(0.1, 'cm'), \r\n                   end_cap = circle(0.1, 'cm'), \r\n                   edge_alpha = 0.5)+\r\n    scale_color_identity()+\r\n        guides(fill = guide_legend(override.aes = list(shape = 22, size = 3)), \r\n             color = \"none\", size = \"none\",\r\n             shape = guide_legend(override.aes = list(size = 3))\r\n             )+\r\n      labs(title = \"Citation Network for DOIs\", \r\n           subtitle = \"(red DOI is inital, green are found via DOIs, others are the refs or citing docs)\", \r\n          caption = paste0(\"Date: \", format(Sys.time(), '%d/%m/%Y'),\r\n                             \"\\nData: crossref.org\"))+\r\n      theme_graph()+\r\n       theme(text=element_text(family=\"PT Sans\", color = \"#53565A\", size = 11),\r\n            legend.title = element_text(size=rel(1), color=\"grey10\"),\r\n            legend.text = element_text(size=rel(0.9), color=\"grey10\"),\r\n            legend.position = \"right\",\r\n            legend.box.background = element_rect(size = 0.2, \r\n                    color = \"grey90\", fill = \"white\"),\r\n            legend.box.margin = margin(10,0,10,0),\r\n            legend.box.spacing = unit(0,\"cm\"),\r\n            plot.caption.position = \"plot\",\r\n            plot.caption = element_text(size = rel(0.9), hjust=1, \r\n                              family = \"PT Sans\", color = \"#969696\", face = \"plain\"),\r\n            plot.subtitle = element_text(hjust=0, size = rel(1), \r\n                              family=\"PT Sans Narrow\"),\r\n            plot.title.position = \"plot\",\r\n            plot.title = element_text(size=rel(1.2), family=\"PT Sans\", \r\n                                      face=\"bold\", color = \"#253494\"),\r\n            plot.margin = margin(5, 5, 5, 5))  \r\n      \r\n  ggsave(gg2, file =  graph_pic2, dpi = 400, width = 18, unit = \"cm\", height = 13)\r\n}\r\nknitr::include_graphics(graph_pic2)\r\n\r\n\r\n\r\n\r\nOut of 3 DOIs that we found in CrossRef using the ORCID (suggested by the name of Nicolas Scelles), only 2 had the references. And the initial DOI has no references in CrossRef as well. So what we can see on picture is that 2 articles have non-overlapping reference lists (maybe this is quite OK for economists, I think, that in experimental disciplines like Molecular biology, the overlap could be larger).\r\nCan we qualify the found ORCID as indeed belonging to Nicolas Scelles, an author of one of the initial publications? Well, for this author the link is pretty weak, but the affiliations in the initial article and in ORCID profile support such match.\r\nSo we have another one. This approach heavily depends on availability of the references in CrossRef, a subject area, and other factors (like age of researcher, their productivity, etc).\r\nConfirmation by Context\r\nFor the initial article and for the articles found in CrossRef by ORCID (found by name), we can try to get the abstracts or the full texts. This can be done via CrossRef, preprint services, publisher sites with a help of different APIs (some useful packages are listed above). And then there are many techniques to measure a proximity of 2 texts (keywords, topics, SVM, etc). This part is really huge to be described in this post. My apologies for skipping this.\r\nFinal Result\r\nAfter having checked the other ORCIDs, I put all of them the into a final table (see below). The column score shows for each author one of 3 values - A (match is confirmed), B (uncertain). C (no ORCID is found).\r\n\r\nShow code\r\nfinal_data_checked <- paste0(dir, \"/final.data.xlsx\") %>% \r\n  readxl::read_xlsx(col_types = \"text\")\r\n\r\nfinal_data_checked %>% \r\n  filter(score!=\"D\") %>% \r\n  arrange(doi, score) %>% \r\n  datatable(rownames = FALSE, escape = FALSE, class = 'compact striped',\r\n             caption = htmltools::tags$caption(style = 'caption-side: bottom; text-align: left; font-size: 80%; color: #969696; font-family: Roboto Condensed;',\r\n               'Data: crossref.org (CRF); orcid.org; academic.microsoft.com (MAG). See the details in the text.'),\r\n            extensions = 'Buttons',\r\n            options = list(searchHighlight = TRUE,\r\n                            dom = 'Bfrtip', buttons = c('csv', \"excel\"), \r\n                           columnDefs = list(\r\n                  list(width = '150px', targets = c(0,14,22)),\r\n                  list(width = '350px', targets = c(6,19)),\r\n                  list(width = '65px', targets = c(1,2)),\r\n                  list(className = 'dt-center', targets = c(1,2)))\r\n                  )\r\n            ) \r\n\r\n\r\n\r\n\r\nWe had just 3 ORCIDs in the CrossRef metadata (for 10 selected DOIs), then we collected additional 6 by searching the ORCID service by DOI, and then another 9 ORCIDs were qualified as a correct match with additional methods (matching by co-authorship, by citation network analysis, by contextual analysis, by affiliation and subject proximity). Those additional methods required an additional human control - but overall, 18 ORCIDs for 28 authors.\r\nThe final part of this should have been about matching the ORCIDs and Organization names/IDs to Wikidata items, and uploading it to Wikidata records for the articles. In my opinion, this deserves a detailed description, so the post would be very-very long, so I decided to stop here and postpone a part about Wikidata to the next post.\r\np.s. when I finished editing the last lines, I realized that I’ve lost my sense of smell. I hope that this will not delay the next part too much.\r\nAcknowledgments\r\nAllaire J, Xie Y, McPherson J, Luraschi J, Ushey K, Atkins A, Wickham H, Cheng J, Chang W, Iannone R (2021). rmarkdown: Dynamic Documents for R. R package version 2.7, <URL: https://github.com/rstudio/rmarkdown>.\r\nChamberlain S (2021). rorcid: Interface to the ‘Orcid.org’ API. R package version 0.7.0, <URL: https://CRAN.R-project.org/package=rorcid>.\r\nChamberlain S, Baker C (2020). microdemic: ‘Microsoft Academic’ API Client. R package version 0.6.0, <URL: https://CRAN.R-project.org/package=microdemic>.\r\nChamberlain S, Baker C (2020). microdemic: ‘Microsoft Academic’ API Client. R package version 0.6.0, <URL: https://CRAN.R-project.org/package=microdemic>.\r\nChamberlain S, Zhu H, Jahn N, Boettiger C, Ram K (2020). rcrossref: Client for Various ‘CrossRef’ ‘APIs’. R package version 1.1.0, <URL: https://CRAN.R-project.org/package=rcrossref>.\r\nChang, W (2014). extrafont: Tools for using fonts. R package version 0.17, <URL: https://CRAN.R-project.org/package=extrafont>.\r\nCsardi G, Nepusz T (2006). “The igraph software package for complex network research.” InterJournal, Complex Systems, 1695. <URL: https://igraph.org>.\r\nHenry L, Wickham H (2020). purrr: Functional Programming Tools. R package version 0.3.4, <URL: https://CRAN.R-project.org/package=purrr>.\r\nOoms J (2014). “The jsonlite Package: A Practical and Consistent Mapping Between JSON Data and R Objects.” arXiv:1403.2805 [stat.CO]. <URL: https://arxiv.org/abs/1403.2805>.\r\nPedersen T (2021). ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. R package version 2.0.5, <URL: https://CRAN.R-project.org/package=ggraph>.\r\nPedersen T (2020). tidygraph: A Tidy API for Graph Manipulation. R package version 1.2.0, <URL: https://CRAN.R-project.org/package=tidygraph>.\r\nvan der Loo M (2014). “The stringdist package for approximate string matching.” The R Journal, 6, 111-122. <URL: https://CRAN.R-project.org/package=stringdist>.\r\nWickham H (2020). tidyr: Tidy Messy Data. R package version 1.1.2, <URL: https://CRAN.R-project.org/package=tidyr>.\r\nWickham H (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN 978-3-319-24277-4, <URL: https://ggplot2.tidyverse.org>.\r\nWickham H (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=stringr>.\r\nWickham H, Francois R, Henry L, Muller K (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.3, <URL: https://CRAN.R-project.org/package=dplyr>.\r\nWickham H, Hester J (2020). readr: Read Rectangular Text Data. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=readr>.\r\nWickham H, Seidel D (2020). scales: Scale Functions for Visualization. R package version 1.1.1, <URL: https://CRAN.R-project.org/package=scales>.\r\nXie Y (2020). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.30, <URL: https://yihui.org/knitr/>.\r\nXie Y (2015). Dynamic Documents with R and knitr, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, <URL: https://yihui.org/knitr/>.\r\nXie Y (2014). “knitr: A Comprehensive Tool for Reproducible Research in R.” In Stodden V, Leisch F, Peng RD (eds.), Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595, <URL: http://www.crcpress.com/product/isbn/9781466561595>.\r\nXie Y, Allaire J, Grolemund G (2018). R Markdown: The Definitive Guide. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9781138359338, <URL: https://bookdown.org/yihui/rmarkdown>.\r\nXie Y, Cheng J, Tan X (2021). DT: A Wrapper of the JavaScript Library ‘DataTables’. R package version 0.17, <URL: https://CRAN.R-project.org/package=DT>.\r\nXie Y, Dervieux C, Riederer E (2020). R Markdown Cookbook. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9780367563837, <URL: https://bookdown.org/yihui/rmarkdown-cookbook>.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-28-post-publication-collecting-orcids-for-the-authors/images/graph.png",
    "last_modified": "2021-06-27T17:52:31+03:00",
    "input_file": {},
    "preview_width": 2834,
    "preview_height": 2047
  },
  {
    "path": "posts/2021-06-15-tagging-the-abstracts-with-wikidata-items/",
    "title": "Tagging the Scientific Abstracts with Wikidata Items",
    "description": "Here I am trying to build a script that process the short scientific texts (abstracts) and finds Wikidata items corresponding to the terms. An interactive and editable table is also created to allow an editor to validate the found matches and find other related items. A bit amateurish attempt by a Wikidata newbie.",
    "author": [
      {
        "name": "Aleksei Lutai",
        "url": "https://www.linkedin.com/in/lutaya/"
      }
    ],
    "date": "2021-06-16",
    "categories": [
      "wikidata",
      "udpipe",
      "api",
      "text mapping",
      "sparql"
    ],
    "contents": "\r\n\r\nContents\r\nSize matters(?)\r\nData\r\nLemmatization and POS-analysis, udpipe\r\nSearching a term in Wikidata\r\nThe results\r\nCase 1\r\nCase 2\r\nCase 3\r\nCase 4\r\nCase 5\r\nFinal Results\r\nLimitations\r\nAcknowledgments\r\n\r\nWith every day Wikidata occupies me more and more. Having played a bit with the academic journals and the institutions, I started thinking how to extract the relavent Wikidata items for any text and use them for tagging.\r\nMy inner voice was like: ‘If Wikidata is an open database, there should be a lot of solutions, you just need to find a proper tutorial’. Aha, the tutorials, a little devil.\r\nThese were the options I found:\r\nWikimedia community has a lot of tools, some of which are designed for tagging. One of it, Mix’n’match seems to be created specifically for tagging. It also provides an option to download the catalogues and dictionaries matched to Wikidata items. There could also be some other tools - Wikipedia Weekly Network/Live Wikidata Editing offers an impressive collection of video episodes that can serve a tutorial function. I have watched just few so far (will go on). But at that moment I was hoping to find a “magical” API to minimize the manual routine.\r\nScholia - a brilliant tool, offers Text2Topic convertor, which also requires a manual input. Its open code is available at Github, but my R+/P- phenotype leaves me no chance for its adoption.\r\nThere is also an R-solution for text search in Wikidata - a new WikidataR package (still in development) offers a “find item” function. It is a wrapper for Wikimedia API module named wbsearchentities that does a very generic search. Try to do query a term “galaxy” and in addition to an astronomical structure, you will have the LA footbal club, military aircraft, US record label, etc, etc.\r\nAnd there are many examples of SPARQL queries for Wikidata Query Service, where I also took some ideas from.\r\nFinally I decided to create my own “synthetic” approach to make a search more specific and automate text tagging. In other words, another nube who pleaded ignorance for a pleasure of re-creating a bycicle. That’s one of the best privilieges the neophytes can enjoy, isn’t it?\r\nSize matters(?)\r\nWhen it comes to scientific articles, there are many more sophisticated (and well developed) techniques for keyword extraction and topic modelling.\r\nthe authors usually suggest the keywords (not always good ones though), which could be used for initial setting of subject area;\r\nthe indexing services (like Medline, Semantics Scholar, Scopus) also assign the terms (not equally well for all subjects though). Some of them provide free API.\r\nthe article have references and (sometimes) the citations - they can be used to find an article’s position in the citation graph and to extract more contextual information from its closest neighbours. COCI seems to be the most practical option for this, while all the World is waiting for OpenAlex aimed for substitution of Microsoft Academic.\r\nwith Open Access development the full texts are getting more available for text data mining (TDM). If you have a facility to do ML on many millions of full-text documents, CORE and Unpaywall can provide you with the data.\r\nSo a large-scale solution is likely to be about using ML on the abovementioned datasets for topic modelling and further fuzzy matching against Wikidata dump.\r\nThis post is very much not about such scale of solution. Let’s pretend that we have just a piece of text (no references, no DOI, no keywords) and there is no budget for IT muscles (does it sound like an editorial department for many journals?).\r\nData\r\nAs a coverage of scientific terms in Wikidata varies between the different subjects, I decided to try my solution on the abstracts from Science (AAAS, issn:1095-9203), as subject-agnostic academic journal. I utilized CrossRef API example to obtain the abstracts of 5 random articles published after Jan 2020.\r\nYou can see them in a table below.\r\n\r\nShow code\r\ndata <- dir %>% list.files(full.names = TRUE) %>% .[grepl(\"crworks.json\",.)] %>% \r\n  fromJSON(flatten = TRUE) %>% \r\n  map(\"items\") %>% .$message %>%   \r\n  mutate(abstract = str_replace_all(abstract, \"<.+?>\",\"\")) %>% \r\n  mutate(abstract = stringi::stri_trans_general(abstract, 'nfc; latin')) %>% \r\n  mutate(abstract = str_squish(abstract))\r\n\r\ndata %>% \r\n  #mutate(abstract - substr(abstract, 1,250)) %>% \r\n  #summarize(abstracts = paste(abstract, collapse  = \" ...\")) %>% \r\n  datatable(rownames = FALSE, options = list(dom = \"tip\", pageLength = 1 ))\r\n\r\n\r\n\r\n\r\nOur next step would be to extract from the abstracts the words and their combinations that can be a scientific term and have an item in Wikidata. I am not a linguist, but from what I read it seemed to me that there is a consensus that most terms are nouns - as a sole word (galaxy), or in a combination (coronavirus disease), or preceded with the adjective (clinical trial).\r\nLemmatization and POS-analysis, udpipe\r\nThe package udpipe offers a set of functions for Tokenization, Parts-of-Speech (POS) tagging, Lemmatization, Dependency Parsing, etc.\r\nThe process I am using below is described here with few exceptions:\r\nudpipe tokenizer splits the words by hyphens (no chance for SARS-CoV-2), so I tokenized the abstracts with regular expression.\r\nin POS-analysis udpipe tags the pronouns as nouns (N) and the numericals as adjectives (A). Therefore, I removed them (“which” or “37” are unlikely to be a sensible terms) at some level. This does not affect the numbers in cov-2 or covid-19, as the hyphens are not cleaved.\r\nMore details are in the code below.\r\n\r\nShow code\r\n# to avoid running the chunks I saved the results on hard disk \r\nfile_d_terms <- paste0(dir, \"data_terms.RDS\")\r\n\r\nif(!file.exists(file_d_terms)){\r\n  # for the first run, model_dir is not used\r\n  # the library is getting downloaded from www (default)\r\n  udmodel <- udpipe_download_model(\r\n    model_dir = paste0(onedrive, \"/Wikidata/Science/\"), \r\n    language = \"english\", \r\n    overwrite = FALSE\r\n    )\r\n  udmodel <- udpipe_load_model(file = udmodel$file_model)\r\n  \r\n  # udpipe breaks by hyphens, so I use str_extract_all with a regex expr.\r\n  datax <- data$abstract %>% \r\n    map(~str_extract_all(.x,\"[[:alnum:]\\\\-]+\") %>% \r\n          map_chr(~paste0(.x, collapse = \"\\n\"))) %>% \r\n    setNames(LETTERS[1:5]) %>%\r\n    map(~udpipe_annotate(object = udmodel, x = .x, \r\n                         tokenizer = \"vertical\") %>%\r\n          as.data.frame() %>% \r\n          mutate(phrase_tag = as_phrasemachine(upos, type = \"upos\")) %>% \r\n          mutate(lemma = tolower(lemma))) \r\n     \r\n  data_terms <- datax %>%\r\n    # udpipe tags pronouns (PRON) similar as nouns (N)\r\n    # I remove the pronons before keywords_phrases\r\n    map(~.x %>% filter(upos!=\"PRON\")) %>%\r\n    # udpipe tags numericals (NUM) similar as adjectives (A)\r\n    # I remove the numericals before keywords_phrases\r\n    map(~.x %>% filter(upos!=\"NUM\")) %>%\r\n    map(~keywords_phrases(x = .x$phrase_tag, term = .x$lemma, \r\n                      pattern = \"N|AN|NN\", is_regex = TRUE, \r\n                      ngram_max = 2, detailed = FALSE) %>% \r\n          select(keyword) %>% filter(nchar(keyword)>2))\r\n  \r\n  # saving the results on disk as RDS file for further using \r\n  list(\r\n    datax = datax,\r\n    data_terms = data_terms\r\n    ) %>% write_rds(file_d_terms)\r\n} else {\r\n  datax <- read_rds(file_d_terms) %>% .[[\"datax\"]] \r\n  data_terms <- read_rds(file_d_terms) %>% .[[\"data_terms\"]] \r\n}\r\n\r\n\r\n\r\nFor each abstract we produced 2 datasets (in my code packed in the lists), containing:\r\nthe results of lemmatization and POS-tagging\r\n\r\nShow code\r\ndatax$A %>% datatable(rownames = FALSE)\r\n\r\n\r\n\r\n\r\nthe keyword phrases (N, N+N, A+N) to be used for searching Wikidata\r\n\r\nShow code\r\ndata_terms$A %>% summarize(keywords = paste(keyword, collapse  = \" | \")) %>% \r\n  datatable(rownames = FALSE, options = list(dom = \"t\"))\r\n\r\n\r\n\r\n\r\nThe terms and phrases seems OK to me, except that I would qualify “allosteric” as an adjective.\r\nNow as we have the terms it’s time to build a search function that will be retrieving the relevant Wikidata items.\r\nSearching a term in Wikidata\r\nIn order to increase a specificity of search (i.e. to retrieve more scientific terms and less “footbal teams” or “rockstar aliases”), I decided to do queries in SPARQL via Wikidata Query Service withe special filters.\r\nThe SPARQL query has the following conditions that:\r\nretrieve the search results from wikibase API\r\nretrieve a number of site links for Wikidata item\r\ncheck against the dictionaries and thesauri (a long chain of wdt:P_ in a code. Some thesauri have direct relations to the scientific concepts (like MeSH - P486, ChEBI ID - P683, Semantic Scholar topic ID - P6611), the others are rather dictionaries and encyclopedias (like Oxford Classical Dictionary - P9106, or Enciclopaedia Britannica - P1417). You should be aware that those terms are also not completely matched to Wikidata items (see Mix’n’Match for particular catallogues).\r\nexcludes disambiguation wikimedia pages (Q4167410) - there’s over 1M such pages in Wikidata.\r\nfilters only English terms\r\nfilters the terms found at least in 3 thesauri\r\nfilters the items that have the same start as the query (e.g. it retrieves not only “neutron” but also a “neutron star”). The filter based on the regular expression leaves a lot of flexibility - e.g. you can also set the strict matching by enframing the terms with ^ and $.\r\nscores the filtered results based on a number of sitelinks (and further by number of dictionaries the term was found in).\r\n\r\nShow code\r\nsparql_composer <- function(term){\r\n  paste0('SELECT ?item (SAMPLE(?itemLabel) as ?item_label)\r\n          (SAMPLE(?typeLabel) as ?entity_type) ?itemDescription \r\n          ?sites (COUNT(distinct(?id)) AS ?count)\r\n    WHERE {hint:Query hint:optimizer \"None\".\r\n      SERVICE wikibase:mwapi {\r\n          bd:serviceParam wikibase:endpoint \"www.wikidata.org\";\r\n           wikibase:api \"EntitySearch\";\r\n            mwapi:search \"', term, '\"; \r\n            mwapi:language \"en\".\r\n          ?item wikibase:apiOutputItem mwapi:item.\r\n      }\r\n      FILTER BOUND (?item)     \r\n      optional{?item wikibase:sitelinks ?sites.}\r\n      ?item wdt:P1417|wdt:P486|wdt:P683|wdt:P6366|wdt:P3916|\r\n            wdt:P227|wdt:P6366|wdt:P244|wdt:P4732|wdt:P231|\r\n            wdt:P1014|wdt:P7859|wdt:P949|wdt:P2671|wdt:P6611|\r\n            wdt:P268|wdt:P2163|wdt:P2581|wdt:P5019|wdt:P646|\r\n            wdt:P2924|wdt:P9106|wdt:P4212|wdt:P3123|wdt:P2347|\r\n            wdt:P1692|wdt:P8814|wdt:P699|wdt:P3219 ?id.\r\n      ?item wdt:P31|wdt:P279 ?type. \r\n      ?item rdfs:label ?itemLabel.\r\n      FILTER(LANGMATCHES(LANG(?itemLabel), \"en\")).\r\n      FILTER REGEX(LCASE(?itemLabel), \"^', term, '\"). \r\n      MINUS {?item wdt:P31 wd:Q4167410}\r\n      SERVICE wikibase:label {\r\n          bd:serviceParam wikibase:language \"en\".\r\n          ?type rdfs:label ?typeLabel.\r\n          ?item schema:description ?itemDescription.\r\n          }  \r\n    }\r\n    group by ?item ?itemDescription ?sites\r\n    HAVING ( ?count > 2 )\r\n    ORDER BY DESC(?sites) DESC(?count) \r\n    LIMIT 2')\r\n} \r\n\r\n\r\n\r\nI tried to use the development version of WikidataR (https://github.com/TS404/WikidataR) for SPARQL queries, but found that it contained a little bug that incorrectly processed the output results with 1 row and prevent putting the function under map_df() control. I updated the code a bit.\r\n\r\nShow code\r\nwd_query <- function(query, format = \"simple\", ...){\r\n  output <- WikidataQueryServiceR::query_wikidata(sparql_query = query, \r\n                    format = format, ...)\r\n  output <- tibble(data.frame(output)) %>% \r\n    mutate_all(~ifelse(grepl(\"Q\\\\d+$\",.x), str_extract(.x, \"Q\\\\d+$\"), .x))\r\n  if (nrow(output) == 0) {output <- tibble(value = NA)}\r\n  return(output)\r\n}\r\n\r\n\r\n\r\nNext I took a vector of unique terms and made a chain of queries with map_df (I could pass a full vector into the request but in that case I would not see in the results which result corrsponds to which term).\r\n\r\nShow code\r\nfile_w_terms <- paste0(dir, \"/terms.csv\")\r\nif(!file.exists(file_w_terms)){\r\n  wiki_terms <- unlist(data_terms) %>% unique() %>% \r\n    map_df(~wd_query(sparql_composer(.x), format = \"simple\") %>% \r\n                  mutate(query = .x) %>% \r\n                  mutate_all(~as.character(.x))\r\n                 ) %>%  \r\n    filter(!is.na(item)) %>% \r\n    select(query, item, item_label, entity_type, \r\n           itemDescription, sites, count) %>% \r\n    arrange(query)\r\n  write_excel_csv(wiki_terms, file_w_terms)\r\n} else {\r\n  wiki_terms <- read_csv(file_w_terms)\r\n}\r\n\r\ndata_edit <- data_terms %>% \r\n  map(~.x %>% \r\n        left_join(wiki_terms, by = c(\"keyword\" = \"query\")) %>% \r\n        filter(!is.na(item))\r\n  )\r\n\r\n\r\n\r\nThe results\r\nThe retrieved results are still far from being 100% specific and need to be validated. For this I created a prototype of checking template - an interactive DT table that:\r\nshows the text excerpts containg the term (+/- 2 words around)\r\nhighlights the terms\r\nprovides a description of the found Wikidata item\r\ncan be edited (right here! Click on ? and change it to “yes” in valid? column)\r\ncan be downloaded to CSV or XLSX file with the introduced changes.\r\n\r\nShow code\r\ntables_rds <- paste0(dir, \"editable_tables.rds\")\r\n\r\nif(!file.exists(tables_rds)){\r\n  tables <- list()\r\n\r\n  for (m in 1:nrow(data)){\r\n    lemma_text <- tolower(paste0(unlist(datax[[m]][\"lemma\"]), collapse = \" \"))\r\n\r\n    y <- data_edit[[m]] %>%\r\n      mutate(seltext = gsub(\" \",\".{0,5}\",keyword)) %>% \r\n      mutate(painter = paste0(\"^\",seltext,\"|\", seltext)) %>%\r\n      mutate(extractor = paste0('((?:\\\\S+\\\\s+){0,2}\\\\b',\r\n                            seltext,\r\n                      '.??\\\\b(\\\\s*|\\\\.)(?:\\\\S+\\\\b\\\\s*){0,2})')) %>% \r\n      mutate(extractor = sapply(extractor, \r\n                  function(x) paste0(\"^\",extractor,\r\n                                     \"|\", extractor))) %>%  \r\n      mutate(text = \"\") \r\n  \r\n    for (i in 1:nrow(y)){\r\n      y[i, \"text\"] <- str_extract_all(lemma_text, y$extractor[i], \r\n                                      simplify = TRUE) %>% \r\n        paste0(\"...\",.,\"...\") %>% paste(collapse = \"\")\r\n      \r\n      y[i, \"text\"] <- str_replace_all(y$text[i], y$painter[i],\r\n                        paste0('<span style=\"background-color: #FEE1E8\">',\r\n                               y$keyword[i],'<\/span>'))\r\n    }  \r\n\r\n  tables[[m]] <- y %>% \r\n    mutate(item_label = paste0(\"<b>label:<\/b> \", toupper(item_label)), \r\n           entity_type = paste0(\"<b>type:<\/b> \", entity_type),\r\n           itemDescription = paste0(\"<i>\", itemDescription, \"<i>\")) %>%\r\n    unite(col = \"details\", c(\"item_label\", \"entity_type\", \r\n                             \"itemDescription\"), sep = \"<\/br>\") %>% \r\n    mutate(\"valid?\" = \"?\")\r\n  }\r\n  write_rds(tables, tables_rds)\r\n  } else {\r\n    tables <- read_rds(tables_rds)\r\n  }\r\n\r\n\r\n\r\nSo there is an editable table in an automatically generated HTML-report that anyone can revise and save.\r\nCase 1\r\n\r\nShow code\r\ntables[[1]] %>% \r\n  select(text, item, `valid?`, details) %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, #filter = 'top', \r\n            editable = TRUE, class = 'compact striped',\r\n             caption = htmltools::tags$caption(style = 'caption-side: bottom; text-align: left; font-size: 80%; color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org (see in the text).'),\r\n            extensions = 'Buttons',\r\n            options = list(searchHighlight = TRUE,\r\n                            dom = 'Bfrtip', buttons = c('csv', \"excel\"), \r\n                           columnDefs = list(\r\n                  list(width = '300px', targets = c(0)),\r\n                  list(width = '500px', targets = c(3)),\r\n                  list(width = '65px', targets = c(1,2)),\r\n                  list(className = 'dt-center', targets = c(2)))\r\n                  )\r\n            ) %>% \r\n     formatStyle('valid?',  backgroundColor = styleEqual('yes', '#90ee90'), fontWeight = 'bold') \r\n\r\n\r\n\r\n\r\nCase 2\r\n\r\nShow code\r\ntables[[2]] %>% \r\n  select(text, item, `valid?`, details) %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, #filter = 'top', \r\n            editable = TRUE, class = 'compact striped',\r\n             caption = htmltools::tags$caption(style = 'caption-side: bottom; text-align: left; font-size: 80%; color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org (see in the text).'),\r\n            extensions = 'Buttons',\r\n            options = list(searchHighlight = TRUE,\r\n                            dom = 'Bfrtip', buttons = c('csv', \"excel\"), \r\n                           columnDefs = list(\r\n                  list(width = '300px', targets = c(0)),\r\n                  list(width = '500px', targets = c(3)),\r\n                  list(width = '65px', targets = c(1,2)),\r\n                  list(className = 'dt-center', targets = c(2)))\r\n                  )\r\n            ) %>% \r\n     formatStyle('valid?',  backgroundColor = styleEqual('yes', '#90ee90'), fontWeight = 'bold') \r\n\r\n\r\n\r\n\r\nCase 3\r\n\r\nShow code\r\ntables[[3]] %>% \r\n  select(text, item, `valid?`, details) %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, #filter = 'top', \r\n            editable = TRUE, class = 'compact striped',\r\n             caption = htmltools::tags$caption(style = 'caption-side: bottom; text-align: left; font-size: 80%; color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org (see in the text).'),\r\n            extensions = 'Buttons',\r\n            options = list(searchHighlight = TRUE,\r\n                            dom = 'Bfrtip', buttons = c('csv', \"excel\"), \r\n                           columnDefs = list(\r\n                  list(width = '300px', targets = c(0)),\r\n                  list(width = '500px', targets = c(3)),\r\n                  list(width = '65px', targets = c(1,2)),\r\n                  list(className = 'dt-center', targets = c(2)))\r\n                  )\r\n            ) %>% \r\n     formatStyle('valid?',  backgroundColor = styleEqual('yes', '#90ee90'), fontWeight = 'bold') \r\n\r\n\r\n\r\n\r\nCase 4\r\n\r\nShow code\r\ntables[[4]] %>% \r\n  select(text, item, `valid?`, details) %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, #filter = 'top', \r\n            editable = TRUE, class = 'compact striped',\r\n             caption = htmltools::tags$caption(style = 'caption-side: bottom; text-align: left; font-size: 80%; color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org (see in the text).'),\r\n            extensions = 'Buttons',\r\n            options = list(searchHighlight = TRUE,\r\n                            dom = 'Bfrtip', buttons = c('csv', \"excel\"), \r\n                           columnDefs = list(\r\n                  list(width = '300px', targets = c(0)),\r\n                  list(width = '500px', targets = c(3)),\r\n                  list(width = '65px', targets = c(1,2)),\r\n                  list(className = 'dt-center', targets = c(2)))\r\n                  )\r\n            ) %>% \r\n     formatStyle('valid?',  backgroundColor = styleEqual('yes', '#90ee90'), fontWeight = 'bold') \r\n\r\n\r\n\r\n\r\nCase 5\r\n\r\nShow code\r\ntables[[5]] %>% \r\n  select(text, item, `valid?`, details) %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, #filter = 'top', \r\n            editable = TRUE, class = 'compact striped',\r\n             caption = htmltools::tags$caption(style = 'caption-side: bottom; text-align: left; font-size: 80%; color: #969696; font-family: Roboto Condensed;',\r\n               'Data: wikidata.org (see in the text).'),\r\n            extensions = 'Buttons',\r\n            options = list(searchHighlight = TRUE,\r\n                            dom = 'Bfrtip', buttons = c('csv', \"excel\"), \r\n                           columnDefs = list(\r\n                  list(width = '300px', targets = c(0)),\r\n                  list(width = '500px', targets = c(3)),\r\n                  list(width = '65px', targets = c(1,2)),\r\n                  list(className = 'dt-center', targets = c(2)))\r\n                  )\r\n            ) %>% \r\n     formatStyle('valid?',  backgroundColor = styleEqual('yes', '#90ee90'), fontWeight = 'bold') \r\n\r\n\r\n\r\n\r\nFinal Results\r\nAfter the editor have checked the results (ok, that was me) and saved CSV files with the selected matches, the files are merged into a joint table for a final demonstration. Here you are - list the pages to see which Wikidata items are found for the abstracts.\r\n\r\nShow code\r\nfinal_table <- paste0(dir, \"/csvs/\") %>% \r\n  list.files(full.names = TRUE) %>% \r\n  map_df(~read_csv(.x) %>% mutate(no = .x)) %>% \r\n  filter(`valid?`==\"yes\") %>% \r\n  mutate(details = str_extract(details, \r\n                               \"(?<=label:).+?(?=type)\")) %>% \r\n  select(-text) %>% distinct() %>% \r\n  mutate(no = str_extract(no, \"\\\\d(?=.csv)\")) %>%\r\n  mutate(url = paste0('https://www.wikidata.org/wiki/', item)) %>% \r\n  mutate(txt = paste0(#'<span style=\"background-color: #90ee90\">', \r\n                       tolower(details),\r\n                       #'<\/span>',\r\n                        ' :  (<a href=',url, ' target=\"_blank\">',\r\n                      item,'<\/a>)')) %>% \r\n  group_by(no) %>%\r\n  summarize(wikidata_items = paste(txt, collapse = \"<\/br>\")) %>% \r\n  ungroup() %>% \r\n  cbind(data) %>% \r\n  select(abstract, wikidata_items)\r\n\r\ndatatable(final_table, rownames = FALSE, escape = FALSE, \r\n            editable = TRUE, class = 'compact striped', \r\n          options = list(pageLength = 1, dom = \"tip\",\r\n                        columnDefs = list(\r\n                  list(width = '550px', targets = c(0)),\r\n                  list(width = '300px', targets = c(1)))\r\n                  ))\r\n\r\n\r\n\r\n\r\nLimitations\r\nthere could be more relevant items for the terms, but I have not found it. Sure. So far this can be viewed as an initial suggestion and a pointer. Each item in the table above directs via URL to a Wikidata page where the related items can further be found.\r\nNot all dictionaries were included in SPARQL. True. I listed those that I met first while investigating some random Wikidata items. This is a customized option - a list of catalogues for searching only the biomedical terms would require less catalogues, etc.\r\nThere are special properties which points at the scientific terms with high probability. This is TRUE, of course. I tried to play with studied by, P2579, but the problem of using it that many terms have no such property. For example, neutron, Q2348 has no P2579 property. But neutron is a subclass of atomic nucleus, Q37147 which Wikidata record has P2579 statements referring to the subject area. I skipped it.\r\nAnother option that I haven’t tried is to check if the items retrieved by initial API request are main subjects, P921 present in any scientific articles, Q13442814 or (more generally) with the items published, P1433 in the academic journals, Q5633421.\r\nI really need to watch those videos…\r\nAcknowledgments\r\nAllaire J, Iannone R, Presmanes Hill A, Xie Y (2021). distill: ‘R Markdown’ Format for Scientific and Technical Writing. R package version 1.2, <URL: https://CRAN.R-project.org/package=distill>.\r\nAllaire J, Xie Y, McPherson J, Luraschi J, Ushey K, Atkins A, Wickham H, Cheng J, Chang W, Iannone R (2021). rmarkdown: Dynamic Documents for R. R package version 2.7, <URL: https://github.com/rstudio/rmarkdown>.\r\nHenry L, Wickham H (2020). purrr: Functional Programming Tools. R package version 0.3.4, <URL: https://CRAN.R-project.org/package=purrr>.\r\nPopov M (2020). WikidataQueryServiceR: API Client Library for ‘Wikidata Query Service’. R package version 1.0.0, <URL: https://CRAN.R-project.org/package=WikidataQueryServiceR>.\r\nShafee T, Keyes O, Signorelli S, Lum A, Graul C, Popov M (2021). WikidataR: Read-Write API Client Library for ‘Wikidata’. R package version 2.2.0, <URL: https://github.com/TS404/WikidataR/issues>.\r\nWickham H (2020). tidyr: Tidy Messy Data. R package version 1.1.2, <URL: https://CRAN.R-project.org/package=tidyr>.\r\nWickham H (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=stringr>.\r\nWickham H, Francois R, Henry L, Muller K (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.3, <URL: https://CRAN.R-project.org/package=dplyr>.\r\nWickham H, Hester J (2020). readr: Read Rectangular Text Data. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=readr>.\r\nWijffels J (2021). udpipe: Tokenization, Parts of Speech Tagging, Lemmatization and Dependency Parsing with the ‘UDPipe’ ‘NLP’ Toolkit. R package version 0.8.6, <URL: https://CRAN.R-project.org/package=udpipe>.\r\nXie Y (2020). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.30, <URL: https://yihui.org/knitr/>.\r\nXie Y (2015). Dynamic Documents with R and knitr, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, <URL: https://yihui.org/knitr/>.\r\nXie Y (2014). “knitr: A Comprehensive Tool for Reproducible Research in R.” In Stodden V, Leisch F, Peng RD (eds.), Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595, <URL: http://www.crcpress.com/product/isbn/9781466561595>.\r\nXie Y, Allaire J, Grolemund G (2018). R Markdown: The Definitive Guide. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9781138359338, <URL: https://bookdown.org/yihui/rmarkdown>.\r\nXie Y, Cheng J, Tan X (2021). DT: A Wrapper of the JavaScript Library ‘DataTables’. R package version 0.17, <URL: https://CRAN.R-project.org/package=DT>.\r\nXie Y, Dervieux C, Riederer E (2020). R Markdown Cookbook. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9780367563837, <URL: https://bookdown.org/yihui/rmarkdown-cookbook>.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-15-tagging-the-abstracts-with-wikidata-items/images/editable_table.png",
    "last_modified": "2021-06-16T10:51:12+03:00",
    "input_file": {},
    "preview_width": 939,
    "preview_height": 532
  },
  {
    "path": "posts/2021-06-09-russian-journals-indexed-by-scopus-rsci-and-wos/",
    "title": "Russian Journals indexed by Scopus, RSCI, and WoS",
    "description": "How difficult it can be to build an aggregated list of the scientific journal titles indexed in A&I databases and citation indices? Extremely difficult, if those venues are the Russian academic journals. In this post I am reviewing the key obstacles and trying to build such a list of the Russian journals indexed in Web of Science Core Collection, Scopus, and RSCI (Russian Citation Index by Web of Science). This is a version updated on June 9, 2021.",
    "author": [
      {
        "name": "Aleksei Lutai",
        "url": "https://www.linkedin.com/in/lutaya/"
      }
    ],
    "date": "2021-06-09",
    "categories": [
      "russian data",
      "citation indices",
      "scopus",
      "web of science",
      "r",
      "rsci",
      "issn.org",
      "crossref",
      "lens.org"
    ],
    "contents": "\r\n\r\nContents\r\nObstacles\r\nScopus\r\nRSCI (WoS/Russian Science Citation Index)\r\nWeb of Science Core Collection\r\nAggregation\r\nFinal Table\r\nLimitations\r\nWhy Lens?\r\nAcknowledgments\r\n\r\nObstacles\r\nLegacy\r\nThe well-known problem of the academic titles is that they can change a title, publisher, and the credentials (ISSN, DOI prefix, etc). This can be a difficulty, if you have no access to the commercial services provided by ISSN Agency. But still it is a typical, not “Russian” problem.\r\nTranslated Titles\r\nA story started in 1954, when AIP got additonal funding from NSF for translation of the foreign journals. At that time USSR did not pay respect to things like copyright, so the commercial relations were quite difficult and involved the intermediaries, who launched the English versions of the Soviet titles (eventually privatized). In 1973 USSR signed the Universal Copyright Convention which spurred a M$-royalty-driven cooperation. After the Soviet Union ceased to exist, many publishers visited Moscow, and few managed to purchase the licenses for the translated versions (I have not seen the documents myself, but assume that the licenses are lifelong and irrevokable). That’s why nowadays there are dozens of journals existing in 2 language versions (Rus/Eng), registered by different publishers and in different countries, with up to 4 ISSN numbers (2 times p+e), but… also having one editorial board and practically the same set of articles.\r\nMost A&I databases index the English versions, which de jure are not Russian, but de facto originate from the Russian editions. Shall we count those indexed titles as Russian or not?\r\nCombined titles\r\nA quality of Soviet journals was not equally superb for all the titles, and some publishers decided to pack the best articles from few journals and sell it under a new cover. Such journals still exist. Assume that a combo title, traslating 50% of 2 Russian journals, is indexed in SCI - shall we count those Russian titles as “indexed”? There are some circulating “white lists” that regard such sources as “indexed” (though with special notes).\r\nCountry of origin\r\nThis is the most controversary aspect. What makes a journal Russian or say Turkish? Is it a language? Or citizenship of the editors? Or a country stated in the ISSN journal profile? A location of the publisher? Or maybe of the founders? What if there are few international founders?\r\nI hope this explains why the question “How many Russian academic titles are indexed in Scopus or Web of Science?” is doomed.\r\nScopus\r\nRussian office of Elsevier publish their own, manually-curated list of the Russian titles indexed by Scopus. I will use the version released today (!) and remove the discontinued titles. I take it as it is, leaving Elsevier responsible for possible errors.\r\n\r\nShow code\r\nscopus <- readxl::read_xlsx(paste0(dir, \"/sources/russian_titles_in_scopus_06.2021.xlsx\"), \r\n                    sheet = \"Title List\", skip = 6) %>% \r\n  select(code = 1, srctitle = 3, pissn = 4, eissn = 5, Status) %>% \r\n  filter(Status==\"Active\", grepl(\"J\", code)) %>% \r\n  pivot_longer(cols = c(\"pissn\", \"eissn\"), names_to = \"type\", values_to = \"issn\") %>% \r\n  filter(!is.na(issn)) %>% \r\n  mutate(db = \"scopus\") %>% \r\n  select(db, srctitle, type, issn)\r\n\r\nglimpse(scopus) \r\n\r\n\r\nRows: 1,119\r\nColumns: 4\r\n$ db       <chr> \"scopus\", \"scopus\", \"scopus\", \"scopus\", \"scopus\", \"~\r\n$ srctitle <chr> \"Ab Imperio\", \"Ab Imperio\", \"Acarina\", \"Acarina\", \"~\r\n$ type     <chr> \"pissn\", \"eissn\", \"pissn\", \"eissn\", \"eissn\", \"pissn~\r\n$ issn     <chr> \"2164-9731\", \"2164-9731\", \"0132-8077\", \"2221-5115\",~\r\n\r\nRSCI (WoS/Russian Science Citation Index)\r\nRussian Science Citation Index is a joined project of Clarivate and the National Electronic Library aka eLIBRARY.RU. It has an ambiguous name, as eLIBRARY.RU has its own citation index (РИНЦ/RINC), which is also translated as Russian Science Citation Index. Sometimes, they are confused, e.g. RSCI Wikipedia page relates to the RINC, but also contains the external links relating to the Clarivate. As a result this page is referenced here and there in different sense - e.g. Wikipedia page for Web of Science refers to RSCI Wiki page as to its regional database). The project is supervised by the Russian Academy of Sciences (RAS) whose role is thought to ensure a proper selection process to fill in RSCI with the best Russian titles.\r\nIn May 2021 RAS issued a new list of RSCI titles. You know, as a table in PDF file (RAS-style).\r\nI will not copy here a code I used to parse that PDF into a CSV, it is the same as I used for parsing the other PDF table in one of previous posts. So now I just read CSV (and you can download it as Excel file in the end of the post).\r\n\r\nShow code\r\nrsci <- read_csv(paste0(dir, \"/sources/2021_rsci_list_parsed.csv\")) %>% \r\n  select(srctitle = title, issn1, issn2) %>% \r\n  mutate(db = \"rsci\") %>% \r\n  pivot_longer(cols = c(\"issn1\", \"issn2\"), \r\n               names_to = \"type\", values_to = \"issn\") %>% \r\n  filter(!is.na(issn)) %>% \r\n  select(db, srctitle, type, issn) %>% \r\n  filter(!is.na(issn))\r\n\r\nglimpse(rsci)\r\n\r\n\r\nRows: 1,286\r\nColumns: 4\r\n$ db       <chr> \"rsci\", \"rsci\", \"rsci\", \"rsci\", \"rsci\", \"rsci\", \"rs~\r\n$ srctitle <chr> \"Academia. Архитектура и строительство\", \"Acarina\",~\r\n$ type     <chr> \"issn1\", \"issn1\", \"issn2\", \"issn1\", \"issn2\", \"issn1~\r\n$ issn     <chr> \"2077-9038\", \"0132-8077\", \"2221-5115\", \"2541-9420\",~\r\n\r\nWeb of Science Core Collection\r\nClarivate do not craft a “special list” of the Russian journal titles, so my approach is first to select from Master Journal List all the titles where the country of origin is RUSSIA. This leaves a lot of “meta-Russian” journals apart, so I also took from Web of Science Masterl lists the titles present in Scopus and RSCI. This adds, to the previously selected titles, translated titles and some other of mixed origin. I used the versions dated as of May 18, 2021.\r\n\r\nShow code\r\nwos_merged <- paste0(dir, \"/sources/\") %>% list.files(full.names = TRUE) %>% \r\n  .[grepl(\"wos-core\",.) & grepl(\"csv\",.)] %>% \r\n  map_df(read_csv) %>% \r\n  pivot_longer(cols = c(\"ISSN\", \"eISSN\"), \r\n               names_to = \"issn.type\", values_to = \"issn\") %>% \r\n  filter(!is.na(issn)) %>% \r\n  select(srctitle = 1, address = 3, type = issn.type, issn = issn)\r\n\r\nwos <- bind_rows(\r\n  wos_merged %>% filter(grepl(\"russia\",address, ignore.case = TRUE)),\r\n  wos_merged %>% filter(issn %in% rsci$issn),\r\n  wos_merged %>% filter(issn %in% scopus$issn)\r\n  ) %>% \r\n  mutate(db = \"wos\") %>% \r\n  select(db, srctitle, type, issn) %>% \r\n  distinct() \r\n\r\nremove(wos_merged)\r\nglimpse(wos)\r\n\r\n\r\nRows: 788\r\nColumns: 4\r\n$ db       <chr> \"wos\", \"wos\", \"wos\", \"wos\", \"wos\", \"wos\", \"wos\", \"w~\r\n$ srctitle <chr> \"NOVOE LITERATURNOE OBOZRENIE\", \"NOVYI MIR\", \"QUAES~\r\n$ type     <chr> \"ISSN\", \"ISSN\", \"ISSN\", \"eISSN\", \"ISSN\", \"eISSN\", \"~\r\n$ issn     <chr> \"0869-6365\", \"0130-7673\", \"2311-911X\", \"2313-6871\",~\r\n\r\nAggregation\r\nThe next steps involved:\r\ncleaning and few manual corrections (removing “–” or Cyrillix X from ISSNs).\r\nadding ISSN-L to each individual ISSN (I use ISSN-L/ISSN mapping files, provided by ISSN.org) and ISSN registration info for each ISSN-L (the code is similar to that in previous post about VAK titles)\r\nchecking if ISSNs registered in CrossRef and if any 2020 publications are deposited (the code is also available in the post about VAK titles)\r\ncombining the harvested information into a wide table with ISSN-L as a key\r\nFinal Table\r\nAs the joint table is the main purpose of this post, some columns are adjusted with URLs:\r\nISSN-L will bring you to portal.issn.org and opens the journal infopage\r\ntitles shows the journal titles present in the lists of providers\r\nissn shows the journal serial identificators from ISSN.org\r\nwos, rsci, scopus have “yes” value if the journal is indexed (June 2021)\r\nin_Lens directs to Lens.org and will open all the documents found for the journals ISSNs\r\ncr_2020 shows a number of 2020 documents registered in CrossRef and will open a result of corresponding API-request (you may check). The journals with zero publications either ignore registering the DOIs, or register them in other places (DataCite, Zenodo, Mendeley,…)\r\ncr_publisher will open the publisher’s information in CrossRef (more about CrossRef API)\r\nissn_country - as present in ISSN.org\r\nissn_titles - as present in ISSN.org\r\nYou can use the interactive version below or download the table in Excel or CSV format.\r\n\r\nShow code\r\ndb_pivot <- read_csv(paste0(dir, \"/russian_titles_in_WSR_pivot_wc_wp.csv\"))  %>% \r\n  select(issn_L, titles, wos, rsci, scopus,\r\n         issn, issn_country = country_issn, \r\n         issn_title = src_title_issn, \r\n         cr_member = member,\r\n         cr_publisher = publisher,\r\n         cr_location = location,\r\n         cr_2020_number = starts_with(\"cr_20\")) \r\n\r\ndb_pivot %>% arrange(desc(cr_2020_number)) %>% \r\n  mutate(issn_L = paste0('<a href=\\'', \r\n                         paste0(\"https://portal.issn.org/resource/ISSN/\", issn_L), \r\n                         '\\'\\\\s target = \\\"_blank\\\")>', issn_L, '<\/a>')) %>%\r\n  mutate(in_Lens = sapply(str_split(issn, \"\\\\|\"), \r\n                       function(x) paste0(paste0(\"source.issn:\", x), collapse = \"%20OR%20\"))) %>%\r\n  mutate(in_Lens = paste0(\"https://www.lens.org/lens/search/scholar/list?q=\", \r\n                       gsub(\"-\",\"\",in_Lens), \r\n                       \"&p=0&n=100\")) %>%\r\n  mutate(in_Lens = paste0('<a href=\\'', in_Lens, \r\n                       '\\'\\\\s target = \\\"_blank\\\")>', \"search\", '<\/a>')) %>%\r\n  mutate(cr_2020 = sapply(str_split(issn, \"\\\\|\"), \r\n                       function(x) paste0(paste0(\"issn:\", x), collapse = \",\"))) %>%\r\n  mutate(cr_2020 = paste0(\"https://http://api.crossref.org/works?filter=\", \r\n                        cr_2020, \r\n                       \",from-pub-date:2020-01,until-pub-date:2020-12\")) %>%\r\n  mutate(cr_2020 = paste0('<a href=\\'', cr_2020, \r\n                       '\\'\\\\s target = \\\"_blank\\\")>', \r\n                       ifelse(is.na(cr_2020_number),0,cr_2020_number), \r\n                       '<\/a>')) %>%\r\n  rowwise() %>% \r\n  mutate(cr_member = ifelse(is.na(cr_member), NA_character_, \r\n         paste0(\"https://api.crossref.org/members/\", cr_member))) %>% \r\n  mutate(cr_location = ifelse(is.na(cr_location), NA_character_, \r\n                              paste0(\"(location: \", cr_location,\")\"))) %>% \r\n  mutate(cr_publisher = ifelse(is.na(cr_member), NA_character_,\r\n           paste0('<a href=\\'', cr_member,  '\\'\\\\s target = \\\"_blank\\\")>', \r\n                  cr_publisher, '<\/a>'))) %>%\r\n  unite(c(\"cr_publisher\", \"cr_location\"), col = \"cr_publisher\", \r\n        sep = \"<\/br>\", na.rm=TRUE) %>% \r\n  mutate_at(c(\"wos\", \"rsci\", \"scopus\"), ~as.factor(.x)) %>% \r\n  mutate_at(c(\"issn\", \"titles\"), ~gsub(\"\\\\|\",\";<\/br>\",.x)) %>% \r\n  select(issn_L, titles, issn, wos, rsci, scopus, \r\n        in_Lens, cr_2020, cr_publisher, issn_country, issn_title) %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, filter = 'top', \r\n               caption = htmltools::tags$caption(style = 'caption-side: bottom; text-align: left; font-size: 80%; color: #969696; font-family: Roboto Condensed;',\r\n               'Data: portal.issn.org | crossref.org | elibrary.ru/project_rsci.asp | mjl.clarivate.com | elsevierscience.ru/products/scopus/ (see in the text).'),\r\n               class = 'compact striped', extensions = 'Buttons', \r\n               options = list(searchHighlight = TRUE,\r\n                 columnDefs = list(\r\n                  list(width = '350px', targets = c(1)),\r\n                  list(width = '300px', targets = c(8)),\r\n                  list(width = '250px', targets = c(10)),\r\n                  list(width = '80px', targets = c(0,2)),\r\n                  list(width = '40px', targets = c(3:5)),\r\n                  list(className = 'dt-center', targets = c(2:7))\r\n                 ),\r\n                buttons = c('csv', \"excel\"))) %>% \r\n     formatStyle('scopus',  backgroundColor = styleEqual('yes', '#ff6c00'), fontWeight = 'bold') %>% \r\n    formatStyle('wos',  backgroundColor = styleEqual('yes', '#5e33bf'), fontWeight = 'bold') %>% \r\n    formatStyle('rsci',  backgroundColor = styleEqual('yes', '#5EF085'), fontWeight = 'bold')\r\n\r\n\r\n\r\n\r\nLimitations\r\nThe table includes few journal titles that are hardly Russian, but RAS approved them to be a part of RSCI (it is their headache now).\r\nThere can be some translated titles, indexed by Web of Science with other than Russian country of origin, which could be considered as “meta-Russian” in our exercise.\r\nThe ISSN values and other details are used as it is.\r\nSome journals are present in the table with both translated and original versions. Like Physical Mesomechanics - WoS and Scopus index its English version (issn-L: 1029-9599), RSCI index its Russian version (issn-L: 1683-805X). Those two versions are not linked according to the data in ISSN.org or CrossRef.org.\r\nSome journals are shown to have no publications in CrossRef,.click on the link\r\nWhy Lens?\r\nLens.org (about) imports the data from multiple sources (to name a few: CrossRef, MEDLINE, Microsoft Academic, Core, Patent Agencies), develops incredibly fast and provides a lot of flexibility for further analysis like Reports, Patent Analysis tools, Author Profiles, etc.\r\nAnd it is OPEN!\r\nIf you never tried the Lens, you can give it a try.\r\n\r\n\r\nAcknowledgments\r\nAllaire J, Iannone R, Presmanes Hill A, Xie Y (2021). distill: ‘R Markdown’ Format for Scientific and Technical Writing. R package version 1.2, <URL: https://CRAN.R-project.org/package=distill>.\r\nAllaire J, Xie Y, McPherson J, Luraschi J, Ushey K, Atkins A, Wickham H, Cheng J, Chang W, Iannone R (2021). rmarkdown: Dynamic Documents for R. R package version 2.7, <URL: https://github.com/rstudio/rmarkdown>.\r\nHenry L, Wickham H (2020). purrr: Functional Programming Tools. R package version 0.3.4, <URL: https://CRAN.R-project.org/package=purrr>.\r\nWickham H (2020). tidyr: Tidy Messy Data. R package version 1.1.2, <URL: https://CRAN.R-project.org/package=tidyr>.\r\nWickham H (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN 978-3-319-24277-4, <URL: https://ggplot2.tidyverse.org>.\r\nWickham H (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=stringr>.\r\nWickham H, Francois R, Henry L, Muller K (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.3, <URL: https://CRAN.R-project.org/package=dplyr>.\r\nWickham H, Hester J (2020). readr: Read Rectangular Text Data. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=readr>.\r\nWickham H, Seidel D (2020). scales: Scale Functions for Visualization. R package version 1.1.1, <URL: https://CRAN.R-project.org/package=scales>.\r\nXie Y (2020). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.30, <URL: https://yihui.org/knitr/>.\r\nXie Y (2015). Dynamic Documents with R and knitr, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, <URL: https://yihui.org/knitr/>.\r\nXie Y (2014). “knitr: A Comprehensive Tool for Reproducible Research in R.” In Stodden V, Leisch F, Peng RD (eds.), Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595, <URL: http://www.crcpress.com/product/isbn/9781466561595>.\r\nXie Y, Allaire J, Grolemund G (2018). R Markdown: The Definitive Guide. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9781138359338, <URL: https://bookdown.org/yihui/rmarkdown>.\r\nXie Y, Cheng J, Tan X (2021). DT: A Wrapper of the JavaScript Library ‘DataTables’. R package version 0.17, <URL: https://CRAN.R-project.org/package=DT>.\r\nXie Y, Dervieux C, Riederer E (2020). R Markdown Cookbook. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9780367563837, <URL: https://bookdown.org/yihui/rmarkdown-cookbook>.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-09-russian-journals-indexed-by-scopus-rsci-and-wos/images/rus_jpournals_joint_list.png",
    "last_modified": "2021-06-09T17:07:27+03:00",
    "input_file": {},
    "preview_width": 884,
    "preview_height": 543
  },
  {
    "path": "posts/2021-06-07-incites-topics-4-benchmarking/",
    "title": "Using R to analyze the InCites (Clarivate) Citation Topics",
    "description": "Even the most succesful information tools can not offer all the possible analytical interfaces or visualization patterns. In this post I am using the R and few htmlwidgets to build a UI for benchmarking the universities based on their InCites citation topics (Clarivate).",
    "author": [
      {
        "name": "Aleksei Lutai",
        "url": "https://www.linkedin.com/in/lutaya/"
      }
    ],
    "date": "2021-06-07",
    "categories": [
      "russian data",
      "citation topics",
      "benchmarking",
      "incites",
      "r",
      "htmlwidgets"
    ],
    "contents": "\r\n\r\nContents\r\nInCites Built-in Functionality\r\nData\r\nBenchmarking by number of topics\r\nBenchmarking by number of unique topics\r\nHierarchy of Unique Citation Topics\r\nFurther development\r\nAnalysis of the Shared Citation Topics\r\nAcknowledgments\r\n\r\nIn December 2020 Clarivate Analytics announced a new InCites feature - the citation topics - developed in collaboration with the Centre for Science and Technology Studies (CWTS) at Leiden University (NL).\r\nThe topics are the clusters derived from the Web of Science citation network. According to the available information:\r\nthe topics are grouped to 3-level hierarchy with 10 (Macro) > 326 (Meso) > 2444 (Micro) entities\r\nan article can be assigned either to 1 topic, or to none\r\nThe hierarchy is also provided, the current version is dated as of 26 Feb 2021.\r\nIn this post I will try to use the citation topics to look into 2016-2019 publications of 2 largest Russian universities - Moscow State University and Saint-Petersburg State University.\r\n\r\nShow code\r\n## here is a theme for ggplot, it uses the external fonts \r\n## and requires {extrafont} package and the fonts to be installed\r\n\r\nmy_theme <- theme_classic()+\r\n  theme(text=element_text(family=\"PT Sans\", \r\n                          color = \"#53565A\", size = 12),\r\n        panel.grid.major = element_line(size=0.1, \r\n                          linetype = 2, color=\"grey70\"),\r\n        panel.grid.minor = element_blank(),\r\n        axis.line = element_line(size = 0.3, \r\n                          linetype = 1, color = \"grey10\"),\r\n        axis.ticks = element_line(size=0.3, \r\n                          linetype = 1, color=\"grey10\"),\r\n        axis.text = element_text(size = rel(0.9)),\r\n        axis.title = element_text(size = rel(1.1), family=\"PT Sans Narrow\"),\r\n        legend.title = element_text(size=rel(1), color=\"grey10\"),\r\n        legend.text = element_text(size=rel(0.9), color=\"grey10\"),\r\n        legend.position = \"bottom\",\r\n        legend.background = element_rect(size = 0.1, \r\n                                         color = \"grey90\", fill = \"#fff7fb\"),\r\n        legend.box.margin = margin(10,0,10,0),\r\n        legend.box.spacing = unit(0,\"cm\"),\r\n        plot.caption.position = \"plot\",\r\n        plot.caption = element_text(size = rel(0.9), hjust=1, \r\n                                    color = \"#969696\"),\r\n        plot.subtitle = element_text(hjust=0, size = rel(1), \r\n                                     family=\"PT Sans Narrow\"),\r\n        plot.title.position = \"plot\",\r\n        plot.title = element_text(size=rel(1.2), family=\"PT Sans\", \r\n                                  face=\"bold\", color = \"#253494\"),\r\n        plot.margin = margin(5, 5, 5, 5),\r\n        strip.background = element_rect(fill = \"white\", \r\n                                        colour = \"white\",  size = 0),\r\n        strip.text = element_text(size = rel(1), \r\n                                  colour = \"#52422F\", family = \"PT Sans Narrow\", \r\n                                  face = \"bold\", margin = margin(3,0,3,0), \r\n                                  hjust = 0.01, vjust = 1))\r\n\r\n\r\n\r\nInCites Built-in Functionality\r\nInCites allows to split the publications (which in our case are the university output) into the sets corresponding to the citation topics of specified level, and analyze it further using an interactive table. So it takes just few clicks to see the most cited or most prolific citation topics of the university (below).\r\n\r\nShow code\r\nknitr::include_graphics(\"images/incites_table.png\")\r\n\r\n\r\n\r\n\r\nFigure 1: Screenshot of the Analyze interface of InCites (05 June 2021). Open the image in new tab to see the details in full scale.\r\n\r\n\r\n\r\nIn addition to the table InCites offers a number of chart types (below).\r\n\r\nShow code\r\nknitr::include_graphics(\"images/incites_citation_heatmap.png\")\r\n\r\n\r\n\r\n\r\nFigure 2: Screenshot of the citation topics heatmap in InCites (05 June 2021). Open the image in new tab to see the details in full scale.\r\n\r\n\r\n\r\nWhat I am personally missing in InCites is an ability to benchmark few institutions and spot their strong & unique citation topics. Most instruments focus on finding the similarities of the items (collaborators / experts / peer reviewers), but in some cases the institution’s unique R&D features need to be highlighted.\r\nIs it due to Clarivate’s understanding that there can’t be no universal UI, or is this just a part of strategy, but InCites also provides an option to export the underlying publication data, so anyone can take the data and build its own visualization.\r\nData\r\nIn order to analyze the citation topics in 2016-2019 publications of MSU and SPbU, I did the following steps:\r\nopened the InCites > Analyze > Research Areas interface.\r\nset the Citation Topics as Schema and Micro as Level.\r\napplied the filters for Organization Name & the Publication Date,\r\nadded the Baseline for All Items (Baselines tab).\r\nThe results (see the picture below) were exported on June 5, 2021 in 2 different ways:\r\nas summary (follow the blue mark),\r\nas a dataset of publications with the citation topics (follow the red mark).\r\n\r\nShow code\r\nknitr::include_graphics(\"images/incites_table_marks.png\")\r\n\r\n\r\n\r\n\r\nFigure 3: Screenshot of the InCites Analyze interface used for data collection (05 June 2021). Open the image in new tab to see the details in full scale.\r\n\r\n\r\n\r\nBenchmarking by number of topics\r\nOut of 28 757 MSU publications, 3 559 (12%) are not assigned to any topic. For SPbU a number of unassigned articles is a bit higher - 2 465 out of 15 390 (16%).\r\nA part of those articles are not assigned to any citation topic simply because they haveno connections in the Web of Science citation graph. If an article is not cited by other WoS documents and does not refer to any WoS documents, then the clustering algorithms ignore it. But this is not an only explanation, of course.\r\nAs a number of unassigned articles is substantially higher for the latest years, one can conclude that they were uploaded into Web of Science after the current citation topics had been calculated. Many of them are likely to be assigned to some topics next year with the topics update. In this post the unassigned articles are ignored.\r\n\r\nShow code\r\nchart_filename <- paste0(getwd(), \"/images/by_n_topics.png\")\r\n\r\nif(!file.exists(chart_filename)){\r\n  tb1<- univ_summaries %>% group_by(univ) %>% \r\n    summarize(\r\n      `number_of_topics` = n_distinct(topic), \r\n      `...with 5 or more pubs` = sum(n_pubs>=5, na.rm = TRUE),\r\n      `...with 10 or more pubs` = sum(n_pubs>=10, na.rm = TRUE),\r\n      `...with 25 or more pubs` = sum(n_pubs>=25, na.rm = TRUE),\r\n      `...with % int.collab < 25% & min 5 pubs` = \r\n        sum(share_intern_collab<=25 &n_pubs>=5, na.rm = TRUE),\r\n      `...with % int.collab > 75% & min 5 pubs` =\r\n        sum(share_intern_collab>=75 &n_pubs>=5, na.rm = TRUE)\r\n      ) %>% \r\n    pivot_longer(-c(1), names_to = \"category\", values_to = \"count\") \r\n  \r\n  tb1$category <- factor(tb1$category, \r\n              levels = c(\"number_of_topics\",\r\n                         \"...with 5 or more pubs\", \r\n                         \"...with 10 or more pubs\",\r\n                        \"...with 25 or more pubs\",\r\n                      \"...with % int.collab < 25% & min 5 pubs\",\r\n                      \"...with % int.collab > 75% & min 5 pubs\"))\r\n  \r\n  tb1 %>% \r\n    mutate(label = ifelse(count/2444<0.1, \r\n                          paste0(count, \" [\",percent(count/2444, 0.1),\"]\"),\r\n                          paste0(count, \" [\",percent(count/2444, 1),\"]\"))) %>% \r\n    ggplot() + \r\n    geom_col(aes(x = count, y = univ, fill = univ), \r\n             position = \"identity\")+\r\n    geom_label(aes(x = count + 10, y = univ, label = label), \r\n              color = \"grey20\", \r\n              label.padding = unit(0.25, \"lines\"),\r\n              label.r = unit(0.1, \"lines\"), label.size = 0,\r\n              size = 3.4, hjust = 0, vjust = 0.5)+\r\n    facet_wrap(~category, ncol = 2)+\r\n    scale_x_continuous(expand = expansion(add=c(0,NA)), \r\n                       limits = c(0,2400))+\r\n    scale_fill_manual(name = \"University\", \r\n                      values = c(\"MSU\"=\"#0772A8\", \"SPbU\"=\"#D90479\"))+\r\n    labs(title = 'Number of Citation Topics in 2016-19 Publications of MSU and SPbU', \r\n         subtitle = paste0(\"Each publication is assigned to a single citation topic, \",\r\n                           \"[share %, out of total number of citation topics].\"\r\n                           ) %>%  str_wrap(120), \r\n         caption = paste0(\"Date: \", format(Sys.time(), '%d.%m.%Y'),\r\n                          \"\\nData: InCites (Clarivate)\"),  x = NULL, y = NULL) +\r\n    guides(fill = guide_legend(title = NULL, reverse = TRUE))+\r\n    my_theme+\r\n    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())+\r\n    ggsave(chart_filename,  width = 18, height = 12, dpi = 400, units = \"cm\")\r\n}\r\nknitr::include_graphics(chart_filename)\r\n\r\n\r\n\r\n\r\nThe picture above tells us that there are 1758 citation topics in 2016-2019 publications of MSU, which is 72% of all existing citation topics. For SPbU there is a lower number of topics - 1466 (60%). Counting only the larger topics (i.e. with at least N publications in 2016-2019) does not change an expected dominance of MSU - as a larger organization MSU produced the publications assigned to a wider range of topics. A number of topics with at least 25 articles for MSU (255) is twice as high as for SPbU (126).\r\nA comparable ratio is observed for the topics with at least 5 articles and low share of the international collaborators - such topics can be regarded as local competences - 503 topics in MSU publications and 263 in SPbU publications. What is surprising that a number of topics with at least 5 publications in 2016-2019 and high international collaboration share (>75%) is a bit higher for SPbU (55) than for MSU (43).\r\nBenchmarking by number of unique topics\r\nThe picture above says nothing about how the citation topics present in the publications of 2 universities overlap, and which topics are unique. For convenience I will call the topic “unique”, if it is present in a portfolio of one university (with at least 3 articles) and absent (less than 3 or none articles) in a portfolio of the other. Let’s have a look at such topics in 2016-2019 publications of MSU and SPbU.\r\n\r\nShow code\r\nunique_topics <- \r\n  univ_summaries %>% filter(n_pubs>2) %>% \r\n  add_count(topic) %>% \r\n  mutate(univ = ifelse(n==1, paste0(\"only \", univ), \"both univs\")) %>%\r\n  filter(univ!=\"both univs\") %>% \r\n  mutate(level3 = str_extract(topic, \"^\\\\d+\\\\.\\\\d+\\\\.\\\\d+\")) %>% \r\n  left_join(hier) \r\n\r\nchart_filename2 <- paste0(getwd(), \"/images/by_n_unique_topics.png\") \r\nif(!file.exists(chart_filename2)){\r\n\r\ntb2<- unique_topics %>% \r\n  group_by(univ) %>% \r\n summarize(\r\n      `number_of_topics` = n_distinct(topic), \r\n      `...with 5 or more pubs` = sum(n_pubs>=5, na.rm = TRUE),\r\n      `...with 10 or more pubs` = sum(n_pubs>=10, na.rm = TRUE),\r\n      `...with 25 or more pubs` = sum(n_pubs>=25, na.rm = TRUE),\r\n      `...with % int.collab < 25% & min 5 pubs` = \r\n        sum(share_intern_collab<=25 &n_pubs>=5, na.rm = TRUE),\r\n      `...with % int.collab > 75% & min 5 pubs` =\r\n        sum(share_intern_collab>=75 &n_pubs>=5, na.rm = TRUE)\r\n      ) %>% \r\n    pivot_longer(-c(1), names_to = \"category\", values_to = \"count\") \r\n  \r\n  tb2$category <- factor(tb2$category, \r\n              levels = c(\"number_of_topics\",\r\n                         \"...with 5 or more pubs\", \r\n                         \"...with 10 or more pubs\",\r\n                        \"...with 25 or more pubs\",\r\n                      \"...with % int.collab < 25% & min 5 pubs\",\r\n                      \"...with % int.collab > 75% & min 5 pubs\"))\r\n  \r\n  tb2 %>% \r\n    mutate(label = ifelse(count/2444<0.1, \r\n                          paste0(count, \" [\",percent(count/2444, 0.1),\"]\"),\r\n                          paste0(count, \" [\",percent(count/2444, 1),\"]\"))) %>% \r\n    ggplot() + \r\n    geom_col(aes(x = count, y = univ, fill = univ), \r\n             position = \"identity\")+\r\n    geom_label(aes(x = count + 10, y = univ, label = label), \r\n              color = \"grey20\", \r\n              label.padding = unit(0.25, \"lines\"),\r\n              label.r = unit(0.1, \"lines\"), label.size = 0,\r\n              size = 3.4, hjust = 0, vjust = 0.5)+\r\n    facet_wrap(~category, ncol = 2)+\r\n    scale_x_continuous(expand = expansion(add=c(0,NA)), \r\n                       limits = c(0,2400))+\r\n    scale_fill_manual(name = \"University\", \r\n                      values = c(\"only MSU\"=\"#0772A8\", \"only SPbU\"=\"#D90479\"))+\r\n    labs(title = 'Number of Unique Citation Topics in 2016-19 Publications of MSU and SPbU', \r\n         subtitle = paste0(\"Each publication is assigned to a single citation topic, \",\r\n                           \"[share %, out of total number of citation topics].\"\r\n                           ) %>%  str_wrap(120), \r\n         caption = paste0(\"Date: \", format(Sys.time(), '%d.%m.%Y'),\r\n                          \"\\nData: InCites (Clarivate)\"),  x = NULL, y = NULL) +\r\n    guides(fill = guide_legend(title = NULL, reverse = TRUE))+\r\n    my_theme+\r\n    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())+\r\n    ggsave(chart_filename2,  width = 18, height = 12, dpi = 400, units = \"cm\")\r\n}\r\nknitr::include_graphics(chart_filename2)\r\n\r\n\r\n\r\n\r\nBoth universities have unique topics - a number of those for MSU is 3x higher and the ratio increases while moving from smaller to larger topics (by number of publications). SPbU has only 3 unique topics with at least 25 publications in 2016-2019, while MSU has 37 such topics.\r\nAnd here somebody at C-level of the university can barge in: “I say! We are unique in something! Show me those topics”\r\nHierarchy of Unique Citation Topics\r\nFew designs are really practical in showing the hierarchical structures. When I first saw the Citation Topics, my immediate thought was about sunburstR package. So I tried to use it to visualize a structure of unique citation topics for each university.\r\nA sunburst diagramm shows both an hierarchy of units (nodes) and their sizes (shares). As it is an interactive widget, one can see not only the units and their sizes, but also a cumulative contribution of units to the topics of higher level (by hovering on the sectors). The higher levels (Macro>Meso>) are placed closer to the centre of sunburst diagram. The contribution is recalculated at global level (on hover) or at sub-level (on click). To return to the defaul view, just click the root (central) node.\r\nA suburst diagram below shows the unique citation topics for MSU with more than 10 publications in 2016-2019.\r\n\r\nShow code\r\ndtree_msu <- unique_topics %>% \r\n  filter(univ==\"only MSU\" & n_pubs > 10) %>% \r\n  select(level1_label, level2_label, level3_label, size = n_pubs) %>% \r\n  group_by(level1_label) %>% mutate(ordersize = sum(size)) %>% ungroup() %>% \r\n  arrange(-ordersize) %>% select(-ordersize)\r\n  \r\ntree_msu <- d3_nest(dtree_msu, value_cols = \"size\")\r\n\r\nsb3_msu <- sund2b(tree_msu, width=\"85%\", rootLabel = \"total\")\r\n\r\nsb3_msu\r\n\r\n\r\n\r\n\r\nOn hovering over the level 1 sectors one can find out that the almost 25% of publications asssigned to the unique topics of MSU relate to the Macro Topic named 1.Clinical & Life Sciences.\r\nOn clicking over this sector one will also find out that its largest Meso Topic is named 1.197 Molecular & Cell Biology - Mitochondria (its share is 10% to the output associated with the Macro Topic, or 2.4% to the total output associated with the unique topics).\r\nLet’s build the similar diagram for SPbU.\r\n\r\nShow code\r\ndtree_spbu <- unique_topics %>% \r\n  filter(univ==\"only SPbU\" & n_pubs > 10) %>% \r\n  select(level1_label, level2_label, level3_label, size = n_pubs) %>% \r\n  group_by(level1_label) %>% mutate(ordersize = sum(size)) %>% ungroup() %>% \r\n  arrange(-ordersize) %>% select(-ordersize)\r\n  \r\ntree_spbu <- d3_nest(dtree_spbu, value_cols = \"size\")\r\n\r\nsb3_spbu <- sund2b(tree_spbu, width=\"85%\", rootLabel = \"total\")\r\n\r\nsb3_spbu\r\n\r\n\r\n\r\n\r\nFor SPbU the largest Macro Topic is named 3 Agriculture, Environment & Ecology (24%). The largest Meso Topic under 3 Agriculture, Environment & Ecology is 3.97 Plant Pathology with 30 articles (28.3% contribution to the Marco Level, 6.8% to the total output asssociated with the unique topics).\r\nFurther development\r\nIt would be great to add more interactivity to the sunburst diagram, e.g. synchronize it to the table and filter the rows by hovering/clicking the sectors so that one could see the underlying publications. Unfortunately, my knowledge of JS scripts is too limited so I can not build such an interaction within the current RMarkdown document.\r\nBut there are still some options for client-side (server-free) interactivity. I will use the packages plotly and DT, synced via crosstalk.\r\nAnalysis of the Shared Citation Topics\r\nFor benchmark analysis I selected the topics for which both universities had at least 25 publications in 2016-2019.\r\n\r\nShow code\r\ndatafile <- paste0(dir, \"univ_result.csv\")\r\n\r\nif(!file.exists(datafile)){\r\n  msu_pubs <- paste0(dir, \"msu_publications_2016_2019.csv\") %>% \r\n    read_csv() %>% \r\n    select(wos_id = \"Accession Number\", title = \"Article Title\", \r\n           topic = \"Research Area\", doctype = \"Document Type\", \r\n           cites = \"Times Cited\", url = \"Link\", \r\n           jnci = \"Journal Normalized Citation Impact\", \r\n           cnci =\"Category Normalized Citation Impact\") %>% \r\n    filter(topic!=\"No Topic assigned\") %>% \r\n    add_count(topic, name = \"n_pubs\") %>% \r\n    mutate(univ = \"MSU\")               \r\n    \r\n  spbu_pubs <- paste0(dir, \"spbu_publications_2016_2019.csv\") %>% \r\n    read_csv() %>% \r\n    select(wos_id = \"Accession Number\", title = \"Article Title\", \r\n           topic = \"Research Area\", doctype = \"Document Type\", \r\n           cites = \"Times Cited\", url = \"Link\", \r\n           jnci = \"Journal Normalized Citation Impact\", \r\n           cnci =\"Category Normalized Citation Impact\") %>% \r\n    filter(topic!=\"No Topic assigned\") %>% \r\n    add_count(topic, name = \"n_pubs\") %>% \r\n    mutate(univ = \"SPbU\")   \r\n  \r\n  univ_pubs <- bind_rows(msu_pubs, spbu_pubs) %>% \r\n    mutate_at(c(\"jnci\", \"cnci\"), ~as.numeric(.x))\r\n  \r\n  hurdle <- 25\r\n  \r\n  univ_pubs2 <- univ_pubs %>%\r\n    # fitering the topics with 25+ publications\r\n    group_by(univ, topic) %>% \r\n    mutate(pubtest = n_distinct(wos_id)>hurdle) %>% \r\n    ungroup() %>% \r\n    filter(pubtest==TRUE) %>% select(-n_pubs, -pubtest) %>%\r\n    ## filtering the topics present in both univs \r\n    group_by(topic) %>%\r\n    mutate(n_funds = n_distinct(univ)) %>%\r\n    ungroup() %>%\r\n    filter(n_funds==2) %>% select(-n_funds) %>%\r\n    ## setting the name group for topics present for both Universities  \r\n    add_count(wos_id, name = \"n_wosid\") %>% \r\n    mutate(univ = ifelse(n_wosid==2, \"MSU & SPbU\", univ)) %>% \r\n    select(-n_wosid) %>% \r\n    distinct()\r\n  \r\n  wos_url <- univ_pubs2 %>% select(wos_id, title, url) %>% distinct()\r\n   \r\n  univ_pubs3 <- univ_pubs2 %>% \r\n     select(topic, univ, wos_id, jnci, cnci) %>% \r\n     group_by(topic, univ) %>%\r\n     arrange(-cnci, -jnci) %>%\r\n    # picking up the most impactful article\r\n     mutate(wos_id_max = head(wos_id, 1),\r\n            n_pubs = n_distinct(wos_id),\r\n            cnci_median = median(cnci), \r\n            jnci_median = median(jnci)\r\n            ) %>% \r\n     ungroup() %>% \r\n     filter(wos_id==wos_id_max) %>% \r\n     group_by(topic) %>% \r\n     mutate(total_pubs = sum(n_pubs)) %>% \r\n     ungroup() %>% \r\n     mutate(univ_share = round(n_pubs/total_pubs, 3))\r\n  \r\n  univ_pubs3 %>% \r\n    left_join(wos_url) %>% \r\n    write_excel_csv(datafile)\r\n} else {\r\n  univ_pubs3 <- read_csv(datafile)\r\n}\r\n\r\n\r\n\r\nOn the left plot the citation topics are positioned based on the shares of each university into the topic, excluding collaboration. For example, assume MSU produced 15 publications, SPbU - 10, and 5 of those were produced in their collaboration. Then the total number of publications is 20, a share of MSU is (15-5)/20 = 1/2, SPbU is (10-5)/20 = 1/4, and (MSU + SPbU) also 1/4.\r\nOn the right plot the citation topics are positioned based on a median value of CNCI (category normalized citation impact) for the associated publications (also excluding the publications produced in their collaboration).\r\n\r\nShow code\r\nuniv_pubs3a <- univ_pubs3 %>% \r\n  mutate(xurl = paste0('<a href=\\'', url,   \r\n            '\\'\\\\s target = \\\"_blank\\\")>', title, '<\/a>')) %>% \r\n  mutate(xlink = paste0(xurl, \"<br />CNCI: \",\r\n                        cnci, \", JNCI: \",jnci)) %>% \r\n  select(topic, total_pubs, univ, univ_share, \r\n         xlink, cnci_median, jnci_median) %>% \r\n  pivot_wider(1:2, names_from = \"univ\", \r\n              values_from = c(\"univ_share\", \"xlink\", \r\n                              \"cnci_median\", \"jnci_median\"))\r\n \r\nm <- highlight_key(univ_pubs3a, ~topic)\r\n#chart1\r\nch1 <- m %>%\r\n  ggplot(aes(text = paste0(topic,\r\n    \"\\nMSU Share: \", percent(`univ_share_MSU`,1) ,\r\n    \"\\nSPbU Share: \", percent(`univ_share_SPbU`,1)))) +\r\n  geom_abline(intercept = 0, slope = 1, size = 0.5,\r\n              linetype=3, color = \"grey50\")+\r\n  geom_point(aes(x = `univ_share_MSU`,\r\n                 y = `univ_share_SPbU`,\r\n                 fill = topic, size = total_pubs),\r\n             shape = 21, stroke = 0.2, alpha = 0.8)+\r\n  scale_x_continuous(labels = percent_format(1))+\r\n  scale_y_continuous(labels = percent_format(1))+\r\n  scale_size_continuous(range = c(1.5, 7))+\r\n  labs(x = \"MSU Share\",\r\n       y = \"SPbU Share\",\r\n       caption = paste0(\"Date: \",\r\n                        format(Sys.time(), '%d.%m.%Y'),\r\n                        \"\\nData: InCites (Clarivate)\"))+\r\n  coord_cartesian() + guides(fill = \"none\") +\r\n  my_theme+\r\n  theme(panel.grid.major.x = element_line(size=0.1,\r\n                             linetype = 2, color=\"grey70\"),\r\n        panel.grid.major.y = element_line(size=0.1,\r\n                             linetype = 2, color=\"grey70\"))\r\n\r\nch2 <- m %>%\r\n  ggplot(aes(text = paste0(topic,\r\n      \"\\nMedian CNCI (MSU): \",round(`cnci_median_MSU`,2),\r\n      \"\\nMedian CNCI (SPbU): \", round(`cnci_median_SPbU`,2)))) +\r\n  geom_abline(intercept = 0, slope = 1, size = 0.5,\r\n              linetype=3, color = \"grey50\")+\r\n  geom_point(aes(x = `cnci_median_MSU`,\r\n                 y = `cnci_median_SPbU`,\r\n                 fill = topic, size = total_pubs),\r\n             shape = 21, stroke = 0.2, alpha = 0.8)+\r\n  scale_x_continuous(labels = number_format(0.01))+\r\n  scale_y_continuous(labels = number_format(0.01))+\r\n  scale_size_continuous(range = c(1.5, 7))+\r\n  labs(x = \"Median CNCI of MSU Publications\",\r\n       y = \"Median CNCI of SPbU Publications\",\r\n       caption = paste0(\"Date: \",\r\n                        format(Sys.time(), '%d.%m.%Y'),\r\n                        \"\\nData: InCites (Clarivate)\"))+\r\n  coord_cartesian() + guides(fill = \"none\")+\r\n  my_theme+\r\n  theme(panel.grid.major.x = element_line(size=0.1,\r\n                      linetype = 2, color=\"grey70\"),\r\n        panel.grid.major.y = element_line(size=0.1,\r\n                      linetype = 2, color=\"grey70\"))\r\n\r\nfont <- list(size = 14, color = \"white\")\r\nlabel <- list(bgcolor = \"#232F34\", bordercolor = \"transparent\", font = font)\r\n\r\n\r\n\r\nAn interactive table under the charts shows the most impactful article for each citation topic produced by each university by separate and together (with URL link to Web of Science platform), and also CNCI and JNCI metric values.\r\nCNCI is a document-level metric comparing a number of citations received by the selected publication to the average citation rate of other similar publications (i.e. of same subject, year, publication type).\r\nJNCI compares a number of received citations to the average citation rate of the documents published in the same journal in the same year.\r\nEach citation topic is represented by a separate point. Hovering over the point does the following:\r\nshadows the other points,\r\nshows a tooltip with the citation topic name and the metrics,\r\nselects the corresponding point in the other chart,\r\nfilters the corresponding row in a table below.\r\nTo remove a selection, double click on any chart.\r\n\r\nDistill blog conflicts with Crosstalk package because of Bootstrap’s CSS (see a discussion on Github). The function unstrap() used as a workaround was borrowed from a package govdown.\r\n\r\nShow code\r\nunstrap <- function(x) {\r\n# https://rdrr.io/cran/govdown/man/unstrap.html\r\n    attr(x, \"html_dependencies\") <-\r\n    Filter(\r\n      function(dependency) {dependency$name != \"bootstrap\"},\r\n      attr(x, \"html_dependencies\")\r\n    )\r\n  x\r\n}\r\n\r\nunstrap(\r\n  bscols(\r\n    subplot(ggplotly(ch1, tooltip = \"text\", width = 800, height = 300),\r\n           ggplotly(ch2, tooltip = \"text\", width = 800, height = 300),\r\n          margin = 0.05, titleX = TRUE, titleY = TRUE) %>%\r\n    hide_legend() %>%\r\n    style(hoverlabel = label) %>%\r\n    layout(font = font, autosize = FALSE) %>%\r\n    highlight(\"plotly_hover\", opacityDim = 0.3, dynamic = FALSE)\r\n    )\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\nunstrap(\r\n  bscols(\r\n  m %>% \r\n    DT::datatable(rownames = FALSE, escape = FALSE, filter = \"top\",\r\n      class = 'compact', options = list(\r\n      pageLength = 3, dom = 'Brtip', autoWidth = TRUE, \r\n      autoWidth = TRUE, scrollX = TRUE,\r\n      columnDefs = list(\r\n         list(className = 'dt-top texttop', targets = c(0:10)),\r\n         list(visible = FALSE,targets = c(1:4, 8:13)),\r\n         list(width = '120px', targets = c(0)))),\r\n      colnames = c(\"Top 1 article (MSU only)\" = \"xlink_MSU\", \r\n                   \"Top 1 article (SPbU only)\" = \"xlink_SPbU\", \r\n                   \"Citation Topic\" = \"topic\",\r\n                   \"Top 1 article (MSU+SPbU)\" = \"xlink_MSU & SPbU\"))\r\n      )\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nAcknowledgments\r\nAllaire J, Iannone R, Presmanes Hill A, Xie Y (2021). distill: ‘R Markdown’ Format for Scientific and Technical Writing. R package version 1.2, <URL: https://CRAN.R-project.org/package=distill>.\r\nAllaire J, Xie Y, McPherson J, Luraschi J, Ushey K, Atkins A, Wickham H, Cheng J, Chang W, Iannone R (2021). rmarkdown: Dynamic Documents for R. R package version 2.7, <URL: https://github.com/rstudio/rmarkdown>.\r\nBostock M, Rodden K, Warne K, Russell K (2020). sunburstR: Sunburst ‘Htmlwidget’. R package version 2.1.5, <URL: https://CRAN.R-project.org/package=sunburstR>.\r\nBostock M, Russell K, Aisch G, Pearce A (2020). d3r: ‘d3.js’ Utilities for R. R package version 0.9.1, <URL: https://CRAN.R-project.org/package=d3r>.\r\nChang, W (2014). extrafont: Tools for using fonts. R package version 0.17, <URL: https://CRAN.R-project.org/package=extrafont>.\r\nCheng J (2020). crosstalk: Inter-Widget Interactivity for HTML Widgets. R package version 1.1.0.1, <URL: https://CRAN.R-project.org/package=crosstalk>.\r\nCheng J, Sievert C, Chang W, Xie Y, Allen J (2021). htmltools: Tools for HTML. R package version 0.5.1.1, <URL: https://CRAN.R-project.org/package=htmltools>.\r\nGarmonsway D (2021). govdown: GOV.UK Style Templates for R Markdown. R package version 0.10.1, <URL: https://CRAN.R-project.org/package=govdown>\r\nHenry L, Wickham H (2020). purrr: Functional Programming Tools. R package version 0.3.4, <URL: https://CRAN.R-project.org/package=purrr>.\r\nSievert C (2020). Interactive Web-Based Data Visualization with R, plotly, and shiny. Chapman and Hall/CRC. ISBN 9781138331457, <URL: https://plotly-r.com>.\r\nWickham H (2020). tidyr: Tidy Messy Data. R package version 1.1.2, <URL: https://CRAN.R-project.org/package=tidyr>.\r\nWickham H (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN 978-3-319-24277-4, <URL: https://ggplot2.tidyverse.org>.\r\nWickham H (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=stringr>.\r\nWickham H, Francois R, Henry L, Muller K (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.3, <URL: https://CRAN.R-project.org/package=dplyr>.\r\nWickham H, Hester J (2020). readr: Read Rectangular Text Data. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=readr>.\r\nWickham H, Seidel D (2020). scales: Scale Functions for Visualization. R package version 1.1.1, <URL: https://CRAN.R-project.org/package=scales>.\r\nXie Y (2020). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.30, <URL: https://yihui.org/knitr/>.\r\nXie Y (2015). Dynamic Documents with R and knitr, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, <URL: https://yihui.org/knitr/>.\r\nXie Y (2014). “knitr: A Comprehensive Tool for Reproducible Research in R.” In Stodden V, Leisch F, Peng RD (eds.), Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595, <URL: http://www.crcpress.com/product/isbn/9781466561595>.\r\nXie Y, Allaire J, Grolemund G (2018). R Markdown: The Definitive Guide. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9781138359338, <URL: https://bookdown.org/yihui/rmarkdown>.\r\nXie Y, Cheng J, Tan X (2021). DT: A Wrapper of the JavaScript Library ‘DataTables’. R package version 0.17, <URL: https://CRAN.R-project.org/package=DT>.\r\nXie Y, Dervieux C, Riederer E (2020). R Markdown Cookbook. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9780367563837, <URL: https://bookdown.org/yihui/rmarkdown-cookbook>.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-07-incites-topics-4-benchmarking/images/sunburst_screenshot.png",
    "last_modified": "2021-06-07T11:43:09+03:00",
    "input_file": {},
    "preview_width": 853,
    "preview_height": 381
  },
  {
    "path": "posts/2021-05-27-academic-journals-through-the-lens-of-wikidata/",
    "title": "National Academic Journals in Wikidata and Wikipedia",
    "description": "In this post I am using SPARQL for searching the Russian academic journals in Wikidata and Wikipedia services, and trying to assess their standing.",
    "author": [
      {
        "name": "Aleksei Lutai",
        "url": "https://www.linkedin.com/in/lutaya/"
      }
    ],
    "date": "2021-05-26",
    "categories": [
      "russian data",
      "journal identifier",
      "r",
      "wikidata",
      "wikipedia",
      "sparql"
    ],
    "contents": "\r\n\r\nContents\r\nSPARQL or how to find anything\r\nSearching Russian Academic Journals\r\nFound Journals\r\nOr Lost Journals\r\nHow complete is Wikidata?\r\nAny chance for Wikidata-based bibliometrics?\r\nCases for using Wikidata\r\nFinal Remarks\r\nUseful links\r\nLimitations\r\nAcknowledgments\r\n\r\nIn one of the recent posts I queried portal.issn.org to harvest the journal metadata and found out that the exported pdfs contained the references to Wikidata and Wikipedia pages. What surprized me later is that just 60% of titles have Wikidata profiles, and as little as 2% have Wikipedia articles.\r\nLike many people who have never created a page in Wikipedia, I was living peacefully with 2 myths in my mind.\r\nMyth 1: Any academic journal can create a page in Wikipedia and inform the research community about its goals & successes. Well, I’ve changed my mind after reading the criteria listed at WikiProject_Academic_Journals. Especially, those reflecting a notability:\r\nCriterion 1: The journal is considered by reliable sources to be influential in its subject area.\r\nCriterion 2: The journal is frequently cited by other reliable sources.\r\nCriterion 3: The journal is historically important in its subject area.\r\nThis should explain why a number of Russian journals indexed in Scopus is almost 5 times higher than in Wikipedia.\r\nMyth 2: Wikidata records are composed from the bits of information present on corresponding Wikipedia page. It is not true - as we have already seen for VAK titles, too many scientific journals (and also the organizations, researchers, publications, etc) have Wikidata IDs without having a dedicated page in Wikipedia.\r\nWikidata is indeed about the items and their relations.\r\n\r\nBy <a href=“//commons.wikimedia.org/wiki/User:HenkvD” title=“User:HenkvD”>HenkvD<\/a> - <span class=“int-own-work” lang=“en”>Own work<\/span>, CC BY-SA 4.0, Link\r\n\r\nSo if Wikipedia page includes the properly quoted Wikidata items (e.g. statements referring to the scientific articles, researchers, academic awards, etc.), the related Wikidata records will inherit those relations from Wikipedia article (I guess so).\r\nSPARQL or how to find anything\r\nFor anyone who has never met a SPARQL query (like me few weeks ago) reading the collections of demo SPARQL queries (like below) can be a fascinating experience.\r\nhttps://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial\r\nhttps://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries\r\nhttps://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples\r\nhttps://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples/advanced\r\nhttps://github.com/sebotic/SPARQL\r\nhttps://shkspr.mobi/blog/2019/11/the-greater-bear-using-wikidata-to-generate-better-artwork/\r\nA blend of amusement and cognitive efforts, which me personally made recalling “I like pleasure spiked with pain…”.\r\nFor those whose interests are mostly about scholarly communication and academic publishing, there is a fantastic cookbook with ready-to-go recipies. It is named Scholia.\r\nPick up any of the pre-set scenario (e.g. profile of Carol Greider), give a web page some time to load, and you’ll see how powerful Wikidata is thought to be. Further, under any chart or table you can click a link “Edit on query.Wikidata.org” and try/adapt a SPARQL query. I hope the creators of this service feel our vibes of love every day.\r\nSearching Russian Academic Journals\r\nIn most databases to check if the journal is indexed, we would have to search it using a title or ISSNs. This is how it works in Web of Science, Scopus, the Lens, and many others.\r\nWith Wikidata a query must include not the trivial names (like “The Lancet”), but the identifiers of Wikidata items and properties. You can find many useful identifiers in the examples provided by Scholia, or in the list of bibliographic properties, or using Wikidata search.\r\nMy initial query sounded like this - to find the items that are defined (wdt:P31/wdt:P279*) as Scientific or Academic journal (wd:Q5633421 wd:Q737498) originated from (P495) a country named (17) Russian Federation (Q159).\r\nBut soon I found out that some academic journals that publish in Russian, but have no a property “country” that would connect it to Russia. Thus we have to add another condition - find the items that are defined (wdt:P31/wdt:P279*) as Scientific or Academic journal (wd:Q5633421 wd:Q737498) whose language of work (P407) is Russian (Q7737).\r\nIn SPARQL the combined query looks like this:\r\nSELECT DISTINCT ?journal ?journalLabel WHERE {\r\n  {VALUES ?type {wd:Q5633421 wd:Q737498}\r\n          ?journal wdt:P31/wdt:P279* ?type.\r\n          ?journal wdt:P407 wd:Q7737.\r\n          MINUS { ?journal wdt:P495 [] }.\r\n  }\r\n  UNION\r\n  {VALUES ?type {wd:Q5633421 wd:Q737498}\r\n          ?country wdt:P17 wd:Q159.\r\n          ?journal wdt:P31/wdt:P279* ?type.\r\n          ?journal wdt:P495 ?country.\r\n  }\r\n  SERVICE wikibase:label {bd:serviceParam wikibase:language \"[AUTO_LANGUAGE], en, ru\".}\r\n  }\r\nYou can see the results of this SPARQL query at Wikidata Query Service, further named as WDQS.\r\n\r\nShow code\r\nknitr::include_graphics(paste0(img_dir, \"wiki_query.png\"))\r\n\r\n\r\n\r\n\r\nThe results can be saved from WDQS using the button Download (JSON, CSV). To perform a SPARQL query in R and retrieve the results I use WikidataQueryServiceR package, which I found very convenient. A function named query_wikidata accepts a string with a SPARQL query and return a dataframe.\r\n\r\nShow code\r\nres_journals <- WikidataQueryServiceR::query_wikidata(\r\n  as.character(\r\n    'SELECT DISTINCT ?journal ?journalLabel \r\n      WHERE {\r\n        {VALUES ?type {wd:Q5633421 wd:Q737498}\r\n                ?journal wdt:P31/wdt:P279* ?type.\r\n                ?journal wdt:P407 wd:Q7737.\r\n                MINUS { ?journal wdt:P495 [] }.\r\n        }\r\n        UNION\r\n        {VALUES ?type {wd:Q5633421 wd:Q737498}\r\n                ?country wdt:P17 wd:Q159.\r\n                ?journal wdt:P31/wdt:P279* ?type.\r\n                ?journal wdt:P495 ?country.\r\n        }\r\n        SERVICE wikibase:label {\r\n        bd:serviceParam wikibase:language \"[AUTO_LANGUAGE], en, ru\".\r\n        }\r\n      }\r\n    LIMIT 5')\r\n    )\r\n\r\n\r\n\r\nThe <\/>Code button at WDQS also helps to wrap the SPARQL syntax for the different languages. But for R a suggested library is SPARQL, which I did not try (it seems like it was not updated for quite a long time).\r\nIn order to retrieve more fields from Wikidata records the query must be upgraded (I am not sure if the one below is optimal, but it works).\r\nSELECT DISTINCT ?journal ?journal_title ?issnL ?cref_jid \r\n                ?lang ?article ?url ?url_srcLabel ?indexedLabel ?scopus\r\nWITH {\r\n  SELECT DISTINCT ?journal \r\n  WHERE {\r\n    {VALUES ?type {wd:Q5633421 wd:Q737498}\r\n             ?journal wdt:P31/wdt:P279* ?type.\r\n             ?journal wdt:P407 wd:Q7737.\r\n     MINUS { ?journal wdt:P495 [] }.}\r\n    UNION\r\n    {VALUES ?type {wd:Q5633421 wd:Q737498}\r\n             ?country wdt:P17 wd:Q159.\r\n             ?journal wdt:P31/wdt:P279* ?type.\r\n             ?journal wdt:P495 ?country.}\r\n  }\r\n } AS %result\r\nWHERE {\r\n  INCLUDE %result\r\n          optional {?journal p:P856 ?urlx.\r\n                       ?urlx ps:P856 ?url.\r\n                       ?urlx prov:wasDerivedFrom ?refnode2.\r\n                       ?refnode2 pr:P143 ?url_src.}\r\n            optional {?journal wdt:P1476 ?journal_title.}           \r\n            optional{?article schema:about ?journal ;               \r\n                              schema:inLanguage ?lang ;               \r\n                              schema:isPartOf [ wikibase:wikiGroup \"wikipedia\" ] .\r\n                    }\r\n            optional {?journal wdt:P7363 ?issnL.}\r\n            optional {?journal wdt:P8375 ?cref_jid.}\r\n            optional {?journal wdt:P8875 ?indexed.} \r\n            optional {?journal wdt:P1156 ?scopus.}\r\n      SERVICE wikibase:label {\r\n          bd:serviceParam wikibase:language \"[AUTO_LANGUAGE], en, ru\".\r\n          ?url_src rdfs:label ?url_srcLabel .\r\n          ?indexed rdfs:label ?indexedLabel.\r\n          }\r\n  }\r\nThe code snippet below shortens the enquoted SPARQL query, so most likely you will not be able to see it in the code, but you can copy it from above and make a query at WDQS.\r\nFound Journals\r\n\r\nShow code\r\nwiki_journals_raw_file <-  paste0(onedrive,\r\n        \"/Wikidata/Wiki_Journals/blog_russian_wikidata_journals_raw.csv\")\r\n\r\nif(!file.exists(wiki_journals_raw_file)){\r\n\r\nrus_wiki_journals <- query_wikidata(as.character(\r\n    [1496 chars quoted with '''])\r\n    )\r\n  \r\n  rus_wiki_journals %>% distinct() %>% write_excel_csv(wiki_journals_raw_file)\r\n  } else {\r\n  rus_wiki_journals <- read_csv(wiki_journals_raw_file, \r\n                  col_types = cols(.default = col_character(),\r\n                                  web_site = col_integer(),\r\n                                  n_languages = col_integer()\r\n                                  )\r\n                  )\r\n}\r\n\r\nrus_wiki_journals %>% arrange(journal_title) %>% head(20) %>% \r\n  ## extracting Wikidata URLs into IDs\r\n  mutate(journal = str_extract(journal, \"Q\\\\d+$\")) %>%  \r\n  ## shortening the wikipedia Article URLs as they can be too long for tables \r\n  mutate(article = ifelse(is.na(article), NA, \r\n                          paste0(str_sub(article, 1,30),\"...\"))) %>% \r\n  datatable(rownames = FALSE, filter = \"none\", \r\n            escape = FALSE, class = \"row-border\", \r\n            options = list(columnDefs = list(\r\n              list(width = '70px', targets = c(2)),\r\n              list(width = '200px', targets = c(1,5,8)))\r\n              )\r\n            )\r\n\r\n\r\n\r\n\r\nAs some fields (ISSN, Wikipedia page, language, etc) contain multiple values, the resulting dataframe has degenerate rows. I used the Wikidata items and ISSN-L as grouping variables to glue up the string values in the other columns, and finally get a table where each row corresponds to unique journal.\r\n\r\nShow code\r\nwiki_journals_grouped_file <-  paste0(onedrive,\r\n        \"/Wikidata/Wiki_Journals/blog_russian_wikidata_journals_grouped.csv\")\r\n\r\nif(!file.exists(wiki_journals_grouped_file)){\r\n\r\n  rus_wiki_journals_grouped <- rus_wiki_journals %>%  \r\n    mutate(wiki = str_extract(journal, \"Q\\\\d+$\")) %>%\r\n    select(wiki, journal_title, issnL, cref_jid, scopus, lang, \r\n           wiki_article = article, web_site = url, indexed_in = indexedLabel) %>% \r\n    group_by(wiki, issnL) %>% \r\n    summarize(\r\n      journal_title = paste0(unique(na.omit(journal_title)), collapse = \"|\"),\r\n      crossref = paste0(unique(na.omit(cref_jid)), collapse = \"|\"),\r\n      scopus = paste0(unique(na.omit(scopus)), collapse = \"|\"),\r\n      web_site = n_distinct(web_site, na.rm = TRUE),\r\n      languages = paste0(unique(na.omit(lang)), collapse = \"|\"),\r\n      n_languages = n_distinct(lang, na.rm = TRUE),\r\n      indexed_in = paste0(unique(na.omit(indexed_in)), collapse = \"|\")\r\n    ) %>% ungroup() %>% \r\n    mutate_all(~ifelse(.x==\"\",NA,.x))\r\n  \r\n  rus_wiki_journals_grouped %>% write_excel_csv(wiki_journals_grouped_file)\r\n  } else {\r\n  rus_wiki_journals_grouped  <- read_csv(wiki_journals_grouped_file,\r\n                  col_types = cols(.default = col_character(),\r\n                                  web_site = col_integer(),\r\n                                  n_languages = col_integer()))\r\n}\r\n\r\nrus_wiki_journals_grouped %>% arrange(journal_title) %>%  head(20) %>% \r\n    datatable(rownames = FALSE, filter = \"none\", \r\n              escape = FALSE, class = \"row-border\", \r\n            options = list(columnDefs = list(\r\n              list(width = '350px', targets = c(2)))))\r\n\r\n\r\n\r\n\r\nOr Lost Journals\r\nWikidata returned a list of 2828 Russian journals, in which:\r\n97.3% have ISSN-L,\r\n90.0% have CrossRef Journal ID,\r\n3.0% have official web site URL,\r\n4.9% have Wikipedia articles in at least one language, 2.2% have Wikipedia articles in more than 1 language.\r\n20.9% according to Wikidata are indexed in in bibliographic review(s) and/or database(s).\r\nLet’s check if being indexed is associated with a higher chances of having a Wikipedia article.\r\n\r\nShow code\r\nrus_wiki_journals_grouped %>% \r\n  mutate(languages = !is.na(languages), indexed_in = !is.na(indexed_in)) %>%\r\n  count(indexed_in, languages) %>% \r\n  mutate(share = percent(n/nrow(rus_wiki_journals_grouped),0.1)) %>%\r\n   datatable(rownames = FALSE, filter = \"none\", \r\n             escape = FALSE, class = \"row-border\", \r\n             options = list(autoWidth = FALSE, \r\n                         columnDefs = list(\r\n                          list(className = 'dt-center', targets = c(0:3)))))\r\n\r\n\r\n\r\n\r\nThe indexed titles more often have Wikipedia articles, but more than a half of those having the article(s) in Wikipedia are not indexed (according to Wikidata) in any prominent bibliographic databases.\r\nHow complete is Wikidata?\r\nOne may suggest that if Wikidata import (in some way or other) the data from CrossRef, then why to bother about it, as CrossRef provides fantastic API-services and gets the metadata directly from the publishers.\r\nLet’s first check whether Wikidata contains complete data deposited in CrossRef.\r\nI select as an example a German journal named Atmospheric Chemistry and Physics, published since 2001. Its Scholia profile looks quite impressive.\r\nAnd yet…\r\nWikidata contains approx 5.5k articles - check\r\nCrossRef contains more than 11.7k publications - check\r\nFor 2019 publications the difference is still substantial:\r\nWikidata contains 55 articles - check\r\nCrossRef contains 820+ publications - check\r\nAnd I also counted a number of authors with ORCID in 2019 articles:\r\nWikidata returs a list of 163 researchers - check\r\nCrossRef does not return the researchers, but we can obtain a list of publications where at least one of the authors has ORCID. There were 749 such articles - check. I looked into the search results and identified 1808 researchers with unique ORCIDs.\r\nAny chance for Wikidata-based bibliometrics?\r\nAs Wikidata does not contain all publications from CrossRef, it should not be viewed as an alternative to CrossRef or other databases indexing the academic journals cover-to-cover.\r\nLet’s try to extract all publications in the previously found 2828 Russian journals, satisfying next 3 condistions:\r\npublished in 2016-2020\r\n\r\npresent in Wikidata (i.e. owning the Wikidata ID)\r\n\r\nauthored by the researchers who also present in Wikidata.\r\n\r\nFor speed we will retrieve just journal IDs, publication IDs, and author IDs.\r\nSELECT ?journal ?work ?author ?country\r\nWITH {\r\n  SELECT DISTINCT ?journal \r\n  WHERE {\r\n    {VALUES ?type {wd:Q5633421 wd:Q737498}\r\n             ?journal wdt:P31/wdt:P279* ?type.\r\n             ?journal wdt:P407 wd:Q7737.\r\n     MINUS { ?journal wdt:P495 [] }.}\r\n    UNION\r\n    {VALUES ?type {wd:Q5633421 wd:Q737498}\r\n             ?country wdt:P17 wd:Q159.\r\n             ?journal wdt:P31/wdt:P279* ?type.\r\n             ?journal wdt:P495 ?country.}\r\n  }\r\n } AS %result\r\nWHERE {\r\n  INCLUDE %result\r\n              ?work wdt:P1433 ?journal.\r\n              ?work wdt:P577 ?pubdate.\r\n              ?work wdt:P50 ?author.\r\n              ?author wdt:P27 wd:Q159.              \r\n  FILTER((?pubdate >= \"2016-01-01T00:00:00Z\"^^xsd:dateTime) && \r\n          (?pubdate <= \"2020-12-31T00:00:00Z\"^^xsd:dateTime))\r\n  }\r\n\r\nThis SPARQL query is very strict, as it filters the items at every condition. Clearly some journals may have the articles that are present in Wikidata, but have no “publication date” property or its authors have no Wikidata ID. Such articles will not be retrieved by this query.\r\n\r\nShow code\r\nwiki_journals_2016_2020_file <-  paste0(onedrive,\r\n        \"/Wikidata/Wiki_Journals/blog_russian_wikidata_journals_2016_2020.csv\")\r\n\r\nif(!file.exists(wiki_journals_2016_2020_file)){\r\n\r\nwiki_journals_2016_2020 <- query_wikidata(as.character(\r\n  'SELECT ?journal ?work ?author ?country\r\nWITH {\r\n  SELECT DISTINCT ?journal \r\n  WHERE {\r\n    {VALUES ?type {wd:Q5633421 wd:Q737498}\r\n             ?journal wdt:P31/wdt:P279* ?type.\r\n             ?journal wdt:P407 wd:Q7737.\r\n     MINUS { ?journal wdt:P495 [] }.}\r\n    UNION\r\n    {VALUES ?type {wd:Q5633421 wd:Q737498}\r\n             ?country wdt:P17 wd:Q159.\r\n             ?journal wdt:P31/wdt:P279* ?type.\r\n             ?journal wdt:P495 ?country.}\r\n  }\r\n } AS %result\r\nWHERE {\r\n  INCLUDE %result\r\n              ?work wdt:P1433 ?journal.\r\n              ?work wdt:P577 ?pubdate.\r\n              ?work wdt:P50 ?author.\r\n              ?author wdt:P27 wd:Q159.              \r\n  FILTER((?pubdate >= \"2016-01-01T00:00:00Z\"^^xsd:dateTime) && \r\n        (?pubdate <= \"2020-12-31T00:00:00Z\"^^xsd:dateTime))\r\n  }\r\n')\r\n  )\r\n  \r\n  wiki_journals_2016_2020 %>% distinct() %>% write_excel_csv(wiki_journals_2016_2020_file)\r\n  } else {\r\n  wiki_journals_2016_2020 <- read_csv(wiki_journals_2016_2020_file, \r\n                  col_types = cols(.default = col_character()))\r\n}\r\n\r\n\r\n\r\nWhat have we got?\r\n70 unique journals (which is 2.5%)\r\n312 unique publications\r\n115 unique authors\r\nTempted to see a pretty picture?\r\n\r\nShow code\r\nchart_filename <- paste0(img_dir, \"5_year_output.png\")\r\n\r\nif(!file.exists(chart_filename)){\r\n  \r\n  library(igraph)\r\n  library(ggraph)\r\n  library(tidygraph)\r\n  library(extrafont)\r\n  \r\n  sizes <- bind_rows(\r\n    wiki_journals_2016_2020 %>% \r\n      group_by(journal) %>% \r\n      summarize(n_pubs = n_distinct(work)) %>% \r\n      ungroup() %>% rename(name = 1),\r\n    wiki_journals_2016_2020 %>% \r\n      group_by(author) %>% \r\n      summarize(n_pubs = n_distinct(work)) %>% \r\n      ungroup() %>% rename(name = 1)\r\n  )\r\n  \r\n  g <- wiki_journals_2016_2020 %>% \r\n    select(from = author, to = journal) %>% \r\n    graph_from_data_frame(directed = FALSE) %>% \r\n    as_tbl_graph() %>% \r\n    left_join(sizes) %>% \r\n    mutate(type = ifelse(name %in% wiki_journals_2016_2020$journal, \r\n                         \"journal\", \"author\"))\r\n  \r\nggraph(g, layout = \"nicely\") + \r\n  geom_edge_link0(color = \"grey60\", edge_width = 0.3)+\r\n  geom_node_point(aes(x = x, y = y, fill = type, size = n_pubs),\r\n                  shape = 21, stroke = 0.3, color = \"grey20\")+\r\n  scale_fill_manual(values = c(\"journal\" = \"#53565A\", \"author\" = \"coral\"), \r\n                    name = NULL)+\r\n  labs(title = \r\n  \"5-Year Output of 2828 Russian Academic Journals Through the Lens of Wikidata\",\r\n      subtitle = paste0(\"(\",n_distinct(wiki_journals_2016_2020$work),\r\n                          \" publications and \", \r\n                          n_distinct(wiki_journals_2016_2020$author), \r\n                          \" Russian authors visible in \",\r\n                          n_distinct(wiki_journals_2016_2020$journal), \r\n                          \" journals)\"),\r\n        caption = paste0(\"Date: \", format(Sys.time(), '%d/%m/%Y'),\r\n                         \"\\nData: wikidata.org\"))+\r\n  scale_size_continuous(range = c(1.5,6), breaks = pretty_breaks(6),\r\n                        name = \"Number of\\npublications\")+\r\n  guides(fill = guide_legend(override.aes = list(size = 5)))+\r\n  theme_graph() +\r\n  theme(text=element_text(family=\"PT Sans\", color = \"#53565A\", size = 12),\r\n        legend.title = element_text(size=rel(1), color=\"grey10\"),\r\n        legend.text = element_text(size=rel(0.9), color=\"grey10\"),\r\n        legend.position = \"right\",\r\n        legend.box.background = element_rect(size = 0.2, \r\n                color = \"grey90\", fill = \"white\"),\r\n        legend.box.margin = margin(10,0,10,0),\r\n        legend.box.spacing = unit(0,\"cm\"),\r\n        plot.caption.position = \"plot\",\r\n        plot.caption = element_text(size = rel(0.9), hjust=1, \r\n                          family = \"PT Sans\", color = \"#969696\", face = \"plain\"),\r\n        plot.subtitle = element_text(hjust=0, size = rel(1), \r\n                          family=\"PT Sans Narrow\"),\r\n        plot.title.position = \"plot\",\r\n        plot.title = element_text(size=rel(1.2), family=\"PT Sans\", \r\n                                  face=\"bold\", color = \"#253494\"),\r\n        plot.margin = margin(5, 5, 5, 5)) + \r\n  ggsave(chart_filename, width = 18, height = 12, dpi = 400, units = \"cm\")\r\n}\r\nknitr::include_graphics(chart_filename)\r\n\r\n\r\n\r\n\r\nCases for using Wikidata\r\nThere are still some cases that can be much easier solved with Wikidata. As a knowledge graph connecting the entities of different nature, Wikidata allows to build complex queries combining the items properties or statements that may not exist together in any other single database.\r\nJust imagine a query that retrieves the journal authors having the international awards (see examples in Scholia).\r\nWell, one may argue that the qualified editor remember by heart all the star authors. Hope so.\r\nLet’s take a more complex scenario. Assume that the publisher decided to dedicate a special issue to women in Molecular biology and the editor now has to find the female researchers who won the international awards and ask them kindly to write a short article. The editor may think that those who cited the journal articles would be more impressed and likely to agree.\r\nCan Wikidata help to this? Yes, it can!\r\nA SPARQL query below will retrieve a list of women, researchers, who won the international awards and cited the Russian journal Molecular Biology.\r\nSELECT \r\n  ?authorLabel ?orcid ?awards\r\nWITH {\r\n  SELECT ?authorLabel ?author (GROUP_CONCAT(DISTINCT(?award); separator=\" | \") AS ?awards)\r\n  WHERE {\r\n  ?work wdt:P1433 wd:Q4300349.\r\n  ?reference_pub wdt:P2860 ?work.\r\n  ?reference_pub wdt:P50 ?author .\r\n  ?author p:P166 ?award_statement.\r\n  ?award_statement ps:P166 ?award_.\r\n  ?author wdt:P21 wd:Q6581072 .\r\n    SERVICE wikibase:label {\r\n      bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" . \r\n      ?award_ rdfs:label ?award.\r\n      ?author rdfs:label ?authorLabel\r\n    } \r\n  }\r\nGROUP BY ?author ?authorLabel \r\n} AS %result\r\nWHERE {\r\n  INCLUDE %result \r\n   optional{?author wdt:P496 ?orcid .}\r\n    SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" . }\r\n}\r\n\r\nShow code\r\nwinners <- query_wikidata(as.character(\r\n'SELECT \r\n      ?author ?orcid ?awards\r\n WITH {\r\n    SELECT ?authorLabel ?author \r\n            (GROUP_CONCAT(DISTINCT(?award); separator=\" | \") AS ?awards)\r\n    WHERE {\r\n      ?work wdt:P1433 wd:Q4300349.\r\n      ?reference_pub wdt:P2860 ?work.\r\n      ?reference_pub wdt:P50 ?author .\r\n      ?author p:P166 ?award_statement.\r\n      ?award_statement ps:P166 ?award_.\r\n      ?author wdt:P21 wd:Q6581072 .\r\n      SERVICE wikibase:label {\r\n          bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" . \r\n          ?award_ rdfs:label ?award.\r\n          ?author rdfs:label ?authorLabel\r\n      } \r\n    }\r\n    GROUP BY ?author ?authorLabel \r\n    } AS %result\r\n  WHERE { INCLUDE %result \r\n      optional{?author wdt:P496 ?orcid .}\r\n      SERVICE wikibase:label {\r\n      bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" . \r\n      }\r\n    }')\r\n    )\r\n\r\nwinners %>% mutate(author = str_extract(author, \"Q\\\\d+$\")) %>%\r\n  datatable(rownames = FALSE, filter = \"none\", \r\n            escape = FALSE, class = \"row-border\", \r\n            options = list(\r\n              columnDefs = list(\r\n                list(width = '130px', targets = c(1)),\r\n                list(width = '800px', targets = c(2)))))\r\n\r\n\r\n\r\n\r\nCoincidentally, today I saw a tweet of Scholia creator who also shared a SPARQL query crafted to find a female speaker on a certain topic. This is another possible application, even closer to reality than mine.\r\n\r\n\r\nlooking for a female speaker? Try @wikidata: https://t.co/vW21zqQGGO Tune the subjects in line 3 to fit your needs. Here, I used metabolomics, data analysis, chemometrics, lipidomicsThe topics are ‘main subject’ of publications on which the scholar is author pic.twitter.com/FIwBjxPDSI\r\n\r\n— Egon Willighⓐgen (@egonwillighagen) May 24, 2021\r\n\r\nFinal Remarks\r\nWikidata is a fantastic open tool that can be very useful for the academic journals. Its only limit is an amount of information that is growing and can be edited.\r\nRussian academic journals have a very poor representation both in Wikipedia (4.9% of 2828 academic/scientific journals) and in Wikidata (few connections to the publications and the researchers).\r\nRussian publishers & editors may need more consulting support to meet the technical requirements & notability criteria erected by WikiProject_Academic_Journals. With dozens of workshops every year on “how to get to Scopus”, I can not recall any good events devoted to Wikipedia or Wikidata for academic journals. The program of the 9th International Scientific and Practical Conference “World-Class Scientific Publication – 2021: Global Trends and National Priorities”, held in Moscow from 24 to 27 May, neglects this topic as well.\r\nUseful links\r\nHere are some useful sources in addition to those mentioned above:\r\nWikipedia and Wikidata courses\r\nWikidata Tours explain how Wikidata works\r\nSearch Item in Wikidata\r\nCreate Wikidata Item\r\nTutorial on how to annotate the scientific publications\r\nWikidata Tools - a directory of tools created for editing Wikidata items.\r\nA very detailed scientific article Scholia, Scientometrics and Wikidata from Wikidata’s creators\r\nWikidata Weekly Status Updates - very useful newsletters with a lot of links, tutorials, examples.\r\nScholia Text to Topics a text processing web application. Accepts the text, returns the topics with Wikidata IDs.\r\nLimitations\r\nThe findings above should be taken with a pinch of salt, as I started to dig into Wikidata just few weeks ago. Some of my statements can be not perfectly correct or even wrong. The main purpose of this post was to share my finding and show what is possible to do with Wikidata.\r\nAcknowledgments\r\nAllaire J, Xie Y, McPherson J, Luraschi J, Ushey K, Atkins A, Wickham H, Cheng J, Chang W, Iannone R (2021). rmarkdown: Dynamic Documents for R. R package version 2.7, <URL: https://github.com/rstudio/rmarkdown>.\r\nChang, W (2014). extrafont: Tools for using fonts. R package version 0.17, <URL: https://CRAN.R-project.org/package=extrafont>.\r\nCsardi G, Nepusz T (2006). “The igraph software package for complex network research.” InterJournal, Complex Systems, 1695. <URL: https://igraph.org>.\r\nHenry L, Wickham H (2020). purrr: Functional Programming Tools. R package version 0.3.4, <URL: https://CRAN.R-project.org/package=purrr>.\r\nPedersen T (2021). ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. R package version 2.0.5, <URL: https://CRAN.R-project.org/package=ggraph>.\r\nPedersen T (2020). tidygraph: A Tidy API for Graph Manipulation. R package version 1.2.0, <URL: https://CRAN.R-project.org/package=tidygraph>.\r\nPopov M (2020). WikidataQueryServiceR: API Client Library for ‘Wikidata Query Service’. R package version 1.0.0, <URL: https://CRAN.R-project.org/package=WikidataQueryServiceR>.\r\nWickham H (2020). tidyr: Tidy Messy Data. R package version 1.1.2, <URL: https://CRAN.R-project.org/package=tidyr>.\r\nWickham H (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN 978-3-319-24277-4, <URL: https://ggplot2.tidyverse.org>.\r\nWickham H (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=stringr>.\r\nWickham H, Francois R, Henry L, Muller K (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.3, <URL: https://CRAN.R-project.org/package=dplyr>.\r\nWickham H, Hester J (2020). readr: Read Rectangular Text Data. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=readr>.\r\nWickham H, Seidel D (2020). scales: Scale Functions for Visualization. R package version 1.1.1, <URL: https://CRAN.R-project.org/package=scales>.\r\nXie Y (2020). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.30, <URL: https://yihui.org/knitr/>.\r\nXie Y (2015). Dynamic Documents with R and knitr, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, <URL: https://yihui.org/knitr/>.\r\nXie Y (2014). “knitr: A Comprehensive Tool for Reproducible Research in R.” In Stodden V, Leisch F, Peng RD (eds.), Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595, <URL: http://www.crcpress.com/product/isbn/9781466561595>.\r\nXie Y, Allaire J, Grolemund G (2018). R Markdown: The Definitive Guide. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9781138359338, <URL: https://bookdown.org/yihui/rmarkdown>.\r\nXie Y, Cheng J, Tan X (2021). DT: A Wrapper of the JavaScript Library ‘DataTables’. R package version 0.17, <URL: https://CRAN.R-project.org/package=DT>.\r\nXie Y, Dervieux C, Riederer E (2020). R Markdown Cookbook. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9780367563837, <URL: https://bookdown.org/yihui/rmarkdown-cookbook>.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-27-academic-journals-through-the-lens-of-wikidata/images/5_year_output.png",
    "last_modified": "2021-05-27T01:57:25+03:00",
    "input_file": {},
    "preview_width": 2834,
    "preview_height": 1889
  },
  {
    "path": "posts/2021-05-24-riro/",
    "title": "RIRO - Russian Index of the Research Organizations",
    "description": "With Web of Science & Scopus, ROR & GRID, Wikidata & Microsoft Academic, ISNI & other providers of the research organization identifiers, is there a need for another one? In this post I am going to answer this question by telling you about RIRO project, launched by me and Ivan Sterligov today. You will see what RORI is, how to get the data, and what value it can provide for the bibliometric research.",
    "author": [
      {
        "name": "Aleksei Lutai",
        "url": "https://www.linkedin.com/in/lutaya/"
      },
      {
        "name": "Ivan Sterligov",
        "url": {}
      }
    ],
    "date": "2021-05-25",
    "categories": [
      "russian data",
      "organization identifier",
      "scopus",
      "r",
      "wikidata",
      "web of science",
      "ror",
      "riro",
      "microsoft academic"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is RIRO?\r\nWhy do we need RIRO?\r\nSelection Criteria for Organizations\r\nRIRO dataset\r\nTable 1 - Official Info\r\nTable 2 - Locations and Geodata\r\nTable 3 - Hierarchy\r\nTable 4 - ROR\r\nTable 5 - Wikidata\r\nTable 6 - Scopus\r\nTable 7 - Microsoft Academic\r\nTable 8 - InCites\r\nTable 9 - SciVal\r\nTable 10 - Russian Universities Assessment System\r\nTable 11 - Web of Science\r\nAll IDs for 3 organizations\r\nRIRO’s scope?\r\nAdditional Notes\r\nFeedback\r\nAcknowledgments\r\n\r\nWhat is RIRO?\r\nRIRO is a public, open, and autonomous project, designed to link the numerous identifiers of the Russian organizations to the existing legal entities.\r\npublic as anyone can use it (free as in speech);\r\nopen as anyone can contribute to it (no strings attached);\r\nautonomous as it is self-governed (free as a bird).\r\nIn its current form RIRO v.1.0 is a set of tables linking the organization identifiers and profiles provided by ROR, GRID, Wikidata, Microsoft Academic, Scopus, SciVal, Web of Science, InCites to the Russian legal entities.\r\nThe RIRO tables are available at Zenodo RIRO community: https://zenodo.org/communities/riro/.\r\nThe code & examples of use will be shared via GitHub: https://github.com/OpenRIRO.\r\nThe news & related information will be updated at web site.\r\nWhy do we need RIRO?\r\nThe databases mentioned above have assigned the identifiers to the organizations and put some efforts into curation process, but none of them is confident when it comes to:\r\ncovering the small, less prominent organizations (terra incognita)\r\nupdating the account reorganizations (what government can resist to it?)\r\nAs a result, the identifiers provided by the international services suffer from a patchy coverage and have a limited value for the research assessment and scientometric analysis.\r\nWith RIRO we aim to:\r\ncreate, update, and share a public dataset of the organization identifiers that reflects not only their current statuses, but also their recent (at least) transformations;\r\nasssess how complete is a representation of the Russian organizations in the international registries;\r\nattract the researh community’s attention to the new information services and its features.\r\nSelection Criteria for Organizations\r\nAs RORI is about research, any organization that appears in the affiliations texts of the scientific publications is a good candidate for RORI. And yet some organizations have been prioritized for v.1.0, those are:\r\nstate (federal/regonal) organizations doing fundamental or applied research (research centres, scientific institutes, universities, large clinical centers or hospitals, reserves, museums, etc.);\r\nprivate organizations (mostly universities).\r\nWe have not (even) reviewed the entities associated with the Russian state corporations (RosAtom, RosSpace, RosTech) and the military ones.\r\nRIRO dataset\r\nThe RIRO dataset v.1.0 is a set of CSV tables, having a primary key named code, so one can easily join the tables or build a database.\r\nThe CSV tables can be downloaded from Zenodo community, via OAI-PMH Harvesting API or by using REST API.\r\nBelow I demonstrate how to download the dataset using zen4R package. This will require of you to do the following actions:\r\nInstall [zen4R] package (https://github.com/eblondel/zen4R))\r\nRegister at Zenodo\r\nObtain a token here\r\n\r\nShow code\r\n# initialize a session\r\nzenodo <- ZenodoManager$new(token = your_zenodo_token,  logger = \"INFO\")\r\n\r\n# request the Concept ID (more info -> https://help.zenodo.org/#versioning)\r\nmy_rec <- zenodo$getRecordByConceptId(\"4775290\")\r\n\r\n# get the list of files with names and URLs\r\nfiles <- my_rec$files %>% map_df(~.x %>% flatten()) %>% select(filename, filesize, download)\r\n\r\n# if you set a folder named RIRO_tables for files, download them with\r\nwalk2(files$download, paste0(dir, files$filename), \r\n      ~download.file(url =.x, destfile = .y))\r\n\r\n# At https://help.zenodo.org/#versioning Zenodo says that:\r\n# Currently the Concept DOI resolves to the landing page of the latest version of your record. \r\n# This is not fully correct, and in the future we will change this to create a landing page \r\n# specifically representing the concept behind the record and all of its versions.\r\n# so one day we may have to retrieve the latest DOI my_rec$getLastDOI() or smth else \r\n\r\n\r\n\r\nLet’s see what we have downloaded.\r\n\r\nShow code\r\nriro_files <- list.files(dir, full.names = TRUE)\r\nriro_files_info <- riro_files  %>% file.info() %>% select(size)\r\nriro_files_info\r\n\r\n\r\n                                                 size\r\nD://RIRO_tables/v.1.0_table_01_basic.csv      3156243\r\nD://RIRO_tables/v.1.0_table_02_geo.csv        1819735\r\nD://RIRO_tables/v.1.0_table_03_hierarchy.csv   191991\r\nD://RIRO_tables/v.1.0_table_04_ror.csv         468079\r\nD://RIRO_tables/v.1.0_table_05_wiki.csv        758197\r\nD://RIRO_tables/v.1.0_table_06_scopus.csv      361781\r\nD://RIRO_tables/v.1.0_table_07_mag.csv          14716\r\nD://RIRO_tables/v.1.0_table_08_incites.csv      75484\r\nD://RIRO_tables/v.1.0_table_09_scival.csv       24105\r\nD://RIRO_tables/v.1.0_table_10_monitoring.csv   17944\r\nD://RIRO_tables/v.1.0_table_11_wos_names.csv   354958\r\n\r\nThere are 11 files with an approx. total size 6.9 MB.\r\nBelow are the parts of each table corresponding to the 3 organizations:\r\nKomi Federal Research Centre of the Ural branch of the Russian Academy of Sciences\r\nTomsk National Medical Research Centre\r\nRussian Technological University MIREA\r\nI selected these organizations as they came through a chain of reorganizations in the last 10 years.\r\nTable 1 - Official Info\r\nTable 1 comprises the basic organization details - OGRN (Primary State Registration Number), INN (Taxpayer Identification Number), KPP (Tax Registration Reason Code), full & short names, status {active, liquidated, in reorganization process}, and the branch type {head or branch}.\r\nA table below lists the legal entities found by OGRN and their branches (the branch organization and its head entity share same OGRN and INN). To save some space a column with the short names column is not included.\r\n\r\nShow code\r\nt1 <- riro_files[1] %>% read_csv(col_types = cols(.default = col_character())) \r\n\r\n# filtering data for 3 organizations by OGRN \r\ntest_group <- t1 %>% filter(ogrn %in% c(\"1021100511332\", \"1037739552740\", \"1027000861568\"))\r\n\r\ntest_group %>% select(code, level, status, name_full, ogrn, inn, kpp) %>% \r\n  datatable(rownames = FALSE, filter = \"none\", escape = FALSE, class = \"row-border\", \r\n      options = list(columnDefs = list(list(width = '650px', targets = c(3)))))\r\n\r\n\r\n\r\n\r\nEach row has its unique “code” which serves as a primary key for all the RIRO tables.\r\nTable 2 - Locations and Geodata\r\nTable 2 comprises the full address and its separate parts (in Russian), accompanied with the geocode, geo coordinates and time zone. Table 2 is the only table in RIRO that corresponds 1:1 to Table 1 via “code”. The other tables can have few rows for unique code.\r\n\r\nShow code\r\nt2 <- riro_files[2] %>% read_csv(col_types = cols(.default = col_character()))  \r\n\r\nt2 %>% filter(code %in% test_group$code) %>% \r\n  datatable(rownames = FALSE, filter = \"none\", escape = FALSE, \r\n            class = \"row-border\", \r\n            options = list(columnDefs = list(\r\n              list(width = '450px', targets = c(6)))))\r\n\r\n\r\n\r\n\r\nTable 3 - Hierarchy\r\nThis is a very important table, as it links the parent organizations not only to its existing branches, but also to the predecessors (for convenience, in this document both will be referred as “children accounts”).\r\nTable 3 does not pretend to be complete for few reasons:\r\nthe list includes only the last predecessors of the current organizations, so there is no historical perspective from 2000s or from USSR.\r\nsome organizations have many branch offices (e.g. hospitals), but information about their hierarchy has little or no value from research assessment perspective. Therefore, for some organizations RIRO does not show the branches. The organizations including the branches are mainly the federal organizations (subdued to the ministries and the federal agencies).\r\nThe “child_code” is a code for the children account, the values in the “relation” column reflect a nature of their subordination (it can be a branch or a predecessor).\r\n\r\nShow code\r\nt3 <- riro_files[3] %>% read_csv(col_types = cols(.default = col_character()))\r\n\r\nfull_test <- t3 %>% filter(code %in% test_group$code) %>% arrange(desc(code)) %>% \r\n  mutate(relation = ifelse(relation == \"Филиал\", \"Branch\",\"Predecessor\"))\r\n\r\nfull_test %>% datatable(rownames = FALSE, filter = \"none\", escape = FALSE, class = \"row-border\", \r\n            options = list(autoWidth = FALSE, \r\n                           columnDefs = list(list(className = 'dt-center', targets = c(0:2)))))\r\n\r\n\r\n\r\n\r\nThus, using a list of OGRNs for 3 selected organizations, we extracted from Table 1 6 entities with unique codes (head & branch organizations), further used to retrieve a list of all the predecessors. As a result we have build a list of 42 entities with unique code values.\r\n\r\nShow code\r\nhierarchy <- full_test %>% \r\n  add_row(code = unique(full_test$code), \r\n          child_code = unique(full_test$code),\r\n          relation = \"Parent_Org\") %>% \r\n  rename(parent_code = code, code = child_code) %>% \r\n  arrange(parent_code, relation) \r\n\r\n\r\n\r\nNow we can see how many branches & predecessors exist for 3 selected orgs.\r\n\r\nShow code\r\nhierarchy %>% count(parent_code, relation) %>% \r\n  pivot_wider(names_from = relation, \r\n              values_from = n, values_fill = 0) %>%\r\n  select(parent_code, Parent_Org, Branch, Predecessor) %>% \r\n  datatable(rownames = FALSE, filter = \"none\", \r\n            escape = FALSE, class = \"row-border\", \r\n            options = list(autoWidth = FALSE, \r\n  columnDefs = list(list(className = 'dt-center', \r\n                         targets = c(0:3)))))\r\n\r\n\r\n\r\n\r\nWe will use this hierarchy to gather the matched identifiers from other RIRO tables (see below).\r\nTable 4 - ROR\r\nResearch Organizations Registry (ROR) is an international project launched in 2019 with an ambitious goal to create a public ORCID-like registry for the research organizations. It inherits a lot from GRID (Global Research Identifier Database) and (I guess) from Wikidata. The ROR organization info can be downloaded as a JSON dump or retrieved via API.\r\nTable 4 contains not all the Russian records from ROR, but only those that we matched to the organizations present in RORI.\r\n\r\nShow code\r\nt4 <- riro_files[4] %>% read_csv(col_types = cols(.default = col_character()))  \r\n\r\nhierarchy %>% inner_join(t4, by = \"code\") %>% \r\n  arrange(parent_code) %>% select(-parent_code, - relation, -ror_isni) %>% \r\n  datatable(rownames = FALSE, filter = \"none\", \r\n            escape = FALSE, class = \"row-border\", \r\n            options = list(columnDefs = list(\r\n              list(width = '250px', targets = c(3:4)),\r\n              list(width = '350px', targets = c(5)),\r\n              list(width = '450px', targets = c(8)))))\r\n\r\n\r\n\r\n\r\nThe column “Relationships” have the composite values of following structure:\r\nlabel:xxxx|type:yyyyyy|id:https://ror.org/zzzzz,\r\nwith 3 units (label, type, id) for the relative (according to ROR) organizations.\r\nBut in cases like our example such references can be misleading! According to ROR the research institutes of the Komi Federal Research Center have different parents:\r\nlabel:Department of Chemistry and Material Sciences|type:Parent|id:059tqvg48\r\nlabel:Ural Branch of the Russian Academy of Sciences|type:Parent|id:02s4h3z39\r\nlabel:Russian Academy of Sciences|type:Parent|id:05qrfxd25\r\nSo for a single organization ROR shows 4 accounts subordinating to three different RAS structures.\r\nThe actual truth is that the found 4 research institutes of the Komi Federal Research Center ceased to exist as legal entities 3 years ago, they were merged into one federal research center in May 2018.\r\nUsing the Table 3, one can gather the related identifiers and qualify them as corresponding to a branch or a predecessor.\r\nTable 5 - Wikidata\r\nWikiData is a public repository of structured data originating from multiple sources. Some sources are more or less consistent (like CrossRef or ISSN), but there’s also a lot of Wikidata records that are created and modified by people. As a result, even though Wikidata offers a pre-defined templates for the universities or research organizations, Wikidata profiles have a lot of unpopulated fields.\r\nThe table 5 comprises a list of fields found in the Wikidata profiles for the organizations, but not a full copy. Moreover, Table 5 include only those Russian research organizations that match to the organizations in RIRO.\r\n\r\nShow code\r\nt5 <- riro_files[5] %>% read_csv(col_types = cols(.default = col_character())) \r\n\r\nhierarchy %>% inner_join(t5, by = \"code\") %>% \r\n  arrange(parent_code) %>% \r\n  select(-parent_code, - relation, -wd_item) %>%  \r\n  mutate_at(c(\"wikipedia_eng\", \"wikipedia_rus\", \"wd_itemaltlabel\", \"wd_altlabel\"),\r\n            ~ifelse(is.na(.x),.x, paste0(substr(.x,1,40),\"...\")))%>% \r\n  datatable(rownames = FALSE, filter = \"none\", escape = FALSE, class = \"row-border\", \r\n            options = list(columnDefs = list(\r\n              list(width = '250px', targets = c(2:4)),\r\n              list(width = '150px', targets = c(6:9)))))\r\n\r\n\r\n\r\n\r\nTable 6 - Scopus\r\nScopus is a (one of leading) citation index accumulating the metadata from 20k+ journal titles, selected conference sources, and some academic book titles. Table 6 lists the Scopus affiliation profiles matched to the organizations in RIRO, and also a number of publications under Scopus affiliation profile (on a data of request, April 2021).\r\nScopus Affiliation IDs can be used for search queries via online UI or the API-service. The latter has few wrappers for python and R that make working with API more comfortable.\r\nPlease note that matching the Scopus affiliation profiles to RIRO organizations is based on the affiliation name and location. It does not guarantee that all the publications in the profile are assigned to it correctly. More details on how to edit the affiliation profiles in Scopus can be found on Elsevier web site.\r\n\r\nShow code\r\nt6 <- riro_files[6] %>% read_csv(col_types = cols(.default = col_character())) \r\n\r\nhierarchy %>% inner_join(t6, by = \"code\") %>% \r\n  arrange(parent_code) %>% select(-parent_code, - relation) %>% \r\n  datatable(rownames = FALSE, filter = \"none\", escape = FALSE, class = \"row-border\", \r\n            options = list(columnDefs = list(\r\n              list(className = 'dt-center', targets = c(0,6)),\r\n              list(width = '350px', targets = c(2:3)))))\r\n\r\n\r\n\r\n\r\nTable 7 - Microsoft Academic\r\nMicrosoft Academic Graph (MAG) is a database created based on the information extracted with Bing-parsers from the publisher web sites and PDF files details). This approach is different from the one utilized by Web of Science and Scopus that receive a large part of information for indexation directly from the publishers.\r\nIn recent years MAG has become a source of information for many novel solutions like Lens, Semantic Scholar, Open Academic Graph, Unsub.\r\nEven though the last news about MAG shocked us too, we decided to include the MAG Organization IDs into RORI. Few international companies committed to launching a new tool that may substitute MAG:\r\n\r\n\r\nANNOUNCING: We’re building a replacement for Microsoft Academic Graph. https://t.co/GXelkpt6Zc\r\n\r\n— Our Research (@our_research) May 8, 2021\r\n\r\n\r\n\r\nThanks to @MSFTResearch for providing the Microsoft Academic Graph (MAG). We’ve been working with MSR and MAG since 2018, and we’ve been collaborating on this transition for some time. 1/3https://t.co/7aHTLio8uK\r\n\r\n— Semantic Scholar (@SemanticScholar) May 11, 2021\r\n\r\nTable 7 lists just the MAG organization IDs and names agains the RORI codes.\r\n\r\nShow code\r\nt7 <- riro_files[7] %>% read_csv(col_types = cols(.default = col_character())) \r\n\r\nhierarchy %>% inner_join(t7, by = \"code\") %>% \r\n  arrange(parent_code) %>% select(-parent_code, - relation) %>% \r\n  datatable(rownames = FALSE, filter = \"none\", escape = FALSE, class = \"row-border\", \r\n            options = list(autoWidth = FALSE,\r\n              columnDefs = list(\r\n                list(className = 'dt-center', targets = c(0:1)),\r\n                list(width = '450px', targets = c(2)))))\r\n\r\n\r\n\r\n\r\nTable 8 - InCites\r\nInCites is an analytical solution build over Web of Science Core Collection. It allows to export the records, so the matched names can be used for further analysis.\r\nThe table 8 lists the official organization names in InCite and Web of Science Core Collection against the RORI codes.\r\n\r\nShow code\r\nt8 <- riro_files[8] %>% read_csv(col_types = cols(.default = col_character())) \r\n\r\nhierarchy %>% inner_join(t8, by = \"code\") %>% \r\n  arrange(parent_code) %>% select(-parent_code, - relation) %>% \r\n  datatable(rownames = FALSE, filter = \"none\", escape = FALSE, class = \"row-border\", \r\n            options = list(autoWidth = FALSE,\r\n              columnDefs = list(\r\n                list(className = 'dt-center', targets = c(0,3)),\r\n                list(width = '500px', targets = c(1:2)))))\r\n\r\n\r\n\r\n\r\nTable 9 - SciVal\r\nSciVal is an analytical tool build over Scopus. Some Russian organizations have an access to SciVal API and could use the IDs matched against the RORI codes in the table 9.\r\n\r\nShow code\r\nt9 <- riro_files[9] %>% read_csv(col_types = cols(.default = col_character()))\r\n\r\nhierarchy %>% inner_join(t9, by = \"code\") %>% \r\n  arrange(parent_code) %>% select(-parent_code, - relation) %>% \r\n  datatable(rownames = FALSE, filter = \"none\", escape = FALSE, class = \"row-border\", \r\n            options = list(autoWidth = FALSE,\r\n              columnDefs = list(\r\n                list(className = 'dt-center', targets = c(0:1)),\r\n                list(width = '450px', targets = c(2)))))\r\n\r\n\r\n\r\n\r\nTable 10 - Russian Universities Assessment System\r\nThis system governed by the Russian Ministry of Science & Higher Education, collects the various statistical reports from all Russian higher education institutions (excluding some schools under the Ministry of Defence and alike). Such reports contain a lot of useful information - from financial to enrollment data.\r\nTable 10 lists the IDs that corresponds to the university’s web page on the portal, matched to the RORI codes.\r\n\r\nShow code\r\nt10 <- riro_files[10] %>% read_csv(col_types = cols(.default = col_character()))  \r\n\r\nhierarchy %>% inner_join(t10, by = \"code\") %>% \r\n  arrange(parent_code) %>% select(-parent_code, - relation) %>% \r\n  datatable(rownames = FALSE, filter = \"none\", escape = FALSE, class = \"row-border\", \r\n            options = list(autoWidth = FALSE,\r\n              columnDefs = list(list(className = 'dt-center', \r\n                                     targets = c(0:1)))))\r\n\r\n\r\n\r\n\r\nTable 11 - Web of Science\r\nWeb of Science is by far the world’s oldest and most prominent citation index. At this moment Web of Science does not provide the organization IDs that could be used for search or data retrieval, but the search results have the orgaization names. The table 11 lists almost 4000 such names matched to the organizations in RIRO. This is not a complete list of known affiliation names for the Russian research organizations, but we hope to adjust this table in future releases of RIRO.\r\n\r\nShow code\r\nt11 <- riro_files[11] %>% read_csv(col_types = cols(.default = col_character())) \r\n\r\nhierarchy %>% inner_join(t11, by = \"code\") %>% \r\n  arrange(parent_code) %>% select(-parent_code, - relation) %>% \r\n  datatable(rownames = FALSE, filter = \"none\", \r\n            escape = FALSE, class = \"row-border\", \r\n            options = list(autoWidth = FALSE,\r\n              columnDefs = list(\r\n                list(className = 'dt-center', targets = c(0)),\r\n                list(width = '500px', targets = c(1,3)))))\r\n\r\n\r\n\r\n\r\nAll IDs for 3 organizations\r\nNow, as we have glanced at the identifiers found in each table for 3 selected organizations, we are ready for a wider picture.\r\nAn illustration below shows the identifiers (ROR, GRID, Scopus Affiliation ID, InCites ID, MAG, Wikidata, 1-Monitoring) matched to parent organizations, branches and predecessors - each organization in a separate section. The identifiers are placed along X-axis (by organization). The entities (RORI records) are placed along Y-axis – the existing organizations are shown as squares (the parent organizations are marked with a special sign), the predecessors as circles.\r\n\r\nShow code\r\nknitr::include_graphics(chart_filename)\r\n\r\n\r\n\r\n\r\nThis picture is not an example of clarity, I admit, so let me explain it more thoroughly. The identifiers for each organization are present in a separacte section (from left to right). Each section has its own number of horizontal rows corresponding to the parent organization, branches and predecessors (from top to bottom). Marked as squares are the existing organizations - either a parent one (also marked with a sign, on top) or the branches (under the parent). Marked as circles (with less saturated colours) are the identifiers corresponding to the predecessors (liquidated organizations).\r\nOr in a more plain language - every circle is an organization identifier that corresponds to a predecessor, not to existing legal entity.\r\nRIRO’s scope?\r\nAn UpSet diagram below shows the sets of organizations listen in RIRO (v.1.0) and matched to the various identifiers. Only the head and active (existing) organizations are counted here, so the identifiers referring to the branches or to the predecessors (liquidated via acquisition), are not counted.\r\n\r\nShow code\r\nknitr::include_graphics(chart_filename2)\r\n\r\n\r\n\r\n\r\nRIRO v.1.0 lists 1774 existing parent organizations. Many of those have no external identifiers, but (as previous picture proved) may have a lot of identifiers corresponding to the branches or predecessors.\r\nAdditional Notes\r\nRIRO does not copy all the attributes linked to the records in the sources. We selected only the most useful (to our opinion) fields. In future RIRO releases we may add more fields.\r\nThe data in the tables 4-11 is the same as in the original sources - we have not changed the original records.\r\nSome tables share the common fields. For example, ROR (table 4) has a column Wikidata, and Wikidata (table 5) has a column ROR. We did not chang or check those links, they are kept as they are present in the original sources, but renamed to avoid confusion (added a prefix referring to the source - ror_wikidata, wd_ror, etc.\r\nMatching the identifiers to the organizations was made based on available information (name, location) - we have not checked if the data asssociated with the identifiers (in the source databases) is correctly assigned to the organizations.\r\nFeedback\r\nSo far RIRO is developed by only 2 persons - me and Ivan Sterligov orcid - so we decided to start with a google form (in Russian) with 5 pre-defined scenarios of change requests:\r\nupdate the official organization data\r\nadd new organization absent in RIRO\r\nchange a link between the identifier & organization in tables 4-11 (ROR, GRID, Wikidata, etc)\r\nadd new links, missing in the tables 4-11\r\nsuggest a new data source & identifiers).\r\nThe future RORI releases will include a log of made changes.\r\nAcknowledgments\r\nAllaire J, Xie Y, McPherson J, Luraschi J, Ushey K, Atkins A, Wickham H, Cheng J, Chang W, Iannone R (2021). rmarkdown: Dynamic Documents for R. R package version 2.7, <URL: https://github.com/rstudio/rmarkdown>.\r\nBlondel E (2021). zen4R: Interface to ‘Zenodo’ REST API. R package version 0.4-3, <URL: https://github.com/eblondel/zen4R>.\r\nChang, W (2014). extrafont: Tools for using fonts. R package version 0.17, <URL: https://CRAN.R-project.org/package=extrafont>.\r\nHenry L, Wickham H (2020). purrr: Functional Programming Tools. R package version 0.3.4, <URL: https://CRAN.R-project.org/package=purrr>.\r\nKrassowski M (2020). “ComplexUpset.” doi: 10.5281/zenodo.3700590 (URL: https://doi.org/10.5281/zenodo.3700590), <URL: https://doi.org/10.5281/zenodo.3700590>.\r\nLex A, Gehlenborg N, Strobelt H, Vuillemot R, Pfister H (2014). “UpSet: Visualization of Intersecting Sets,.” IEEE Transactions on Visualization and Computer Graphics, 20(12), 1983–1992. doi: 10.1109/TVCG.2014.2346248 (URL: https://doi.org/10.1109/TVCG.2014.2346248), <URL: https://doi.org/10.1109/TVCG.2014.2346248>.\r\nWickham H (2020). tidyr: Tidy Messy Data. R package version 1.1.2, <URL: https://CRAN.R-project.org/package=tidyr>.\r\nWickham H (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN 978-3-319-24277-4, <URL: https://ggplot2.tidyverse.org>.\r\nWickham H (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=stringr>.\r\nWickham H, Francois R, Henry L, Muller K (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.3, <URL: https://CRAN.R-project.org/package=dplyr>.\r\nWickham H, Hester J (2020). readr: Read Rectangular Text Data. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=readr>.\r\nWickham H, Seidel D (2020). scales: Scale Functions for Visualization. R package version 1.1.1, <URL: https://CRAN.R-project.org/package=scales>.\r\nXie Y (2020). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.30, <URL: https://yihui.org/knitr/>.\r\nXie Y (2015). Dynamic Documents with R and knitr, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, <URL: https://yihui.org/knitr/>.\r\nXie Y (2014). “knitr: A Comprehensive Tool for Reproducible Research in R.” In Stodden V, Leisch F, Peng RD (eds.), Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595, <URL: http://www.crcpress.com/product/isbn/9781466561595>.\r\nXie Y, Allaire J, Grolemund G (2018). R Markdown: The Definitive Guide. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9781138359338, <URL: https://bookdown.org/yihui/rmarkdown>.\r\nXie Y, Cheng J, Tan X (2021). DT: A Wrapper of the JavaScript Library ‘DataTables’. R package version 0.17, <URL: https://CRAN.R-project.org/package=DT>.\r\nXie Y, Dervieux C, Riederer E (2020). R Markdown Cookbook. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9780367563837, <URL: https://bookdown.org/yihui/rmarkdown-cookbook>.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-24-riro/images/RIRO_identifiers_for_3_orgs.png",
    "last_modified": "2021-05-27T02:21:29+03:00",
    "input_file": {},
    "preview_width": 2834,
    "preview_height": 1574
  },
  {
    "path": "posts/2021-05-10-vak-titles-in-crossref/",
    "title": "Checking VAK titles in CrossRef",
    "description": "In this post I am checking how many academic journals from the Russian white list (VAK) deposited their 2020/2021 publications in CrossRef. To do that I queried portal.issn.org & CrossRef, and parsed PDF & JSON files, what I think makes this post of interest to those who analyze the academic journals.",
    "author": [
      {
        "name": "Aleksei Lutai",
        "url": "https://www.linkedin.com/in/lutaya/"
      }
    ],
    "date": "2021-05-12",
    "categories": [
      "russian data",
      "issn.org",
      "crossref",
      "r",
      "gov data"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\n1. Dataset Preparation\r\n2. Harvesting data from ISSN.org\r\n3. ISSN info (a quick glance)\r\n4. Harvesting CrossRef data\r\n5. CrossRef info (a quick glance)\r\n6. Final table\r\nLimitations\r\nShared data\r\nAcknowledgments\r\n\r\nIntroduction\r\nIn a previous post I ventured to parse a 853-page PDF file released by VAK (Russian state regulator) and extracted a list of journal titles. For the Russian academic journals being listed by VAK is a sort of endorsement, as it makes the publications scored:\r\nin some assessment exercises run by funders & ministries,\r\nfor academic degree pre-assessment (main results must be published in at least X credible journals).\r\nThe international journals indexed in Scopus, WoS, CAS, etc are also counted as credible titles, so VAK list is not a big deal for those who can draft a proper research manuscript in English language (or pay for that) and compete for a publication in the international journals.\r\nOther researchers (no, not “are quite dreadful,” as Oscar Wilde suggested) may have their reasons to place their materials in the VAK-listed journals. But what I think many people are missing here is that some of VAK titles are absent in CrossRef and invisible to the international search engines. Sort of ferae naturae, endangered to disappear in case of … the publisher’s bankrputcy or anything else.\r\nSo I decided to check which VAK titles are present in CrossRef with 2020/2021 publications. This is not a difficult thing to do via CrossRef API, but life, of course, proves not to be that plain - the titles can be registered in CrossRef not with the ISSN present in the VAK list, but with the other one, not listed in the VAK list.\r\nThis means that first I need to collect the additional ISSNs from ISSN Portal and only then to check them in CrossRef.\r\n1. Dataset Preparation\r\nThe parsed list can be downloaded from Figshare with the following code:\r\n\r\nShow code\r\nlibrary(httr)\r\nlibrary(rvest)\r\n\r\ndataset_link <- read_html(\"https://doi.org/10.6084/m9.figshare.14561814\", \r\n                          encoding = \"UTF-8\") %>% \r\n  html_nodes(\"a\") %>% html_attr(\"href\") %>% \r\n  .[grepl(\"ndownloader\",.)] %>% unique()\r\n\r\ndata <- httr::GET(dataset_link) %>% httr::content()\r\n\r\n\r\n\r\nbut for speed I will take it from a hard disk.\r\n\r\nShow code\r\ndata <- read_csv(paste0(dir, \"2021_04_vak_list_parsed_data.csv\"), \r\n                      col_types = cols(.default = col_character()))\r\n\r\n\r\n\r\nAt the next step I extract the unique ISSNs and here came the first observation. The VAK list contained the different rows sharing the same ISSNs.\r\n\r\nShow code\r\nissns_list <- data %>% select(title, issns = issn) %>% \r\n  mutate(issn = str_split(issns,\",\")) %>% unnest(issn) %>% \r\n  distinct() %>% na.omit()\r\n\r\nissns_list %>% group_by(issn) %>% \r\n  mutate(n_titles = n_distinct(title)) %>% ungroup() %>% \r\n  filter(n_titles>1) %>% select(title, issns, issn) %>% arrange(issn) %>% \r\n  mutate(issns = gsub(\",\",\", \", issns)) %>% \r\n  DT::datatable(escape = FALSE, class = 'compact',\r\n                options = list(pageLength = 5, deferRender = TRUE, \r\n                               autoWidth = TRUE,  ordering = TRUE, scrollX = TRUE))\r\n\r\n\r\n\r\n\r\nIt appears like some journals have individual e-versions, but are printed under one cover. Well, if so, this can be understood, of course. I do not check this and leave it as it is.\r\nI also manually corrected 2 ISSNs that had wrong format in the original VAK list - “999–4133” (corrected version is 1999-4133) and “224–9877” (changed to 2224-9877).\r\n2. Harvesting data from ISSN.org\r\nTo obtain information from ISSN.org I am using a query that saves the results to the hard disk in PDF format:\r\nhttps://portal.issn.org/custom-search/print/2077-9038/public\r\nFor this I created a downloading function “readUrl”, which is a wrapper of download.file function into a life-saving tryCatch. So the function takes URL and filename, saves the file and returns OK or an error.\r\n\r\nShow code\r\ndir0 <- paste0(dir, \"issns/\")\r\n\r\nissns_list <- issns_list %>% mutate(issn_json = \"\", issn_pdf = \"\")\r\n\r\nreadUrl <- function(url, filename) {\r\n  out <- tryCatch({\r\n    download.file(url, filename, quiet = TRUE, mode = \"wb\")\r\n    return(\"OK\")\r\n  },\r\n  error=function(cond){return(NA)}\r\n  )\r\n  return(out)\r\n}\r\n\r\n\r\n\r\nAnd now - let’s run a cycle and download all the pdf files to the hard disk.\r\n\r\nShow code\r\nfor (k in which(issns_list$issn_pdf==\"\"|is.na(issns_list$issn_pdf))){  \r\n  print(k)\r\n  q <- issns_list$issn[k]\r\n  url <- paste0(\"https://portal.issn.org/custom-search/print/\",q,\"/public\")\r\n  filename <- paste0(dir0, gsub(\"-\",\"_\", q), \"_issn.pdf\")\r\n  issns_list$issn_pdf[k] <- ifelse(file.exists(filename),\r\n                             \"OK\", readUrl(url, filename))\r\n}\r\n\r\n\r\n\r\nThe Portal gives a quick response, so a whole process took about 30 minutes.\r\nNext step would be to parse the downloaded PDF files and link it to the original VAK list.\r\nI will extract a text layer using pdftools package. The information is arranged in the following format Key:Value, so taking some measures for not losing the broken lines and not processing the special lines containing “:” , I have extracted it all.\r\n\r\nShow code\r\nlibrary(pdftools)\r\n\r\ntt <- tibble() \r\n \r\nfor (z in 1:nrow(issns_list)){\r\n  print(z)\r\n  q <- issns_list$issn[z]\r\n  filename <- paste0(dir0, gsub(\"-\",\"_\", q), \"_issn.pdf\")\r\n  if(file.exists(filename)){\r\n  txt <- suppressMessages(pdf_text(filename)) %>% paste(collapse=\"\\\\\\r\\\\\\n\")  \r\n  # some PDFs contains history of publishers, we take only the current info\r\n  if(grepl(\"From:\", txt)){\r\n    txt <- strsplit(txt, split = \"From:\") %>% map_chr(`[`,1)\r\n    }\r\n  txt <- strsplit(txt, split = \"\\\\\\r\\\\\\n\") %>% \r\n    map_dfc(~.x) %>% rename(extracted = 1) %>% \r\n    mutate(tag = ifelse(grepl(\"\\\\:\",extracted), row_number(), NA_integer_)) %>%\r\n    mutate(tag = ifelse(grepl(\"Seri.:|Серия:\", extracted), NA_integer_,tag)) %>%\r\n    fill(tag, .direction = \"down\") %>% \r\n    group_by(tag) %>% \r\n    summarize(txt = paste(unique(na.omit(extracted)), collapse = \" \")) %>% \r\n    ungroup() %>% \r\n    mutate(issn = q)\r\n  \r\n  tt <- bind_rows(tt, txt)\r\n    }\r\n}\r\n\r\n# a bit of cleaning / glueing here\r\ntt_group <- tt %>% \r\n  # separating the key and the value\r\n  ## a bit cmplex regex to avoid splitting after the word \"Seria\":\r\n  mutate(tag = ifelse(grepl(\"^[^:]*?Seri.:|^[^:]*?Серия:\", txt), NA_integer_,tag)) %>%\r\n  mutate(tag = ifelse(grepl(\"^http\", txt), NA_integer_,tag)) %>%\r\n  fill(tag, .direction = \"down\") %>% \r\n  group_by(issn, tag) %>% \r\n  summarize(txt = paste(unique(na.omit(txt)), collapse = \" \")) %>% \r\n  ungroup() \r\n\r\n# glueing  \r\ntt_group <- tt_group %>%   \r\n  mutate(key = str_extract(txt, \"^[^:]+?:\"),\r\n         txt = str_replace(txt, \"^[^:]+?:\",\"\")) %>% \r\n  mutate(txt = gsub(\"!Seri\",\": Seri\", txt)) %>% \r\n  mutate_all(~str_squish(str_replace_all(.x, \"\\\\\\\\\\\\r\\\\\\\\\\\\n|:$\",\"\"))) %>% \r\n  # gluing together\r\n  group_by(issn, key) %>% \r\n  summarize(value = paste0(unique(na.omit(txt)), collapse = \" | \")) %>% \r\n  ungroup() %>%  \r\n  # converting into a wide format\r\n  pivot_wider(id_cols = issn, names_from = key, values_from = value) %>% \r\n  distinct()\r\n\r\nissns_list <- issns_list %>% \r\n  select(-issn_json, -issn_pdf) %>%  \r\n  left_join(tt_group) %>% distinct()\r\n\r\nwrite_excel_csv(issns_list, paste0(dir, \"2021_04_vak_issn_info.csv\"))\r\n\r\n\r\n\r\n3. ISSN info (a quick glance)\r\n\r\nShow code\r\nissns_list <- read_csv(paste0(dir, \"2021_04_vak_issn_info.csv\"), \r\n                      col_types = cols(.default = col_character()))\r\n\r\n\r\n\r\nAt this stage we quickly look at the information obtained from ISSN Portal for 2678 ISSNs present in the VAK list (2586 titles). The collected data is a dataset of 2686 rows and 44 cols, a large part of it is empty.\r\nFew interesting observations:\r\n(1) there is no information at ISSN Portal for some titles\r\nThe ISSNs to Russian journals are assigned by the National ISSN Centre, so all questions about the titles not being present in ISSN.org must be addressed to them. The table below lists such journals.\r\n\r\nShow code\r\nissns_list %>% select(-issn) %>% distinct() %>% \r\n  filter_at(c(\"Country\", \"ISSN\", \"Key-title\", \"URL\", \"Title proper\"), ~is.na(.x)) %>% \r\n  select(vak_title = title, vak_issns = issns,\r\n         `issn.org issn` = ISSN, `issn.org title` = \"Key-title\") %>% \r\n    DT::datatable(escape = FALSE, class = 'compact',\r\n                options = list(pageLength = 5, deferRender = TRUE, \r\n                               autoWidth = TRUE,  ordering = TRUE, scrollX = TRUE))\r\n\r\n\r\n\r\n\r\n(2).Some ISSNs are not affiliated with Russian Federation\r\n\r\nShow code\r\nissns_list %>% \r\n  select(vak_title = title, vak_issns = issns, \r\n         `issn.org country` = Country, `issn.org title` = \"Key-title\", \r\n         `issn.org language` = Language,\r\n         `issn.org prev.publisher` = `Earliest publisher`, \r\n         `issn.org latest.publisher` = `Latest publisher`) %>%\r\n  filter(!is.na(`issn.org country`)&!grepl(\"Russia\", `issn.org country`)) %>% \r\n  count(`issn.org country`) %>% arrange(-n) %>% \r\n  ggplot()+\r\n  geom_bar(aes(y = reorder(`issn.org country`,n), x = n), stat = \"identity\", \r\n           fill = \"#e05915\", color = \"grey50\", size = 0.1)+\r\n  labs(x = \"number of titles\", \r\n       y = \"country (by issn)\", \r\n       title =  \"Countries of non-Russian VAK journals (per issn.org)\", \r\n       subtitle = \"VAK list as of April 8, 2021\",\r\n       caption = paste0(\"Date: \", format(Sys.time(), '%Y.%m.%d'),\r\n                       \".\\nData: portal.issn.org, vak.minobrnauki.gov.ru\"))+\r\n  scale_x_continuous(expand = expansion(mult = c(0,0.05)), \r\n                     breaks = scales::pretty_breaks(6))+\r\n  my_theme\r\n\r\n\r\n\r\n\r\nQuite a set of countries for the Russian white list!\r\n\r\nShow code\r\nissns_list %>% \r\n  select(vak_title = title, vak_issns = issns, \r\n         `issn.org country` = Country, `issn.org title` = \"Key-title\", \r\n         `issn.org language` = Language,\r\n         `issn.org prev.publisher` = `Earliest publisher`, \r\n         `issn.org latest.publisher` = `Latest publisher`) %>%\r\n  filter(!is.na(`issn.org country`)&!grepl(\"Russia\", `issn.org country`)) %>% \r\n  arrange(`issn.org country`) %>% \r\n  DT::datatable(escape = FALSE, class = 'compact', rownames = FALSE,\r\n                options = list(pageLength = 5, deferRender = TRUE, \r\n                               autoWidth = TRUE,  ordering = TRUE, scrollX = TRUE,\r\n              columnDefs = list(\r\n                list(width = '200px', targets = c(5:6)),\r\n                list(width = '300px', targets = c(0,3)),\r\n                list(className = 'dt-center', targets = c(1)),\r\n                list(className = 'dt-left', targets = c(0, 2:6)))))\r\n\r\n\r\n\r\n\r\nSlovakia case seems to be plain - Russian publisher sold its academic titles to Slovakian company (or registered a new one), so now they are de jure Slovakian. But when it comes to the titles registered as Ukrainian, Belarusian or Tajik, an explanation will not be that plain. I would refrain from speculations about this.\r\nNow we are ready to move on to CrossRef.\r\n4. Harvesting CrossRef data\r\nThere is a convinient R wrapper for CrossRef API rCrossRef, but with the complex queries, with few filters, I like to use API service directly and save JSON files to the hard disk (not to parse them on a flight).\r\nSuch approach makes sense for me - if anything goes wrong I can break the process and start it all over, the saving script checks the existing files and will skip what’s been already saved.\r\nA query to get 5 random articles published after 2019 in a journal title with ISSNs (1811-833X, 2311-7133) will consist of 3 parts:\r\nAPI-server: https://api.crossref.org/works\r\nQuery: ?filter=issn:1811-833X,issn:2311-7133,from-pub-date:2020&rows=5\r\nAttributes: &mailto=hereisyouremail.\r\nOf course, we will also exploit ISSN-Ls collected from ISSN.org.\r\n\r\nShow code\r\nissns_cr <- issns_list %>% \r\n  select(title, issn, issnL = `Linking ISSN (ISSN-L)`) %>%  \r\n  pivot_longer(-1) %>% distinct() %>% \r\n  filter(!is.na(value)) %>% \r\n  group_by(title) %>% \r\n  summarize(issns = paste0(unique(value), collapse = \",\")) %>% \r\n  mutate(cr_file = \"\", code = paste0(\"cr_\",row_number()))\r\n\r\n\r\n\r\nAnd a cycle.\r\n\r\nShow code\r\ndir0 <- paste0(dir, \"pdf_cr/\")\r\nfor (i in which(issns_cr$cr_file==\"\"|is.na(issns_cr$cr_file))){\r\n  print(i)\r\n  q <- issns_cr$issns[i] %>% strsplit(., split = \"\\\\,\")\r\n  url <- paste0(\"https://api.crossref.org/works?filter=\",\r\n              unlist(q) %>% paste0(\"issn:\",., collapse = \",\"),\r\n              \",from-pub-date:2020&rows=5&mailto=\", my_email)\r\n  filename <- paste0(dir0, issns_cr$code[i],\".json\")\r\n  issns_cr$cr_file[i] <- ifelse(file.exists(filename),\"OK\", readUrl(url, filename))\r\n} \r\n\r\n\r\n\r\nI had to make breaks few times during this cycle, so I can not accurately estimate a total execution time, but I feel like that it would not take longer than an hour or so, was it run without interruptions.\r\nLet’s parse it with a package jsonlite to extract some data from CrossRef files.\r\n\r\nShow code\r\nlibrary(jsonlite)\r\ncr_data <- data.frame()\r\n\r\nfor (i in 1:NROW(issns_cr)){\r\n    print(i)\r\n    filename <- paste0(dir0, issns_cr$code[i],\".json\")\r\n    jdoc <- fromJSON(filename, flatten = TRUE) %>% .$message \r\n    \r\n    # total number of articles (it is in a header)\r\n    total <- jdoc %>% .$`total-results` %>% as.character()\r\n    \r\n    # other data (it is in the json items)\r\n    a <- jdoc %>% .$items\r\n    if(length(a)>0){\r\n      if(exists(\"issn-type\", a)){\r\n        cristy <- a$`issn-type` %>% map_dfr(~.x) %>% \r\n          unite(col = \"issns\", c(\"type\", \"value\"),sep=\":\") %>% \r\n          unique() %>% map_chr(paste0, collapse = \"|\")\r\n      }\r\n      if(exists(\"ISSN\", a)){\r\n        crisis <- a$ISSN %>% unlist() %>% unique() %>% paste0(., collapse = \"|\")\r\n      }\r\n   \r\n      a2 <- a %>% select(any_of(c(\"prefix\", \"member\", \"publisher\"))) %>% \r\n        mutate_all(~as.character(.x)) %>%\r\n        mutate(code = issns_cr$code[i], \r\n               cr_issn_type = cristy,\r\n               cr_issns = crisis,\r\n               cr_2020_2021 = total)\r\n      a2 <- a2 %>% \r\n        select(any_of(c(\"prefix\", \"member\", \"publisher\", \"code\", \r\n               \"cr_issn_type\", \"cr_issns\", \"cr_2020_2021\"))) %>% \r\n        pivot_longer(-\"code\") %>% \r\n        unique() \r\n      cr_data <- bind_rows(cr_data, a2)\r\n    }\r\n}\r\n\r\n\r\n\r\nThe parsed data is then grouped and converted into a wide format.\r\n\r\nShow code\r\ncr_data_wide <- cr_data %>% distinct() %>% \r\n  group_by(code, name) %>% \r\n  summarize(value = paste0(unique(na.omit(value)), collapse = \" | \")) %>% \r\n  ungroup() %>% \r\n  pivot_wider(id_cols = code)\r\n  \r\nissns_cr_merged <- issns_cr %>% left_join(cr_data_wide)\r\n\r\nwrite_excel_csv(issns_cr_merged, paste0(dir, \"2021_04_vak_crossref_info.csv\"))\r\n\r\n\r\n\r\n5. CrossRef info (a quick glance)\r\n\r\nShow code\r\nissns_cr_merged <- read_csv(paste0(dir, \"2021_04_vak_crossref_info.csv\"), \r\n                      col_types = cols(.default = col_character(), \r\n                                       cr_2020_2021 =col_integer()))\r\n\r\n## lets merge all the datasets together in this chucnk\r\nissn_bit <- issns_list %>% \r\n  select(title, Country, issnL = `Linking ISSN (ISSN-L)`, \r\n         title.proper = `Title proper`) %>% \r\n  group_by(title) %>% \r\n  summarize_all(~paste(unique(na.omit(.x)), collapse = \" | \")) %>% \r\n  ungroup()\r\n\r\nissn_bit <- issn_bit %>% \r\n  mutate(Country = ifelse(is.na(Country), NA, \r\n                         paste0('ISSN_country=', Country)),\r\n         issnL = ifelse(is.na(issnL), NA, \r\n                          paste0('ISSN_L=', issnL)),\r\n         title.proper = ifelse(is.na(title.proper), NA, \r\n                          paste0('ISSN_proper_title=', title.proper))) %>%\r\n  unite(\"issn_jrnl_creds\", c(title.proper, Country, issnL), \r\n        sep = \"; \", na.rm = TRUE) \r\n           \r\ncr_bit <- issns_cr_merged %>% \r\n  select(title, cr_2020_2021, cr_issn_type, prefix,  publisher, member) %>% \r\n  mutate(prefix = ifelse(is.na(prefix), NA, \r\n                         paste0('DOI_prefix=', prefix)), \r\n         cr_issn_type = ifelse(is.na(cr_issn_type), NA, \r\n                               paste0(\"CR_issns=\", cr_issn_type)), \r\n         publisher = ifelse(is.na(publisher), NA, \r\n                            paste0(\"CR_publisher=\", publisher)), \r\n         member = ifelse(is.na(member), NA, \r\n                         paste0(\"CR_member=\", member))) %>% \r\n  unite(\"cr_jrnl_creds\", c(cr_issn_type, prefix), sep = \"; \", na.rm = TRUE) %>% \r\n  unite(\"cr_publ_creds\", c(publisher, member), sep = \"; \", na.rm = TRUE) %>% \r\n  replace_na(list(cr_2020_2021 = c(0)))\r\n\r\ndatax <- data %>% \r\n  left_join(issn_bit, by = \"title\") %>% \r\n  left_join(cr_bit, by = \"title\")\r\n# datax will be used later for the picture and the table \r\n\r\n\r\n\r\nIn contrast to the dataset with an information from ISSN.org, this one has just few most useful (imho) columns like “prefix” (to analyze the citations of these titles in COCI/Lens/Scopus/WoS), or “member” (to restore the publisher details from CrossRef using members API example.\r\nThe main result is that for 41% of VAK titles there are no 2020/2021 publications with ISSNs either indicated in the list, or present in ISSN.org.\r\n\r\nShow code\r\nchart\r\n\r\n\r\n\r\n\r\n6. Final table\r\nFor convinience an interactive table below is made with the composite columns, so that one could filter the rows using the search field, and quickly investigate the data. The links to the full datasets can be found below in a section named “Shared Data”.\r\nBy default the table is shown with few columns hidden (subjects, subj_codes, original VAK title and comments), but those columns are also used by DT Search functionality to filter the rows (try to put “14.01.07” into a Search pane to filter out the journals approved as credible for those who are going to defend a degree in a subject 14.01.07 - Ophthalmic Diseases).\r\n\r\nShow code\r\ndatax %>% \r\n  select(title_main, issn,  title,  comments, issn_jrnl_creds, \r\n         `cr_2020+` = cr_2020_2021, cr_jrnl_creds, cr_publ_creds,\r\n                  subjects,   subj_codes) %>% \r\n DT::datatable(rownames = FALSE, escape = FALSE, \r\n                class = 'compact', extensions = 'Buttons', \r\n         options = list(deferRender = TRUE, autoWidth = TRUE, \r\n                        buttons = list(I('colvis'),'csv', 'excel'),\r\n                        ordering = TRUE, scrollX = TRUE, \r\n                        dom = 'Bfrtip', pageLength = 5, \r\n              columnDefs = list(\r\n                list(visible = FALSE,targets = c(2,3,8,9)),\r\n                list(width = '200px', targets = c(0,1,6,7)),\r\n                list(width = '400px', targets = c(2,3,4,8,9)),\r\n                list(className = 'dt-center', targets = c(5)),\r\n                list(className = 'dt-left', targets = c(0:4,6:9)))\r\n              )\r\n         )\r\n\r\n\r\n\r\n\r\nLimitations\r\nI have not checked every ISSN in the VAK tables There could be some ISSNs that are no longer valid and/or substituted by newer ones, which could explain why I have not found them.\r\nCrossRef info is retrieved by querying 2020/2021 publications, so if the fields “cr_jrnl_creds” and “cr_publ_creds” are empty, it does not mean that the journals are absent in CrossRef. There could be some older publications (but who cares, right?).\r\nAnyway, please use these data with caution, regarding the empty fields as a warning, not as a final confirmation or recommendation.\r\nShared data\r\nThe original PDF file, the parsed and revised data in CSV format, and 2 datasets (issn+, crossref+) are available at Figshare.\r\nFor those who would like to re-use or quote it, please do not hesitate - no permission is required. This post and figshare repository provide ready-to-go citation texts.\r\nAcknowledgments\r\nAllaire J, Xie Y, McPherson J, Luraschi J, Ushey K, Atkins A, Wickham H, Cheng J, Chang W, Iannone R (2021). rmarkdown: Dynamic Documents for R. R package version 2.7, <URL: https://github.com/rstudio/rmarkdown>.\r\nChang, W (2014). extrafont: Tools for using fonts. R package version 0.17, <URL: https://CRAN.R-project.org/package=extrafont>.\r\nHenry L, Wickham H (2020). purrr: Functional Programming Tools. R package version 0.3.4, <URL: https://CRAN.R-project.org/package=purrr>.\r\nOoms J (2014). “The jsonlite Package: A Practical and Consistent Mapping Between JSON Data and R Objects.” arXiv:1403.2805 [stat.CO]. <URL: https://arxiv.org/abs/1403.2805>.\r\nOoms J (2020). pdftools: Text Extraction, Rendering and Converting of PDF Documents. R package version 2.3.1, <URL: https://CRAN.R-project.org/package=pdftools>.\r\nPedersen T (2020). patchwork: The Composer of Plots. R package version 1.1.1, <URL: https://CRAN.R-project.org/package=patchwork>.\r\nWickham H (2020). tidyr: Tidy Messy Data. R package version 1.1.2, <URL: https://CRAN.R-project.org/package=tidyr>.\r\nWickham H (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN 978-3-319-24277-4, <URL: https://ggplot2.tidyverse.org>.\r\nWickham H (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=stringr>.\r\nWickham H (2020). httr: Tools for Working with URLs and HTTP. R package version 1.4.2, <URL: https://CRAN.R-project.org/package=httr>.\r\nWickham H (2021). rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.0, <URL: https://CRAN.R-project.org/package=rvest>.\r\nWickham H, Francois R, Henry L, Muller K (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.3, <URL: https://CRAN.R-project.org/package=dplyr>.\r\nWickham H, Hester J (2020). readr: Read Rectangular Text Data. R package version 1.4.0, <URL: https://CRAN.R-project.org/package=readr>.\r\nWickham H, Seidel D (2020). scales: Scale Functions for Visualization. R package version 1.1.1, <URL: https://CRAN.R-project.org/package=scales>.\r\nXie Y (2020). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.30, <URL: https://yihui.org/knitr/>.\r\nXie Y (2015). Dynamic Documents with R and knitr, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, <URL: https://yihui.org/knitr/>.\r\nXie Y (2014). “knitr: A Comprehensive Tool for Reproducible Research in R.” In Stodden V, Leisch F, Peng RD (eds.), Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595, <URL: http://www.crcpress.com/product/isbn/9781466561595>.\r\nXie Y, Allaire J, Grolemund G (2018). R Markdown: The Definitive Guide. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9781138359338, <URL: https://bookdown.org/yihui/rmarkdown>.\r\nXie Y, Cheng J, Tan X (2021). DT: A Wrapper of the JavaScript Library ‘DataTables’. R package version 0.17, <URL: https://CRAN.R-project.org/package=DT>.\r\nXie Y, Dervieux C, Riederer E (2020). R Markdown Cookbook. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 9780367563837, <URL: https://bookdown.org/yihui/rmarkdown-cookbook>.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-10-vak-titles-in-crossref/images/vak_titles_in_crossref.png",
    "last_modified": "2021-05-27T02:23:57+03:00",
    "input_file": {},
    "preview_width": 1500,
    "preview_height": 750
  },
  {
    "path": "posts/2021-05-09-vak-list-pdfs/",
    "title": "Extracting the Tables from PDF",
    "description": "In this post I am using a R package tabulizer to extract a large table from 853-page PDF, containing a list of VAK (Russian) journal titles.",
    "author": [
      {
        "name": "Aleksei Lutai",
        "url": "https://www.linkedin.com/in/lutaya/"
      }
    ],
    "date": "2021-05-09",
    "categories": [
      "russian data",
      "r",
      "gov data"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\n1. Splitting PDFs\r\n2. Parsing PDF files\r\n3. Cleaning the ISSNs\r\n4. Cleaning the titles\r\n5. Extracting the subject areas\r\n6. Merging\r\n7. How to update this list\r\n\r\nIntroduction\r\nThe VAK list is a Russian thing, or rather “the THING”, for the Russian scientific community. VAK abbreviation stands for “Higher attestation Commission” under the Ministry of education and science of the Russian Federation. This commission approves a list of the Russian journal titles credited for PhD/Doctoral qualifications. The rules are that prior to defending PhD or Doctoral degree the candidate has to publish the main results within at least N publications in the journals that are:\r\neither indexed in one of the authoritative A&I/citation databases (Scopus/WoS/GeoRef/AGRIS/CAS/…),\r\nor present in the VAK list in question.\r\nPutting it in a more straightforward way, VAK defines which scientific journals are worthy to publish in. And the also manage the degree registration process. The almighty state regulator, so to say, which does not excuse them for not publishing this list in a machine-readable format. Most versions I saw were either a simplified lists in Word (just journal titles) or more detailed tables in PDF, but never a properly documented CSV/XLSX. Not very convenient as the table is 800+ pages long.\r\nWell, that’s a task for R/tabulizer.\r\nA fresh version of VAK list can be found here. In this exercise I am going to use the last version of PDF file I have, dated as of April 8, 2021. But as they update the list almost every few months, the exercise will have to be repeated.\r\n1. Splitting PDFs\r\nMy general approach to parsing PDFs is first to split the large file into separate one-page PDF files, and then to parse them. This payoff for this additional work is that the process can be controlled - I can see how many pages are processed, fix the code if there is a strange table or simply re-run that particular page later.\r\nTabulizer can parse multi-page PDFs, but who would like to start the process again and again after finding the unexpected errors in a 853-page document? There is an attribute “page” in tabulizer::extract_tables, so one can try a cycle approach to extract the tables page by page, but on my laptop an execution time started to increase with every cycle and after 5-6 pages froze.\r\nThe files of 20-50 pages are splitted with tabulizer::split_pdf very quickly, but with this file I got a message java.lang.OutOfMemoryError: Java heap space message. The known solution is to allocate Java more memory with options(java.parameters = “-Xmx8g”) (1, 2), but my laptop is just 8 Gb and I am using Win10/RStudio, so the extra memory assignment has not helped.\r\nEventually I splitted the PDF to 853 one-page PDF files with PDF24. It took approximately 3 minutes and, no, this is not an advertisement.\r\n2. Parsing PDF files\r\nIn order to extract a table from PDF I am using tabulizer::extract_tables function with an attribute “columns” that requires a list with x coordinates of the borders between the columns. If a table has 5 columns, pass to the function 4 x-coordinates.\r\nMy general approach is to measure the x distances in PDF in mm (using the build-in measuring tool) and then convert (pro-rate) them to px (for the latter I use a page width in px returned by tabulizer::get_page_dims function). Please see the comments in the code.\r\n\r\nShow code\r\ndir_pdf <- paste0(dir, \"pdf/\")\r\n#reading the list of all one-page PDFs\r\npdfs <- list.files(dir_pdf, full.names = TRUE)\r\n\r\n#getting x-size of the page (width) \r\npage_width <- get_page_dims(pdfs[1]) %>% unlist() %>% .[1]  #595.32\r\n\r\n#the x-distances (converting from mm to px)\r\ncol_borders <- c(25, 82.5, 106, 176)*(page_width/210)\r\n\r\ncolnames <- c(\"no\", \"title\", \"issn\", \"subject\", \"as_of\")\r\n\r\nfor (i in 1:length(pdfs)){\r\n  columns <- extract_tables(file = pdfs[i], \r\n                            columns = list(col_borders),\r\n                            guess = FALSE,  encoding=\"UTF-8\", \r\n                            output = \"matrix\") \r\n  tab <- as.data.frame(columns)\r\n  # sometimes the last column is getting lost (if empty)\r\n  colnames(tab) <- colnames[1:ncol(tab)]\r\n  # saving in files with the names like 001.csv. 056.csv, 123.csv, etc \r\n  write_excel_csv(tab, paste0(dir_pdf, \r\n                              str_extract(paste0(\"00\",i),\"\\\\d{3}$\"),\r\n                              \".csv\"))\r\n  print(i)\r\n}\r\n\r\n\r\n\r\nParsing 1-page PDF takes approximately 2 seconds, so a whole procedure took about 30 min.\r\nThe next step would be to merge all csv-files into one (using purrr::map_df) and clean the data.\r\n\r\nShow code\r\nmerged_data <- list.files(dir_pdf, full.names = TRUE) %>% \r\n  # selecting only CSV files\r\n  .[grepl(\"csv\",.)] %>% \r\n  # merging it with purrr::map_df function\r\n  map_df(~read_csv(.x, col_types = cols(.default = col_character())))\r\n\r\n\r\n\r\nThe result is a dataset with 50021 rows comprising the broken cell values. The first 7 rows are not a part of the original table, but are originated from a text that preceded the table in the original file - we will remove it.\r\nLuckily the text in the original table was aligned to the top of the cells and there was a column with the row numbers that left intact by parsing procedure. This simplifies our task - the positions of non-empty values in the first column correspond to the top border of the rows. And once we know the borders of the row, all we are left to do is to merge in each column all the strings values relating to the particular rows in the original tables.\r\nFirst, we will mark all rows that correspond to the original rows by copying the top border tag in the column “no” down from one top border position to the other. This can be done with tidyr::fill(“down”) function.\r\n\r\nShow code\r\nmerged_data_clean <- merged_data[-c(1:7),] %>% \r\n  fill(no, .direction = \"down\")\r\n\r\n\r\n\r\nAs now we have marked all the original rows with the tags in the column “no”, at the mext steps we will be using “no” as a grouping variable to glue up the string values together with dplyr::summarise(… = paste(., collapse = \" \")). I will do this separately for ISSN, titles, and the subject codes.\r\n3. Cleaning the ISSNs\r\nIn addition to common problems with ISSN lists (dash/hyphen/minus), the lists originating from Russia suffers from Cyrillic “X” letters. So I wrote a special function to clean all spaces, normalize the hyphens, and glue the ISSN values via comma (for some journals there are 2 values).\r\n\r\nShow code\r\n# function to extract ISSSN\r\nextract_issn <- function(text){\r\n  issn<-gsub(\"\\u0425\",\"X\", text)\r\n  issn<-gsub(\"\\u0445\",\"X\", issn)\r\n  issn <- gsub(\"\\u002D|\\u2010|\\u2011|\\u2012|\\u2013|\\u2014|-\",\"-\", issn)\r\n  issn<-gsub(\"\\\\s\\\\-\\\\s\",\"\\\\-\", issn)\r\n  issn<-toupper(issn)\r\n  issn <- ifelse(nchar(issn)==8, \r\n                 paste(substr(issn, 1, 4), substr(issn, 5, 8), sep = \"-\"), issn)\r\n  issn<-sapply(str_extract_all(issn, pattern=\"[[0-9]X\\\\-]+\"),\r\n               function(x) paste0(x, collapse = \",\"))\r\n  issn\r\n}\r\n\r\nmd1 <- merged_data_clean %>% select(no, issn) %>% \r\n  group_by(no) %>% \r\n  summarize_all(~paste(unique(na.omit(.x)), collapse = \" \")) %>%\r\n  ungroup() %>% \r\n  mutate(issn = extract_issn(issn))\r\n\r\n\r\n\r\n4. Cleaning the titles\r\nSome journal titles are accompanied with the comments in round brackets, which we will extract into a separate column. Some titles are also accompanied with the english title, but they have irregular format, so we will not process it.\r\n\r\nShow code\r\nmd2 <- merged_data_clean %>% select(no, title) %>% \r\n  group_by(no) %>% \r\n  summarize_all(~paste(unique(na.omit(.x)), collapse = \" \")) %>%\r\n  ungroup() %>% \r\n  # extracting the main title part (before the round bracket)\r\n  mutate(title_main = str_extract(title, \"^[^\\\\(]+\")) %>% \r\n  # extracting everything after the round bracket\r\n  mutate(comments = str_extract(title, \"\\\\(.+?\\\\)\")) %>% \r\n  mutate_at(c(\"title_main\", \"comments\"), ~str_squish(gsub(\"\\\\|\",\" \", .x)))\r\n\r\n\r\n\r\n5. Extracting the subject areas\r\nEach journal is accompanied with a list of subject codes and subejct names, and also with the dates since when the subject code was assigned to the title. These rules are desined to prevent the specialists to score from publishing in the non-relevant journals. Well, one may argue that there is nothing wrong if say a mathematician contributes to the medical journal, but VAK has been designed to set the rules, not to argue.\r\nAs the subject column containg long strings, there were many broken cells, which we now need to clean up from separators. In order to make this table more convenient, let’s also extract the subject codes into a separate column and re-pack the subejcts under the “as_of” dates. This, I guess, is the least interesting part of the story for non-Russian readers.\r\n\r\nShow code\r\nmd3 <- merged_data_clean %>% select(no, subject, as_of) %>% \r\n  fill(as_of, .direction = \"down\") %>%\r\n  group_by(no, as_of) %>% \r\n  summarize(subject = paste(na.omit(subject), collapse = \" \")) %>%\r\n  ungroup()  \r\n\r\nmd3 <- md3 %>% \r\n  # extracting the subject codes\r\n  mutate(subjs = str_extract_all(subject, \r\n                                 \"\\\\d{2}\\\\.\\\\d{2}\\\\.\\\\d{2}[^\\\\(]+\")) %>% \r\n  unnest(subjs) %>% \r\n  mutate(subjs = str_extract_all(subjs, \r\n                                 \"\\\\d{2}\\\\.\\\\d{2}\\\\.\\\\d{2}[^\\\\d]+\")) %>% \r\n  unnest(subjs) %>%  \r\n  mutate(subjs = str_squish(subjs), \r\n         subjs = gsub(\"\\\\,$|\\\\.$|\\\\;$\",\"\", subjs)) %>%\r\n  mutate(subj_codes = str_extract(subjs, \r\n                                  \"\\\\d{2}\\\\.\\\\d{2}\\\\.\\\\d{2}\")) %>% \r\n  group_by(no, as_of) %>% \r\n  summarize_at(c(\"subjs\", \"subj_codes\"), \r\n               ~paste(unique(na.omit(.x)), collapse = \" | \")) %>% \r\n  mutate(subjects = paste0(\"[\", subjs, \"] \")) %>% \r\n  # re-packing the subjects with the starting dates\r\n  unite(\"subjects\", c(\"as_of\", \"subjs\"), sep = \": \") %>% \r\n  group_by(no) %>% \r\n  summarize(subjects = paste(subjects, collapse = \" || \"),\r\n            subj_codes= paste(subj_codes, collapse = \" | \")) %>% \r\n  ungroup()\r\n\r\n## let's also sort the subj_codes in the cell\r\nmd3 <- md3 %>% \r\n  mutate(subj_codes = str_split(subj_codes,\" \\\\| \")) %>% \r\n  unnest(subj_codes) %>% \r\n  arrange(subj_codes) %>% \r\n  group_by(no, subjects) %>% \r\n  summarize(subj_codes= paste(subj_codes, collapse = \" | \")) %>% \r\n  ungroup()\r\n\r\n\r\n\r\n6. Merging\r\n\r\nShow code\r\ndata_clean <- md1 %>% left_join(md2) %>% left_join(md3) %>% \r\n  mutate(no = as.numeric(gsub(\"\\\\.\", \"\", no))) %>% \r\n  arrange(no)\r\n\r\ndata_clean %>% write_excel_csv(paste0(dir, \"2021_04_vak_list_parsed_data.csv\"))\r\n\r\n\r\n\r\nThe final dataset is a bit too heavy to incroporate into this html, so I put here first 50 rows in DT::datatable format, with filtering & sorting features, and the buttons to make the columns visible (few columns are already hidden) and to download the table excerpt in csv or excel format.\r\n\r\nShow code\r\nread_csv(paste0(dir, \"2021_04_vak_list_parsed_data.csv\"),\r\n         col_types = cols(.default = col_character(), no = col_integer())) %>% \r\n  .[1:50,] %>% \r\n  DT::datatable(rownames = FALSE, escape = FALSE, \r\n                class = 'compact', extensions = 'Buttons', \r\n         options = list(deferRender = TRUE, autoWidth = TRUE, \r\n                        buttons = list(I('colvis'),'csv', 'excel'),\r\n                        ordering = TRUE, scrollX = TRUE, \r\n                        dom = 'Bfrtip', pageLength = 5, \r\n              columnDefs = list(\r\n                list(visible = FALSE,targets = c(2,4:5)),\r\n                list(width = '200px', targets = c(3:4)),\r\n                list(width = '400px', targets = c(5:6)),\r\n                list(className = 'dt-left', targets = c(0:6)))\r\n              )\r\n         )\r\n\r\n\r\n\r\n\r\nThe full version of parsed table is uploaded to Figshare.\r\nLutay, Alexei (2021): VAK list of journal titles v.2021.04.08. figshare. Dataset. https://doi.org/10.6084/m9.figshare.14561814.\r\n7. How to update this list\r\nI do not have plans to update it, so if you need the fresh version there are 2 options.\r\nthe current list in PDF format can be found at VAK site, and then one needs just to run all the scripts above to get the result.\r\nthe current list in Word can also be found at VAK site. So one may try to match the journal titles and identify the new ones to be added manually to this dataset.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-12T11:18:34+03:00",
    "input_file": {}
  }
]
